{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89d6ca84",
   "metadata": {},
   "source": [
    "#### Framework de Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd59115f",
   "metadata": {},
   "source": [
    "#### Conteúdo - Bases e Notebook da aula\n",
    "\n",
    "Base disponivel no caminho:  \n",
    "\n",
    "https://github.com/FIAP/Pos_Tech_DTAT/tree/Framework-de-Big-Data\n",
    "\n",
    "E para correto funcionamento foi necessario a criação de um roteiro de instalação e configuração: \n",
    "\n",
    "https://github.com/RicardViana/fiap-Big-Data/blob/main/PySpark%20para%20VS%20Code%20e%20Anaconda.pdf\n",
    "\n",
    "Como não é possivel usar o Spark para ler diretamnte o arquivo no Github, usar o modelo abaixo  \n",
    "\n",
    "```python\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/FIAP/Pos_Tech_DTAT/refs/heads/Framework-de-Big-Data/Aula%201/banklist.csv\"\n",
    "local_path = \"banklist.csv\"\n",
    "urllib.request.urlretrieve(url, local_path)\n",
    "\n",
    "df = spark.read.csv(local_path, sep=\",\", header=True, inferSchema=True)\n",
    "\n",
    "```\n",
    "\n",
    "Caso demore muito para executar, recomendo reiniciar o Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a770d68e",
   "metadata": {},
   "source": [
    "#### Importação de pacotes, bibliotecas e funções (def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "733f8511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar biblioteca completa\n",
    "import pandas as pd \n",
    "import findspark\n",
    "import urllib.request\n",
    "\n",
    "# Importar função especifica de um módulo\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Import spark libraries\n",
    "from pyspark.sql import Row, DataFrame\n",
    "from pyspark.sql.types import StringType, StructType, StructField, IntegerType\n",
    "\n",
    "#from pyspark.sql.functions import col, expr, lit, substring, concat, concat_ws, when, coalesce\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4d2a35d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como não queremos configurar as variaveis de ambiente, precisamos usar o findspark.init\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d70f28a",
   "metadata": {},
   "source": [
    "#### Aula 1 - Conhecendo o Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "352c0cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.count: 561\n",
      "df.col ct: 6\n",
      "df.columns: ['Bank Name', 'City', 'ST', 'CERT', 'Acquiring Institution', 'Closing Date']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "1. spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "Esta é a linha que inicializa o Spark. Ela é o ponto de entrada para qualquer funcionalidade do Spark.\n",
    "\n",
    "SparkSession: É o principal ponto de entrada para a programação com DataFrames e Datasets no Spark.\n",
    "\n",
    ".builder: É um padrão de projeto (builder pattern) usado para construir o objeto SparkSession com diferentes configurações.\n",
    "\n",
    ".master(\"local[*]\"): Esta é uma das configurações mais importantes. Ela define como e onde o Spark irá executar suas tarefas.\n",
    "\n",
    "\"local[*]\" instrui o Spark a ser executado em modo local, utilizando todos os núcleos de processamento (cores) disponíveis na sua máquina. \n",
    "Se você quisesse usar apenas 2 núcleos, por exemplo, usaria \"local[2]\". Este modo é ideal para desenvolvimento, testes e aprendizado em uma única máquina.\n",
    "\n",
    ".getOrCreate(): Este método cria uma nova SparkSession se uma não existir. Se já houver uma sessão ativa com as mesmas configurações, ele simplesmente a retorna. \n",
    "Isso evita a criação de múltiplas sessões desnecessariamente.\n",
    "\n",
    "Em resumo: esta linha cria e configura uma sessão do Spark para rodar localmente na sua máquina, usando todos os recursos de processamento disponíveis, e armazena essa sessão na variável spark.\n",
    "\n",
    "2. df = spark.read.csv(link, sep= \",\", inferSchema = True, header = True)\n",
    "Esta é a linha que efetivamente lê os dados e os transforma em um DataFrame, que é a principal estrutura de dados do Spark.\n",
    "\n",
    "spark.read: É o objeto usado para ler dados de fontes externas (arquivos, bancos de dados, etc.).\n",
    "\n",
    ".csv(link, ...): Especifica que o formato do arquivo a ser lido é CSV. O primeiro argumento, link, é o caminho para o arquivo.\n",
    "\n",
    "sep= \",\": Este parâmetro (separador) informa ao Spark que as colunas no arquivo CSV são delimitadas por uma vírgula. É o padrão para arquivos CSV, mas é uma boa prática especificá-lo.\n",
    "\n",
    "inferSchema = True: Esta é uma opção muito útil. Ela instrui o Spark a analisar uma amostra dos dados para inferir (deduzir) automaticamente o tipo de dado de cada coluna. \n",
    "Por exemplo, ele tentará identificar se uma coluna contém números inteiros (IntegerType), números decimais (DoubleType) ou texto (StringType). Sem isso, todas as colunas seriam tratadas como texto (String).\n",
    "\n",
    "header = True: Esta opção informa ao Spark que a primeira linha do arquivo CSV contém os nomes das colunas (cabeçalho) e que essa linha não deve ser tratada como dados. \n",
    "O Spark usará esses nomes para as colunas do DataFrame.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "\n",
    "# Baixar o arquivo pois não conseguimos usar o Spark para ler diretamnte o arquivo no Github\n",
    "url = \"https://raw.githubusercontent.com/FIAP/Pos_Tech_DTAT/refs/heads/Framework-de-Big-Data/Aula%201/banklist.csv\"\n",
    "local_path = \"banklist.csv\"\n",
    "urllib.request.urlretrieve(url, local_path)\n",
    "\n",
    "# Ler com Spark\n",
    "df = spark.read.csv(local_path, sep=\",\", header=True, inferSchema=True)\n",
    "\n",
    "print(f\"df.count: {df.count()}\")\n",
    "print(f\"df.col ct: {len(df.columns)}\")\n",
    "print(f\"df.columns: {df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "18b59f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-------------+------------+\n",
      "|Bank Name                       |City         |Closing Date|\n",
      "+--------------------------------+-------------+------------+\n",
      "|The First State Bank            |Barboursville|3-Apr-20    |\n",
      "|Ericson State Bank              |Ericson      |14-Feb-20   |\n",
      "|City National Bank of New Jersey|Newark       |1-Nov-19    |\n",
      "|Resolute Bank                   |Maumee       |25-Oct-19   |\n",
      "+--------------------------------+-------------+------------+\n",
      "only showing top 4 rows\n"
     ]
    }
   ],
   "source": [
    "# Criar uma tabela temporaria\n",
    "df.createOrReplaceTempView(\"banklist\")\n",
    "\n",
    "# Fazer o select \n",
    "df_check = spark.sql(''' select `Bank Name`, `City`,`Closing Date` from banklist ''')\n",
    "df_check.show(4, truncate=False) # --> Retornar apenas os 4 primeiros dados e não truncar textos longos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aacacb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------+----+-----------------+---------------------+------------+\n",
      "|summary|           Bank Name|   City|  ST|             CERT|Acquiring Institution|Closing Date|\n",
      "+-------+--------------------+-------+----+-----------------+---------------------+------------+\n",
      "|  count|                 561|    561| 561|              561|                  561|         561|\n",
      "|   mean|                NULL|   NULL|NULL|31685.68449197861|                 NULL|        NULL|\n",
      "| stddev|                NULL|   NULL|NULL|16446.65659309965|                 NULL|        NULL|\n",
      "|    min|1st American Stat...|Acworth|  AL|               91|      1st United Bank|    1-Aug-08|\n",
      "|    max|               ebank|Wyoming|  WY|            58701|  Your Community Bank|    9-Sep-11|\n",
      "+-------+--------------------+-------+----+-----------------+---------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Executar a estatistica descritiva \n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c7b537ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----+\n",
      "|summary|   City|  ST|\n",
      "+-------+-------+----+\n",
      "|  count|    561| 561|\n",
      "|   mean|   NULL|NULL|\n",
      "| stddev|   NULL|NULL|\n",
      "|    min|Acworth|  AL|\n",
      "|    max|Wyoming|  WY|\n",
      "+-------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Executar a estatistica descritiva \n",
    "df.describe('City', 'ST').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a574c3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de linhas: 561\n",
      "Total de colunas: 6\n",
      "Colunas: ['Bank Name', 'City', 'ST', 'CERT', 'Acquiring Institution', 'Closing Date']\n",
      "Tipo de dados: [('Bank Name', 'string'), ('City', 'string'), ('ST', 'string'), ('CERT', 'int'), ('Acquiring Institution', 'string'), ('Closing Date', 'string')]\n",
      "Schema StructType([StructField('Bank Name', StringType(), True), StructField('City', StringType(), True), StructField('ST', StringType(), True), StructField('CERT', IntegerType(), True), StructField('Acquiring Institution', StringType(), True), StructField('Closing Date', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "# Ver informações do data frame \n",
    "print(f\"Total de linhas: {df.count()}\")\n",
    "print(f\"Total de colunas: {len(df.columns)}\")\n",
    "print(f\"Colunas: {df.columns}\")\n",
    "print(f\"Tipo de dados: {df.dtypes}\")\n",
    "print(f\"Schema {df.schema}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5179310e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Bank Name: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- ST: string (nullable = true)\n",
      " |-- CERT: integer (nullable = true)\n",
      " |-- Acquiring Institution: string (nullable = true)\n",
      " |-- Closing Date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ver o Schema dos dados \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bb12162a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.count: 561\n",
      "df.columns: ['Bank Name', 'City', 'ST', 'CERT', 'Acquiring Institution', 'Closing Date']\n"
     ]
    }
   ],
   "source": [
    "# Remover duplicadas \n",
    "df = df.dropDuplicates()\n",
    "print(f\"df.count: {df.count()}\")\n",
    "print(f\"df.columns: {df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9a89c4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+----------------+\n",
      "|Bank Name                        |City            |\n",
      "+---------------------------------+----------------+\n",
      "|First Bank of Idaho              |Ketchum         |\n",
      "|Amcore Bank, National Association|Rockford        |\n",
      "|Venture Bank                     |Lacey           |\n",
      "|First State Bank of Altus        |Altus           |\n",
      "|Valley Capital Bank, N.A.        |Mesa            |\n",
      "|Michigan Heritage Bank           |Farmington Hills|\n",
      "|Columbia Savings Bank            |Cincinnati      |\n",
      "|Fidelity Bank                    |Dearborn        |\n",
      "|The Park Avenue Bank             |Valdosta        |\n",
      "|Western Commercial Bank          |Woodland Hills  |\n",
      "|Syringa Bank                     |Boise           |\n",
      "|Republic Federal Bank, N.A.      |Miami           |\n",
      "|Westside Community Bank          |University Place|\n",
      "|First United Bank                |Crete           |\n",
      "|HarVest Bank of Maryland         |Gaithersburg    |\n",
      "|BankEast                         |Knoxville       |\n",
      "|Polk County Bank                 |Johnston        |\n",
      "|Colorado Capital Bank            |Castle Rock     |\n",
      "|Access Bank                      |Champlin        |\n",
      "|Pacific National Bank            |San Francisco   |\n",
      "+---------------------------------+----------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Selecionar colunas especificas do Data Set\n",
    "df2 = df.select(*['Bank Name', 'City'])\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "470d74f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------------------------+----------------+----------------------------------------------------+\n",
      "|Closing Date|Bank Name                        |City            |Acquiring Institution                               |\n",
      "+------------+---------------------------------+----------------+----------------------------------------------------+\n",
      "|24-Apr-09   |First Bank of Idaho              |Ketchum         |U.S. Bank, N.A.                                     |\n",
      "|23-Apr-10   |Amcore Bank, National Association|Rockford        |Harris N.A.                                         |\n",
      "|11-Sep-09   |Venture Bank                     |Lacey           |First-Citizens Bank & Trust Company                 |\n",
      "|31-Jul-09   |First State Bank of Altus        |Altus           |Herring Bank                                        |\n",
      "|11-Dec-09   |Valley Capital Bank, N.A.        |Mesa            |Enterprise Bank & Trust                             |\n",
      "|24-Apr-09   |Michigan Heritage Bank           |Farmington Hills|Level One Bank                                      |\n",
      "|23-May-14   |Columbia Savings Bank            |Cincinnati      |United Fidelity Bank, fsb                           |\n",
      "|30-Mar-12   |Fidelity Bank                    |Dearborn        |The Huntington National Bank                        |\n",
      "|29-Apr-11   |The Park Avenue Bank             |Valdosta        |Bank of the Ozarks                                  |\n",
      "|5-Nov-10    |Western Commercial Bank          |Woodland Hills  |First California Bank                               |\n",
      "|31-Jan-14   |Syringa Bank                     |Boise           |Sunwest Bank                                        |\n",
      "|11-Dec-09   |Republic Federal Bank, N.A.      |Miami           |1st United Bank                                     |\n",
      "|11-Jan-13   |Westside Community Bank          |University Place|Sunwest Bank                                        |\n",
      "|28-Sep-12   |First United Bank                |Crete           |Old Plank Trail Community Bank, National Association|\n",
      "|27-Apr-12   |HarVest Bank of Maryland         |Gaithersburg    |Sonabank                                            |\n",
      "|27-Jan-12   |BankEast                         |Knoxville       |U.S. Bank, N.A.                                     |\n",
      "|18-Nov-11   |Polk County Bank                 |Johnston        |Grinnell State Bank                                 |\n",
      "|8-Jul-11    |Colorado Capital Bank            |Castle Rock     |First-Citizens Bank & Trust Company                 |\n",
      "|7-May-10    |Access Bank                      |Champlin        |PrinsBank                                           |\n",
      "|30-Oct-09   |Pacific National Bank            |San Francisco   |U.S. Bank N.A.                                      |\n",
      "+------------+---------------------------------+----------------+----------------------------------------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Selecionar colunas exceto o que tiver na lista \n",
    "col_l = list(set(df.columns) - {'ST', 'CERT'})\n",
    "df2 = df.select(*col_l)\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1bf6bc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+-----+-----+--------------------+------------+\n",
      "|           bank_name|            City|state| cert|     acq_institution|closing_date|\n",
      "+--------------------+----------------+-----+-----+--------------------+------------+\n",
      "| First Bank of Idaho|         Ketchum|   ID|34396|     U.S. Bank, N.A.|   24-Apr-09|\n",
      "|Amcore Bank, Nati...|        Rockford|   IL| 3735|         Harris N.A.|   23-Apr-10|\n",
      "|        Venture Bank|           Lacey|   WA|22868|First-Citizens Ba...|   11-Sep-09|\n",
      "|First State Bank ...|           Altus|   OK| 9873|        Herring Bank|   31-Jul-09|\n",
      "|Valley Capital Ba...|            Mesa|   AZ|58399|Enterprise Bank &...|   11-Dec-09|\n",
      "|Michigan Heritage...|Farmington Hills|   MI|34369|      Level One Bank|   24-Apr-09|\n",
      "|Columbia Savings ...|      Cincinnati|   OH|32284|United Fidelity B...|   23-May-14|\n",
      "|       Fidelity Bank|        Dearborn|   MI|33883|The Huntington Na...|   30-Mar-12|\n",
      "|The Park Avenue Bank|        Valdosta|   GA|19797|  Bank of the Ozarks|   29-Apr-11|\n",
      "|Western Commercia...|  Woodland Hills|   CA|58087|First California ...|    5-Nov-10|\n",
      "|        Syringa Bank|           Boise|   ID|34296|        Sunwest Bank|   31-Jan-14|\n",
      "|Republic Federal ...|           Miami|   FL|22846|     1st United Bank|   11-Dec-09|\n",
      "|Westside Communit...|University Place|   WA|33997|        Sunwest Bank|   11-Jan-13|\n",
      "|   First United Bank|           Crete|   IL|20685|Old Plank Trail C...|   28-Sep-12|\n",
      "|HarVest Bank of M...|    Gaithersburg|   MD|57766|            Sonabank|   27-Apr-12|\n",
      "|            BankEast|       Knoxville|   TN|19869|     U.S. Bank, N.A.|   27-Jan-12|\n",
      "|    Polk County Bank|        Johnston|   IA|14194| Grinnell State Bank|   18-Nov-11|\n",
      "|Colorado Capital ...|     Castle Rock|   CO|34522|First-Citizens Ba...|    8-Jul-11|\n",
      "|         Access Bank|        Champlin|   MN|16476|           PrinsBank|    7-May-10|\n",
      "|Pacific National ...|   San Francisco|   CA|30006|      U.S. Bank N.A.|   30-Oct-09|\n",
      "+--------------------+----------------+-----+-----+--------------------+------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Renomear as colunas \n",
    "df2 = df \\\n",
    "  .withColumnRenamed('Bank Name'            , 'bank_name') \\\n",
    "  .withColumnRenamed('Acquiring Institution', 'acq_institution') \\\n",
    "  .withColumnRenamed('Closing Date'         , 'closing_date') \\\n",
    "  .withColumnRenamed('ST'                   , 'state') \\\n",
    "  .withColumnRenamed('CERT'                 , 'cert') \n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3aba35bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---+-----+---------------------+------------+-----+\n",
      "|           Bank Name|    City| ST| CERT|Acquiring Institution|Closing Date|State|\n",
      "+--------------------+--------+---+-----+---------------------+------------+-----+\n",
      "| First Bank of Idaho| Ketchum| ID|34396|      U.S. Bank, N.A.|   24-Apr-09|   ID|\n",
      "|Amcore Bank, Nati...|Rockford| IL| 3735|          Harris N.A.|   23-Apr-10|   IL|\n",
      "|        Venture Bank|   Lacey| WA|22868| First-Citizens Ba...|   11-Sep-09|   WA|\n",
      "|First State Bank ...|   Altus| OK| 9873|         Herring Bank|   31-Jul-09|   OK|\n",
      "|Valley Capital Ba...|    Mesa| AZ|58399| Enterprise Bank &...|   11-Dec-09|   AZ|\n",
      "+--------------------+--------+---+-----+---------------------+------------+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Adicionar uma nova coluna \n",
    "df2 = df.withColumn('State', col('ST'))\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "57a33a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---+-----+---------------------+------------+-------+\n",
      "|           Bank Name|    City| ST| CERT|Acquiring Institution|Closing Date|country|\n",
      "+--------------------+--------+---+-----+---------------------+------------+-------+\n",
      "| First Bank of Idaho| Ketchum| ID|34396|      U.S. Bank, N.A.|   24-Apr-09|     US|\n",
      "|Amcore Bank, Nati...|Rockford| IL| 3735|          Harris N.A.|   23-Apr-10|     US|\n",
      "|        Venture Bank|   Lacey| WA|22868| First-Citizens Ba...|   11-Sep-09|     US|\n",
      "|First State Bank ...|   Altus| OK| 9873|         Herring Bank|   31-Jul-09|     US|\n",
      "|Valley Capital Ba...|    Mesa| AZ|58399| Enterprise Bank &...|   11-Dec-09|     US|\n",
      "+--------------------+--------+---+-----+---------------------+------------+-------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Adicionar uma nova coluna\n",
    "df2 = df.withColumn('country', lit('US'))\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "899c18ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---+---------------------+------------+\n",
      "|           Bank Name|    City| ST|Acquiring Institution|Closing Date|\n",
      "+--------------------+--------+---+---------------------+------------+\n",
      "| First Bank of Idaho| Ketchum| ID|      U.S. Bank, N.A.|   24-Apr-09|\n",
      "|Amcore Bank, Nati...|Rockford| IL|          Harris N.A.|   23-Apr-10|\n",
      "|        Venture Bank|   Lacey| WA| First-Citizens Ba...|   11-Sep-09|\n",
      "|First State Bank ...|   Altus| OK|         Herring Bank|   31-Jul-09|\n",
      "|Valley Capital Ba...|    Mesa| AZ| Enterprise Bank &...|   11-Dec-09|\n",
      "+--------------------+--------+---+---------------------+------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Apagar coluna \n",
    "df2 = df.drop('CERT')\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0af2a4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------------------+------------+\n",
      "|           Bank Name|    City|Acquiring Institution|Closing Date|\n",
      "+--------------------+--------+---------------------+------------+\n",
      "| First Bank of Idaho| Ketchum|      U.S. Bank, N.A.|   24-Apr-09|\n",
      "|Amcore Bank, Nati...|Rockford|          Harris N.A.|   23-Apr-10|\n",
      "|        Venture Bank|   Lacey| First-Citizens Ba...|   11-Sep-09|\n",
      "|First State Bank ...|   Altus|         Herring Bank|   31-Jul-09|\n",
      "|Valley Capital Ba...|    Mesa| Enterprise Bank &...|   11-Dec-09|\n",
      "+--------------------+--------+---------------------+------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Apagar varias colunas \n",
    "df2 = df.drop(*[\"CERT\",\"ST\"])\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d372cdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------------------+------------+\n",
      "|           Bank Name|    City|Acquiring Institution|Closing Date|\n",
      "+--------------------+--------+---------------------+------------+\n",
      "| First Bank of Idaho| Ketchum|      U.S. Bank, N.A.|   24-Apr-09|\n",
      "|Amcore Bank, Nati...|Rockford|          Harris N.A.|   23-Apr-10|\n",
      "|        Venture Bank|   Lacey| First-Citizens Ba...|   11-Sep-09|\n",
      "|First State Bank ...|   Altus|         Herring Bank|   31-Jul-09|\n",
      "|Valley Capital Ba...|    Mesa| Enterprise Bank &...|   11-Dec-09|\n",
      "+--------------------+--------+---------------------+------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Apagar varias colunas com reduce \n",
    "df2 = reduce(DataFrame.drop, [\"CERT\",\"ST\"], df)\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d76b8f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.count  : 561\n",
      "df2.count : 4\n",
      "df3.count : 9\n",
      "df4.count : 73\n"
     ]
    }
   ],
   "source": [
    "# Realizar filtros \n",
    "df2 = df.where(df['ST'] == 'NE')\n",
    "df3 = df.where(df['CERT'].between('1000','2000'))\n",
    "df4 = df.where(df['ST'].isin('NE','IL'))\n",
    "\n",
    "print('df.count  :', df.count())\n",
    "print('df2.count :', df2.count())\n",
    "print('df3.count :', df3.count())\n",
    "print('df4.count :', df4.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "340dce78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+---+-----+---------------------+------------+\n",
      "|         Bank Name|   City| ST| CERT|Acquiring Institution|Closing Date|\n",
      "+------------------+-------+---+-----+---------------------+------------+\n",
      "|Ericson State Bank|Ericson| NE|18265| Farmers and Merch...|   14-Feb-20|\n",
      "+------------------+-------+---+-----+---------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Realizar filtros --> AND\n",
    "df2 = df.where((df['ST'] == 'NE') & (df['City'] == 'Ericson'))\n",
    "df2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5ba5eb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---+-----+---------------------+------------+\n",
      "|           Bank Name|    City| ST| CERT|Acquiring Institution|Closing Date|\n",
      "+--------------------+--------+---+-----+---------------------+------------+\n",
      "| First Bank of Idaho| Ketchum| ID|34396|      U.S. Bank, N.A.|   24-Apr-09|\n",
      "|Amcore Bank, Nati...|Rockford| IL| 3735|          Harris N.A.|   23-Apr-10|\n",
      "|        Venture Bank|   Lacey| WA|22868| First-Citizens Ba...|   11-Sep-09|\n",
      "|First State Bank ...|   Altus| OK| 9873|         Herring Bank|   31-Jul-09|\n",
      "|Valley Capital Ba...|    Mesa| AZ|58399| Enterprise Bank &...|   11-Dec-09|\n",
      "+--------------------+--------+---+-----+---------------------+------------+\n",
      "only showing top 5 rows\n",
      "Replace 7 in the above dataframe with 17 at all instances\n",
      "+--------------------+--------+---+-----+---------------------+------------+\n",
      "|           Bank Name|    City| ST| CERT|Acquiring Institution|Closing Date|\n",
      "+--------------------+--------+---+-----+---------------------+------------+\n",
      "| First Bank of Idaho| Ketchum| ID|34396|      U.S. Bank, N.A.|   24-Apr-09|\n",
      "|Amcore Bank, Nati...|Rockford| IL| 3735|          Harris N.A.|   23-Apr-10|\n",
      "|        Venture Bank|   Lacey| WA|22868| First-Citizens Ba...|   11-Sep-09|\n",
      "|First State Bank ...|   Altus| OK| 9873|         Herring Bank|   31-Jul-09|\n",
      "|Valley Capital Ba...|    Mesa| AZ|58399| Enterprise Bank &...|   11-Dec-09|\n",
      "+--------------------+--------+---+-----+---------------------+------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Alterar os dados --> Replace\n",
    "df.show(5)\n",
    "\n",
    "print('Replace 7 in the above dataframe with 17 at all instances')\n",
    "df.na.replace(7,17).show(5) # --> Dessa forma, substitui em todas as colunas o que for 7 por 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74af7dfc",
   "metadata": {},
   "source": [
    "#### Aula 2 - Operações Básicas no Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "47eaf554",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "O trecho de código sc = SparkContext.getOrCreate() é uma instrução fundamental em aplicações Apache Spark, especialmente em versões mais antigas da tecnologia. \n",
    "Ele é responsável por inicializar o ponto de entrada principal para a funcionalidade do Spark, o SparkContext. Vamos detalhar o que cada parte significa.\n",
    "\n",
    "O que é o SparkContext?\n",
    "O SparkContext (geralmente abreviado como sc) é o coração de uma aplicação Spark. Ele representa a conexão com um cluster Spark e é usado para coordenar os processos que serão\n",
    "executados nos nós de trabalho (workers) desse cluster. Pense nele como o \"maestro\" da sua orquestra de processamento de dados distribuídos.\n",
    "\n",
    "As principais funções de um SparkContext são:\n",
    "\n",
    "Configurar parâmetros internos do Spark: Ele carrega as configurações da sua aplicação, como o nome da aplicação, o modo de execução (local ou em cluster) e a quantidade de recursos a serem alocados.\n",
    "\n",
    "Conectar-se ao gerenciador do cluster: Seja ele YARN, Mesos ou o gerenciador standalone do próprio Spark.\n",
    "\n",
    "Criar RDDs (Resilient Distributed Datasets): RDDs são as estruturas de dados fundamentais do Spark, representando uma coleção de elementos particionada e imutável que pode ser operada em paralelo. \n",
    "O SparkContext é a porta de entrada para a criação e manipulação desses RDDs.\n",
    "\n",
    "A importância do .getOrCreate()\n",
    "O método .getOrCreate() possui um comportamento crucial:\n",
    "\n",
    "Se já existir um SparkContext ativo na sua aplicação, ele simplesmente o retorna.\n",
    "\n",
    "Se não houver um SparkContext ativo, ele cria um novo.\n",
    "\n",
    "Esse comportamento é essencial porque, em uma mesma aplicação (mais especificamente, em uma mesma Java Virtual Machine - JVM), apenas um SparkContext pode estar ativo por vez. \n",
    "Tentar criar um novo SparkContext quando um já existe resultaria em um erro. O .getOrCreate() elegantemente resolve esse problema, garantindo que sua aplicação sempre utilize\n",
    "uma única instância do SparkContext, seguindo um padrão de projeto conhecido como Singleton.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Inicializar o SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "57cbb38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Cria e configura o ponto de entrada principal para utilizar as funcionalidades do Spark, especialmente para trabalhar com DataFrames e SQL.\n",
    "\n",
    "A variável spark passa a ser o seu \"portal\" para interagir com o Spark\n",
    "\n",
    "Essa linha utiliza um padrão de projeto chamado \"Builder\" (ou Construtor), onde você encadeia vários métodos para configurar um objeto antes de criá-lo. Vamos ver cada parte:\n",
    "\n",
    "SparkSession\n",
    "É a classe principal, o ponto de entrada unificado para uma aplicação Spark a partir da versão 2.0. Ela engloba funcionalidades que antes eram separadas em diferentes \"contextos\" (como o SparkContext e SQLContext).\n",
    "\n",
    ".builder\n",
    "Este é um método que inicia o processo de construção de uma SparkSession. Ele retorna um objeto \"construtor\" que permite que você encadeie as configurações desejadas. \n",
    "Pense nele como o início da \"planta\" da sua aplicação.\n",
    "\n",
    ".appName('PySpark DataFrame From RDD')\n",
    "Este método define um nome para a sua aplicação. O nome que você define aqui ('PySpark DataFrame From RDD') aparecerá na interface de usuário do Spark e nos logs. \n",
    "Isso é extremamente útil para monitorar e identificar sua aplicação quando várias estiverem rodando em um mesmo cluster.\n",
    "\n",
    ".getOrCreate()\n",
    "Este é o método final e crucial. Ele faz duas coisas:\n",
    "\n",
    "Get (Obter): Verifica se já existe uma SparkSession ativa nesta aplicação. Se sim, ele simplesmente retorna essa sessão já existente.\n",
    "\n",
    "Create (Criar): Se nenhuma SparkSession estiver ativa, ele cria uma nova com as configurações que você definiu (como o appName) e a retorna.\n",
    "\n",
    "Esse comportamento garante que você não crie acidentalmente múltiplas sessões do Spark, o que causaria erros. É uma forma segura e inteligente de inicialização\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Criar o ponto de entrada para utilizar as funcionalidades do Spark\n",
    "spark = SparkSession.builder.appName('PySpark DataFrame From RDD').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1f8c1efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Este comando é uma das formas mais diretas de se criar um RDD (Resilient Distributed Dataset) no Spark.\n",
    "A sua principal função é pegar uma coleção de dados que já existe na memória do seu programa principal (o \"driver\") e transformá-la em um conjunto de dados distribuído que o Spark pode processar em paralelo.\n",
    "\n",
    "sc\n",
    "Como já vimos, é o seu SparkContext, o ponto de entrada para a manipulação de RDDs.\n",
    "\n",
    ".parallelize(...)\n",
    "Este é o método chave aqui. O nome \"parallelize\" (paralelizar) é bastante descritivo: ele pega uma coleção de dados comum do Python (neste caso, uma lista) e a distribui (paraleliza)\n",
    "entre os diferentes nós de trabalho (workers) do seu cluster Spark. É a forma mais comum de criar RDDs para fins de teste, aprendizado ou para processar dados que já estão na memória.\n",
    "\n",
    "[('C', ...), ('B', ...), ...]\n",
    "Este é o primeiro argumento do método: uma lista ([]) de tuplas (()). Esta é a coleção de dados que você quer distribuir.\n",
    "No momento, esta lista existe apenas na memória do nó \"driver\", onde seu script está sendo executado.\n",
    "\n",
    ", 4\n",
    "Este segundo argumento é extremamente importante. Ele especifica em quantas partições o RDD deve ser dividido.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Criar um RDD (Resilient Distributed Dataset)\n",
    "rdd = sc.parallelize([('C',85,76,87,91), ('B',85,76,87,91), (\"A\", 85,78,96,92), (\"A\", 92,76,89,96)], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "31df7cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.core.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "# Verificar o tipo de dados do objeto\n",
    "print(type(rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ec2427fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma lista com os cabeçalhos ou schema da tabela \n",
    "sub = ['id_person','value_1','value_2','value_3','value_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ee613837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar o data frame\n",
    "marks_df = spark.createDataFrame(rdd, schema=sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b99d80fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.classic.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Ver o tipo do dado do objeto \n",
    "print(type(marks_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f4637c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_person: string (nullable = true)\n",
      " |-- value_1: long (nullable = true)\n",
      " |-- value_2: long (nullable = true)\n",
      " |-- value_3: long (nullable = true)\n",
      " |-- value_4: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ver os Schema dos dados\n",
    "marks_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b01f8f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------+-------+-------+\n",
      "|id_person|value_1|value_2|value_3|value_4|\n",
      "+---------+-------+-------+-------+-------+\n",
      "|        C|     85|     76|     87|     91|\n",
      "|        B|     85|     76|     87|     91|\n",
      "|        A|     85|     78|     96|     92|\n",
      "|        A|     92|     76|     89|     96|\n",
      "+---------+-------+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ver os dados do data frame \n",
    "marks_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "326dabd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer a instancia do Spark\n",
    "spark = SparkSession.builder.appName('pysparkdf').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "db4a5380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixar o arquivo pois não conseguimos usar o Spark para ler diretamnte o arquivo no Github\n",
    "url = \"https://raw.githubusercontent.com/FIAP/Pos_Tech_DTAT/refs/heads/Framework-de-Big-Data/Aula%202/cereal.csv\"\n",
    "local_path = \"cereal.csv\"\n",
    "urllib.request.urlretrieve(url, local_path)\n",
    "\n",
    "# Ler com Spark\n",
    "df = spark.read.csv(local_path, sep=\",\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "366ffdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- mfr: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- calories: integer (nullable = true)\n",
      " |-- protein: integer (nullable = true)\n",
      " |-- fat: integer (nullable = true)\n",
      " |-- sodium: integer (nullable = true)\n",
      " |-- fiber: double (nullable = true)\n",
      " |-- carbo: double (nullable = true)\n",
      " |-- sugars: integer (nullable = true)\n",
      " |-- potass: integer (nullable = true)\n",
      " |-- vitamins: integer (nullable = true)\n",
      " |-- shelf: integer (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- cups: double (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ver o schema da tabela \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "755b5121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+---------+\n",
      "|                name|mfr|   rating|\n",
      "+--------------------+---+---------+\n",
      "|           100% Bran|  N|68.402973|\n",
      "|   100% Natural Bran|  Q|33.983679|\n",
      "|            All-Bran|  K|59.425505|\n",
      "|All-Bran with Ext...|  K|93.704912|\n",
      "|      Almond Delight|  R|34.384843|\n",
      "|Apple Cinnamon Ch...|  G|29.509541|\n",
      "|         Apple Jacks|  K|33.174094|\n",
      "|             Basic 4|  G|37.038562|\n",
      "|           Bran Chex|  R|49.120253|\n",
      "|         Bran Flakes|  P|53.313813|\n",
      "|        Cap'n'Crunch|  Q|18.042851|\n",
      "|            Cheerios|  G|50.764999|\n",
      "|Cinnamon Toast Cr...|  G|19.823573|\n",
      "|            Clusters|  G|40.400208|\n",
      "|         Cocoa Puffs|  G|22.736446|\n",
      "|           Corn Chex|  R|41.445019|\n",
      "|         Corn Flakes|  K|45.863324|\n",
      "|           Corn Pops|  K|35.782791|\n",
      "|       Count Chocula|  G|22.396513|\n",
      "|  Cracklin' Oat Bran|  K|40.448772|\n",
      "+--------------------+---+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Selecionar as colunas \n",
    "df.select(\"name\",\"mfr\",\"rating\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fa0264ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+---------+\n",
      "|                name|mfr|   rating|\n",
      "+--------------------+---+---------+\n",
      "|           100% Bran|  N|68.402973|\n",
      "|   100% Natural Bran|  Q|33.983679|\n",
      "|            All-Bran|  K|59.425505|\n",
      "|All-Bran with Ext...|  K|93.704912|\n",
      "|      Almond Delight|  R|34.384843|\n",
      "|Apple Cinnamon Ch...|  G|29.509541|\n",
      "|         Apple Jacks|  K|33.174094|\n",
      "|             Basic 4|  G|37.038562|\n",
      "|           Bran Chex|  R|49.120253|\n",
      "|         Bran Flakes|  P|53.313813|\n",
      "|        Cap'n'Crunch|  Q|18.042851|\n",
      "|            Cheerios|  G|50.764999|\n",
      "|Cinnamon Toast Cr...|  G|19.823573|\n",
      "|            Clusters|  G|40.400208|\n",
      "|         Cocoa Puffs|  G|22.736446|\n",
      "|           Corn Chex|  R|41.445019|\n",
      "|         Corn Flakes|  K|45.863324|\n",
      "|           Corn Pops|  K|35.782791|\n",
      "|       Count Chocula|  G|22.396513|\n",
      "|  Cracklin' Oat Bran|  K|40.448772|\n",
      "+--------------------+---+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Outra alternativa para o select \n",
    "df.select(*[\"name\",\"mfr\",\"rating\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b43b6467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- mfr: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- Calories: integer (nullable = true)\n",
      " |-- protein: integer (nullable = true)\n",
      " |-- fat: integer (nullable = true)\n",
      " |-- sodium: integer (nullable = true)\n",
      " |-- fiber: double (nullable = true)\n",
      " |-- carbo: double (nullable = true)\n",
      " |-- sugars: integer (nullable = true)\n",
      " |-- potass: integer (nullable = true)\n",
      " |-- vitamins: integer (nullable = true)\n",
      " |-- shelf: integer (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- cups: double (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "O comando df.withColumn(\"Calories\",df['calories'].cast(\"Integer\")).printSchema() realiza duas operações encadeadas: primeiro, modifica o DataFrame e, em seguida, exibe a estrutura resultante.\n",
    "\n",
    "Parte 1: df['calories'].cast(\"Integer\")\n",
    "Esta é a operação mais interna e define o conteúdo da nova coluna.\n",
    "\n",
    "df['calories']: Aqui, você está selecionando a coluna que já existe no seu DataFrame df com o nome \"calories\" (provavelmente com 'c' minúsculo). O resultado é um objeto do tipo Column.\n",
    "\n",
    ".cast(\"Integer\"): Este método é aplicado à coluna \"calories\". A função .cast() é usada para converter o tipo de dado de uma coluna. \n",
    "Neste caso, ele tenta converter todos os valores da coluna \"calories\" para o tipo Integer (número inteiro). Isso é útil, \n",
    "por exemplo, se a coluna foi lida de um arquivo como texto (string), mas você sabe que ela representa valores numéricos.\n",
    "\n",
    "Parte 2: df.withColumn(\"Calories\", ...)\n",
    "Esta é a principal função de transformação do DataFrame.\n",
    "\n",
    "df.withColumn(...): Este método é usado para adicionar uma nova coluna a um DataFrame ou substituir uma coluna existente com o mesmo nome.\n",
    "É importante notar que, por causa da imutabilidade do Spark, ele não altera o DataFrame df original; ele retorna um novo DataFrame com a modificação.\n",
    "\n",
    "\"Calories\": Este é o primeiro argumento e define o nome da nova coluna. Note que aqui foi usado \"Calories\" com 'C' maiúsculo.\n",
    "\n",
    "df['calories'].cast(\"Integer\"): Este é o segundo argumento, que define os valores que a nova coluna terá. Como vimos na Parte 1, são os valores da coluna antiga \"calories\" convertidos para inteiro.\n",
    "\n",
    "Comportamento importante:\n",
    "\n",
    "Se já existir uma coluna chamada \"Calories\" (com 'C' maiúsculo) no df, ela será sobrescrita com os novos valores.\n",
    "\n",
    "Se não existir, uma nova coluna chamada \"Calories\" será adicionada ao DataFrame.\n",
    "\n",
    "Parte 3: .printSchema()\n",
    "Esta é uma ação, um comando que instrui o Spark a executar as transformações e mostrar um resultado.\n",
    "\n",
    ".printSchema(): Este método exibe a \"planta\" ou esquema (schema) do DataFrame. Ele mostra uma lista de todas as colunas,\n",
    "seus respectivos tipos de dados (ex: string, integer, double) e se elas podem ou não conter valores nulos (nullable).\n",
    "\n",
    "Juntando Tudo\n",
    "O fluxo de execução é o seguinte:\n",
    "\n",
    "O Spark pega o DataFrame original df.\n",
    "\n",
    "Ele cria um novo DataFrame adicionando uma coluna chamada \"Calories\".\n",
    "\n",
    "O conteúdo dessa nova coluna é obtido pegando a coluna existente \"calories\" e convertendo seus valores para o tipo Integer.\n",
    "\n",
    "Finalmente, o método .printSchema() é chamado sobre este novo DataFrame modificado, imprimindo sua estrutura no console. Você verá a lista de colunas, e a coluna \"Calories\" aparecerá com o tipo integer.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Alterar o nome da coluna e o tipo de dados\n",
    "df.withColumn(\"Calories\",df['calories'].cast(\"Integer\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a55bdc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-----+\n",
      "|                name|calories|count|\n",
      "+--------------------+--------+-----+\n",
      "|Just Right Fruit ...|     140|    1|\n",
      "|         Raisin Bran|     120|    1|\n",
      "|Shredded Wheat sp...|      90|    1|\n",
      "|           Corn Pops|     110|    1|\n",
      "|  Honey Nut Cheerios|     110|    1|\n",
      "|Muesli Raisins; D...|     150|    1|\n",
      "|      Fruity Pebbles|     110|    1|\n",
      "|           100% Bran|      70|    1|\n",
      "|       Fruitful Bran|     120|    1|\n",
      "|         Puffed Rice|      50|    1|\n",
      "|      Raisin Squares|      90|    1|\n",
      "|   Total Raisin Bran|     140|    1|\n",
      "|      Golden Grahams|     110|    1|\n",
      "|   Nutri-grain Wheat|      90|    1|\n",
      "|   100% Natural Bran|     120|    1|\n",
      "|Apple Cinnamon Ch...|     110|    1|\n",
      "|Mueslix Crispy Blend|     160|    1|\n",
      "|Shredded Wheat 'n...|      90|    1|\n",
      "|              Smacks|     110|    1|\n",
      "|      Quaker Oatmeal|     100|    1|\n",
      "+--------------------+--------+-----+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "#Group by dos dados por name\n",
    "df.groupBy(\"name\",\"calories\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "27c04273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|calories|count|\n",
      "+--------+-----+\n",
      "|     140|    3|\n",
      "|     120|   10|\n",
      "|     100|   17|\n",
      "|     130|    2|\n",
      "|      50|    3|\n",
      "|      80|    1|\n",
      "|     160|    1|\n",
      "|      70|    2|\n",
      "|      90|    7|\n",
      "|     110|   29|\n",
      "|     150|    2|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Group by dos dados por calories\n",
    "df.groupBy(\"calories\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ba0941bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|                name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|   rating|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|        Cap'n'Crunch|  Q|   C|     120|      1|  2|   220|  0.0| 12.0|    12|    35|      25|    2|   1.0|0.75|18.042851|\n",
      "|Cinnamon Toast Cr...|  G|   C|     120|      1|  3|   210|  0.0| 13.0|     9|    45|      25|    2|   1.0|0.75|19.823573|\n",
      "|         Cocoa Puffs|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    55|      25|    2|   1.0| 1.0|22.736446|\n",
      "|           Corn Pops|  K|   C|     110|      1|  0|    90|  1.0| 13.0|    12|    20|      25|    2|   1.0| 1.0|35.782791|\n",
      "|       Count Chocula|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    65|      25|    2|   1.0| 1.0|22.396513|\n",
      "|      Frosted Flakes|  K|   C|     110|      1|  0|   200|  1.0| 14.0|    11|    25|      25|    1|   1.0|0.75|31.435973|\n",
      "|      Fruity Pebbles|  P|   C|     110|      1|  1|   135|  0.0| 13.0|    12|    25|      25|    2|   1.0|0.75|28.025765|\n",
      "|      Golden Grahams|  G|   C|     110|      1|  1|   280|  0.0| 15.0|     9|    45|      25|    2|   1.0|0.75|23.804043|\n",
      "|    Honey Graham Ohs|  Q|   C|     120|      1|  2|   220|  1.0| 12.0|    11|    45|      25|    2|   1.0| 1.0|21.871292|\n",
      "|          Honey-comb|  P|   C|     110|      1|  0|   180|  0.0| 14.0|    11|    35|      25|    1|   1.0|1.33|28.742414|\n",
      "|         Puffed Rice|  Q|   C|      50|      1|  0|     0|  0.0| 13.0|     0|    15|       0|    3|   0.5| 1.0|60.756112|\n",
      "|           Rice Chex|  R|   C|     110|      1|  0|   240|  0.0| 23.0|     2|    30|      25|    1|   1.0|1.13|41.998933|\n",
      "|                Trix|  G|   C|     110|      1|  1|   140|  0.0| 13.0|    12|    25|      25|    2|   1.0| 1.0|27.753301|\n",
      "|         Froot Loops|  K|   C|     110|      2|  1|   125|  1.0| 11.0|    13|    30|      25|    2|   1.0| 1.0|32.207582|\n",
      "|Apple Cinnamon Ch...|  G|   C|     110|      2|  2|   180|  1.5| 10.5|    10|    70|      25|    1|   1.0|0.75|29.509541|\n",
      "|             Crispix|  K|   C|     110|      2|  0|   220|  1.0| 21.0|     3|    30|      25|    3|   1.0| 1.0|46.895644|\n",
      "|         Apple Jacks|  K|   C|     110|      2|  0|   125|  1.0| 11.0|    14|    30|      25|    2|   1.0| 1.0|33.174094|\n",
      "|         Corn Flakes|  K|   C|     100|      2|  0|   290|  1.0| 21.0|     2|    35|      25|    1|   1.0| 1.0|45.863324|\n",
      "|        Golden Crisp|  P|   C|     100|      2|  0|    45|  0.0| 11.0|    15|    40|      25|    1|   1.0|0.88|35.252444|\n",
      "|Crispy Wheat & Ra...|  G|   C|     100|      2|  1|   140|  2.0| 11.0|    10|   120|      25|    3|   1.0|0.75|36.176196|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "#Order by dos dados\n",
    "df.orderBy(\"protein\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "278e0d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|                name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|   rating|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|All-Bran with Ext...|  K|   C|      50|      4|  0|   140| 14.0|  8.0|     0|   330|      25|    3|   1.0| 0.5|93.704912|\n",
      "|         Puffed Rice|  Q|   C|      50|      1|  0|     0|  0.0| 13.0|     0|    15|       0|    3|   0.5| 1.0|60.756112|\n",
      "|        Puffed Wheat|  Q|   C|      50|      2|  0|     0|  1.0| 10.0|     0|    50|       0|    3|   0.5| 1.0|63.005645|\n",
      "|           100% Bran|  N|   C|      70|      4|  1|   130| 10.0|  5.0|     6|   280|      25|    3|   1.0|0.33|68.402973|\n",
      "|            All-Bran|  K|   C|      70|      4|  1|   260|  9.0|  7.0|     5|   320|      25|    3|   1.0|0.33|59.425505|\n",
      "|      Shredded Wheat|  N|   C|      80|      2|  0|     0|  3.0| 16.0|     0|    95|       0|    1|  0.83| 1.0|68.235885|\n",
      "|           Bran Chex|  R|   C|      90|      2|  1|   200|  4.0| 15.0|     6|   125|      25|    1|   1.0|0.67|49.120253|\n",
      "|         Bran Flakes|  P|   C|      90|      3|  0|   210|  5.0| 13.0|     5|   190|      25|    3|   1.0|0.67|53.313813|\n",
      "|   Nutri-grain Wheat|  K|   C|      90|      3|  0|   170|  3.0| 18.0|     2|    90|      25|    3|   1.0| 1.0|59.642837|\n",
      "|      Raisin Squares|  K|   C|      90|      2|  0|     0|  2.0| 15.0|     6|   110|      25|    3|   1.0| 0.5|55.333142|\n",
      "|Shredded Wheat 'n...|  N|   C|      90|      3|  0|     0|  4.0| 19.0|     0|   140|       0|    1|   1.0|0.67|74.472949|\n",
      "|Shredded Wheat sp...|  N|   C|      90|      3|  0|     0|  3.0| 20.0|     0|   120|       0|    1|   1.0|0.67|72.801787|\n",
      "|Strawberry Fruit ...|  N|   C|      90|      2|  0|    15|  3.0| 15.0|     5|    90|      25|    2|   1.0| 1.0|59.363993|\n",
      "|         Corn Flakes|  K|   C|     100|      2|  0|   290|  1.0| 21.0|     2|    35|      25|    1|   1.0| 1.0|45.863324|\n",
      "|Cream of Wheat (Q...|  N|   H|     100|      3|  0|    80|  1.0| 21.0|     0|    -1|       0|    2|   1.0| 1.0|64.533816|\n",
      "|Crispy Wheat & Ra...|  G|   C|     100|      2|  1|   140|  2.0| 11.0|    10|   120|      25|    3|   1.0|0.75|36.176196|\n",
      "|         Double Chex|  R|   C|     100|      2|  0|   190|  1.0| 18.0|     5|    80|      25|    3|   1.0|0.75|44.330856|\n",
      "| Frosted Mini-Wheats|  K|   C|     100|      3|  0|     0|  3.0| 14.0|     7|   100|      25|    2|   1.0| 0.8|58.345141|\n",
      "|        Golden Crisp|  P|   C|     100|      2|  0|    45|  0.0| 11.0|    15|    40|      25|    1|   1.0|0.88|35.252444|\n",
      "|   Grape Nuts Flakes|  P|   C|     100|      3|  1|   140|  3.0| 15.0|     5|    85|      25|    3|   1.0|0.88|52.076897|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "#Order by dos dados\n",
    "df.orderBy(\"calories\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f14aeb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----------------------------------------------------+\n",
      "|                name|vitamins|CASE WHEN (vitamins >= 25) THEN rich in vitamins END|\n",
      "+--------------------+--------+----------------------------------------------------+\n",
      "|           100% Bran|      25|                                    rich in vitamins|\n",
      "|   100% Natural Bran|       0|                                                NULL|\n",
      "|            All-Bran|      25|                                    rich in vitamins|\n",
      "|All-Bran with Ext...|      25|                                    rich in vitamins|\n",
      "|      Almond Delight|      25|                                    rich in vitamins|\n",
      "|Apple Cinnamon Ch...|      25|                                    rich in vitamins|\n",
      "|         Apple Jacks|      25|                                    rich in vitamins|\n",
      "|             Basic 4|      25|                                    rich in vitamins|\n",
      "|           Bran Chex|      25|                                    rich in vitamins|\n",
      "|         Bran Flakes|      25|                                    rich in vitamins|\n",
      "|        Cap'n'Crunch|      25|                                    rich in vitamins|\n",
      "|            Cheerios|      25|                                    rich in vitamins|\n",
      "|Cinnamon Toast Cr...|      25|                                    rich in vitamins|\n",
      "|            Clusters|      25|                                    rich in vitamins|\n",
      "|         Cocoa Puffs|      25|                                    rich in vitamins|\n",
      "|           Corn Chex|      25|                                    rich in vitamins|\n",
      "|         Corn Flakes|      25|                                    rich in vitamins|\n",
      "|           Corn Pops|      25|                                    rich in vitamins|\n",
      "|       Count Chocula|      25|                                    rich in vitamins|\n",
      "|  Cracklin' Oat Bran|      25|                                    rich in vitamins|\n",
      "+--------------------+--------+----------------------------------------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "#Usando o when (case when)\n",
    "df.select(\"name\",\"vitamins\", when(df.vitamins >= \"25\",\"rich in vitamins\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ab5ebb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-----------------+\n",
      "|                name|vitamins|vitamins_category|\n",
      "+--------------------+--------+-----------------+\n",
      "|           100% Bran|      25| rich in vitamins|\n",
      "|   100% Natural Bran|       0|             NULL|\n",
      "|            All-Bran|      25| rich in vitamins|\n",
      "|All-Bran with Ext...|      25| rich in vitamins|\n",
      "|      Almond Delight|      25| rich in vitamins|\n",
      "|Apple Cinnamon Ch...|      25| rich in vitamins|\n",
      "|         Apple Jacks|      25| rich in vitamins|\n",
      "|             Basic 4|      25| rich in vitamins|\n",
      "|           Bran Chex|      25| rich in vitamins|\n",
      "|         Bran Flakes|      25| rich in vitamins|\n",
      "|        Cap'n'Crunch|      25| rich in vitamins|\n",
      "|            Cheerios|      25| rich in vitamins|\n",
      "|Cinnamon Toast Cr...|      25| rich in vitamins|\n",
      "|            Clusters|      25| rich in vitamins|\n",
      "|         Cocoa Puffs|      25| rich in vitamins|\n",
      "|           Corn Chex|      25| rich in vitamins|\n",
      "|         Corn Flakes|      25| rich in vitamins|\n",
      "|           Corn Pops|      25| rich in vitamins|\n",
      "|       Count Chocula|      25| rich in vitamins|\n",
      "|  Cracklin' Oat Bran|      25| rich in vitamins|\n",
      "+--------------------+--------+-----------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "#Usando o when (case when) com alias\n",
    "df.select(\n",
    "    \"name\",\n",
    "    \"vitamins\",\n",
    "    when(df.vitamins >= 25, \"rich in vitamins\").alias(\"vitamins_category\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "993c047a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|                name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|   rating|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|         Corn Flakes|  K|   C|     100|      2|  0|   290|  1.0| 21.0|     2|    35|      25|    1|   1.0| 1.0|45.863324|\n",
      "|Cream of Wheat (Q...|  N|   H|     100|      3|  0|    80|  1.0| 21.0|     0|    -1|       0|    2|   1.0| 1.0|64.533816|\n",
      "|Crispy Wheat & Ra...|  G|   C|     100|      2|  1|   140|  2.0| 11.0|    10|   120|      25|    3|   1.0|0.75|36.176196|\n",
      "|         Double Chex|  R|   C|     100|      2|  0|   190|  1.0| 18.0|     5|    80|      25|    3|   1.0|0.75|44.330856|\n",
      "| Frosted Mini-Wheats|  K|   C|     100|      3|  0|     0|  3.0| 14.0|     7|   100|      25|    2|   1.0| 0.8|58.345141|\n",
      "|        Golden Crisp|  P|   C|     100|      2|  0|    45|  0.0| 11.0|    15|    40|      25|    1|   1.0|0.88|35.252444|\n",
      "|   Grape Nuts Flakes|  P|   C|     100|      3|  1|   140|  3.0| 15.0|     5|    85|      25|    3|   1.0|0.88|52.076897|\n",
      "|                Life|  Q|   C|     100|      4|  2|   150|  2.0| 12.0|     6|    95|      25|    2|   1.0|0.67|45.328074|\n",
      "|               Maypo|  A|   H|     100|      4|  1|     0|  0.0| 16.0|     3|    95|      25|    2|   1.0| 1.0|54.850917|\n",
      "|Multi-Grain Cheerios|  G|   C|     100|      2|  1|   220|  2.0| 15.0|     6|    90|      25|    1|   1.0| 1.0|40.105965|\n",
      "|          Product 19|  K|   C|     100|      3|  0|   320|  1.0| 20.0|     3|    45|     100|    3|   1.0| 1.0| 41.50354|\n",
      "|  Quaker Oat Squares|  Q|   C|     100|      4|  1|   135|  2.0| 14.0|     6|   110|      25|    3|   1.0| 0.5|49.511874|\n",
      "|      Quaker Oatmeal|  Q|   H|     100|      5|  2|     0|  2.7| -1.0|    -1|   110|       0|    1|   1.0|0.67|50.828392|\n",
      "|     Raisin Nut Bran|  G|   C|     100|      3|  2|   140|  2.5| 10.5|     8|   140|      25|    3|   1.0| 0.5|  39.7034|\n",
      "|   Total Whole Grain|  G|   C|     100|      3|  1|   200|  3.0| 16.0|     3|   110|     100|    3|   1.0| 1.0|46.658844|\n",
      "|          Wheat Chex|  R|   C|     100|      3|  1|   230|  3.0| 17.0|     3|   115|      25|    1|   1.0|0.67|49.787445|\n",
      "|            Wheaties|  G|   C|     100|      3|  1|   200|  3.0| 17.0|     3|   110|      25|    1|   1.0| 1.0|51.592193|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Realizando operação de filtro\n",
    "df.filter(df.calories ==\"100\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "89242163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|                name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|   rating|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|   100% Natural Bran|  Q|   C|     120|      3|  5|    15|  2.0|  8.0|     8|   135|       0|    3|   1.0| 1.0|33.983679|\n",
      "|      Almond Delight|  R|   C|     110|      2|  2|   200|  1.0| 14.0|     8|    -1|      25|    3|   1.0|0.75|34.384843|\n",
      "|Apple Cinnamon Ch...|  G|   C|     110|      2|  2|   180|  1.5| 10.5|    10|    70|      25|    1|   1.0|0.75|29.509541|\n",
      "|         Apple Jacks|  K|   C|     110|      2|  0|   125|  1.0| 11.0|    14|    30|      25|    2|   1.0| 1.0|33.174094|\n",
      "|             Basic 4|  G|   C|     130|      3|  2|   210|  2.0| 18.0|     8|   100|      25|    3|  1.33|0.75|37.038562|\n",
      "|        Cap'n'Crunch|  Q|   C|     120|      1|  2|   220|  0.0| 12.0|    12|    35|      25|    2|   1.0|0.75|18.042851|\n",
      "|            Cheerios|  G|   C|     110|      6|  2|   290|  2.0| 17.0|     1|   105|      25|    1|   1.0|1.25|50.764999|\n",
      "|Cinnamon Toast Cr...|  G|   C|     120|      1|  3|   210|  0.0| 13.0|     9|    45|      25|    2|   1.0|0.75|19.823573|\n",
      "|            Clusters|  G|   C|     110|      3|  2|   140|  2.0| 13.0|     7|   105|      25|    3|   1.0| 0.5|40.400208|\n",
      "|         Cocoa Puffs|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    55|      25|    2|   1.0| 1.0|22.736446|\n",
      "|           Corn Chex|  R|   C|     110|      2|  0|   280|  0.0| 22.0|     3|    25|      25|    1|   1.0| 1.0|41.445019|\n",
      "|         Corn Flakes|  K|   C|     100|      2|  0|   290|  1.0| 21.0|     2|    35|      25|    1|   1.0| 1.0|45.863324|\n",
      "|           Corn Pops|  K|   C|     110|      1|  0|    90|  1.0| 13.0|    12|    20|      25|    2|   1.0| 1.0|35.782791|\n",
      "|       Count Chocula|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    65|      25|    2|   1.0| 1.0|22.396513|\n",
      "|  Cracklin' Oat Bran|  K|   C|     110|      3|  3|   140|  4.0| 10.0|     7|   160|      25|    3|   1.0| 0.5|40.448772|\n",
      "|Cream of Wheat (Q...|  N|   H|     100|      3|  0|    80|  1.0| 21.0|     0|    -1|       0|    2|   1.0| 1.0|64.533816|\n",
      "|             Crispix|  K|   C|     110|      2|  0|   220|  1.0| 21.0|     3|    30|      25|    3|   1.0| 1.0|46.895644|\n",
      "|Crispy Wheat & Ra...|  G|   C|     100|      2|  1|   140|  2.0| 11.0|    10|   120|      25|    3|   1.0|0.75|36.176196|\n",
      "|         Double Chex|  R|   C|     100|      2|  0|   190|  1.0| 18.0|     5|    80|      25|    3|   1.0|0.75|44.330856|\n",
      "|         Froot Loops|  K|   C|     110|      2|  1|   125|  1.0| 11.0|    13|    30|      25|    2|   1.0| 1.0|32.207582|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "#Realizando operação de filtro\n",
    "df.filter(df.calories >=\"100\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bd4c7d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|                name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|   rating|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|           100% Bran|  N|   C|      70|      4|  1|   130| 10.0|  5.0|     6|   280|      25|    3|   1.0|0.33|68.402973|\n",
      "|   100% Natural Bran|  Q|   C|     120|      3|  5|    15|  2.0|  8.0|     8|   135|       0|    3|   1.0| 1.0|33.983679|\n",
      "|            All-Bran|  K|   C|      70|      4|  1|   260|  9.0|  7.0|     5|   320|      25|    3|   1.0|0.33|59.425505|\n",
      "|All-Bran with Ext...|  K|   C|      50|      4|  0|   140| 14.0|  8.0|     0|   330|      25|    3|   1.0| 0.5|93.704912|\n",
      "|      Almond Delight|  R|   C|     110|      2|  2|   200|  1.0| 14.0|     8|    -1|      25|    3|   1.0|0.75|34.384843|\n",
      "|Apple Cinnamon Ch...|  G|   C|     110|      2|  2|   180|  1.5| 10.5|    10|    70|      25|    1|   1.0|0.75|29.509541|\n",
      "|         Apple Jacks|  K|   C|     110|      2|  0|   125|  1.0| 11.0|    14|    30|      25|    2|   1.0| 1.0|33.174094|\n",
      "|             Basic 4|  G|   C|     130|      3|  2|   210|  2.0| 18.0|     8|   100|      25|    3|  1.33|0.75|37.038562|\n",
      "|           Bran Chex|  R|   C|      90|      2|  1|   200|  4.0| 15.0|     6|   125|      25|    1|   1.0|0.67|49.120253|\n",
      "|         Bran Flakes|  P|   C|      90|      3|  0|   210|  5.0| 13.0|     5|   190|      25|    3|   1.0|0.67|53.313813|\n",
      "|        Cap'n'Crunch|  Q|   C|     120|      1|  2|   220|  0.0| 12.0|    12|    35|      25|    2|   1.0|0.75|18.042851|\n",
      "|            Cheerios|  G|   C|     110|      6|  2|   290|  2.0| 17.0|     1|   105|      25|    1|   1.0|1.25|50.764999|\n",
      "|Cinnamon Toast Cr...|  G|   C|     120|      1|  3|   210|  0.0| 13.0|     9|    45|      25|    2|   1.0|0.75|19.823573|\n",
      "|            Clusters|  G|   C|     110|      3|  2|   140|  2.0| 13.0|     7|   105|      25|    3|   1.0| 0.5|40.400208|\n",
      "|         Cocoa Puffs|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    55|      25|    2|   1.0| 1.0|22.736446|\n",
      "|           Corn Chex|  R|   C|     110|      2|  0|   280|  0.0| 22.0|     3|    25|      25|    1|   1.0| 1.0|41.445019|\n",
      "|         Corn Flakes|  K|   C|     100|      2|  0|   290|  1.0| 21.0|     2|    35|      25|    1|   1.0| 1.0|45.863324|\n",
      "|           Corn Pops|  K|   C|     110|      1|  0|    90|  1.0| 13.0|    12|    20|      25|    2|   1.0| 1.0|35.782791|\n",
      "|       Count Chocula|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    65|      25|    2|   1.0| 1.0|22.396513|\n",
      "|  Cracklin' Oat Bran|  K|   C|     110|      3|  3|   140|  4.0| 10.0|     7|   160|      25|    3|   1.0| 0.5|40.448772|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "#Filtro usando o is not null \n",
    "df.filter(df.name.isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8bc9f39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+------+\n",
      "|name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|rating|\n",
      "+----+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+------+\n",
      "+----+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Filtro usando o is null \n",
    "df.filter(df.name.isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17a17a",
   "metadata": {},
   "source": [
    "#### Aula 3 - Consultas e Seleções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c3e3ad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "77a00750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Status|\n",
      "+------+\n",
      "|    OK|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Validando o sql\n",
    "df = spark.sql(''' select 'OK' as Status''')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e2bc2ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|                name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|   rating|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|           100% Bran|  N|   C|      70|      4|  1|   130| 10.0|  5.0|     6|   280|      25|    3|   1.0|0.33|68.402973|\n",
      "|   100% Natural Bran|  Q|   C|     120|      3|  5|    15|  2.0|  8.0|     8|   135|       0|    3|   1.0| 1.0|33.983679|\n",
      "|            All-Bran|  K|   C|      70|      4|  1|   260|  9.0|  7.0|     5|   320|      25|    3|   1.0|0.33|59.425505|\n",
      "|All-Bran with Ext...|  K|   C|      50|      4|  0|   140| 14.0|  8.0|     0|   330|      25|    3|   1.0| 0.5|93.704912|\n",
      "|      Almond Delight|  R|   C|     110|      2|  2|   200|  1.0| 14.0|     8|    -1|      25|    3|   1.0|0.75|34.384843|\n",
      "|Apple Cinnamon Ch...|  G|   C|     110|      2|  2|   180|  1.5| 10.5|    10|    70|      25|    1|   1.0|0.75|29.509541|\n",
      "|         Apple Jacks|  K|   C|     110|      2|  0|   125|  1.0| 11.0|    14|    30|      25|    2|   1.0| 1.0|33.174094|\n",
      "|             Basic 4|  G|   C|     130|      3|  2|   210|  2.0| 18.0|     8|   100|      25|    3|  1.33|0.75|37.038562|\n",
      "|           Bran Chex|  R|   C|      90|      2|  1|   200|  4.0| 15.0|     6|   125|      25|    1|   1.0|0.67|49.120253|\n",
      "|         Bran Flakes|  P|   C|      90|      3|  0|   210|  5.0| 13.0|     5|   190|      25|    3|   1.0|0.67|53.313813|\n",
      "|        Cap'n'Crunch|  Q|   C|     120|      1|  2|   220|  0.0| 12.0|    12|    35|      25|    2|   1.0|0.75|18.042851|\n",
      "|            Cheerios|  G|   C|     110|      6|  2|   290|  2.0| 17.0|     1|   105|      25|    1|   1.0|1.25|50.764999|\n",
      "|Cinnamon Toast Cr...|  G|   C|     120|      1|  3|   210|  0.0| 13.0|     9|    45|      25|    2|   1.0|0.75|19.823573|\n",
      "|            Clusters|  G|   C|     110|      3|  2|   140|  2.0| 13.0|     7|   105|      25|    3|   1.0| 0.5|40.400208|\n",
      "|         Cocoa Puffs|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    55|      25|    2|   1.0| 1.0|22.736446|\n",
      "|           Corn Chex|  R|   C|     110|      2|  0|   280|  0.0| 22.0|     3|    25|      25|    1|   1.0| 1.0|41.445019|\n",
      "|         Corn Flakes|  K|   C|     100|      2|  0|   290|  1.0| 21.0|     2|    35|      25|    1|   1.0| 1.0|45.863324|\n",
      "|           Corn Pops|  K|   C|     110|      1|  0|    90|  1.0| 13.0|    12|    20|      25|    2|   1.0| 1.0|35.782791|\n",
      "|       Count Chocula|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    65|      25|    2|   1.0| 1.0|22.396513|\n",
      "|  Cracklin' Oat Bran|  K|   C|     110|      3|  3|   140|  4.0| 10.0|     7|   160|      25|    3|   1.0| 0.5|40.448772|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Baixar o arquivo pois não conseguimos usar o Spark para ler diretamnte o arquivo no Github\n",
    "url = \"https://raw.githubusercontent.com/FIAP/Pos_Tech_DTAT/refs/heads/Framework-de-Big-Data/Aula%202/cereal.csv\"\n",
    "local_path = \"cereal.csv\"\n",
    "urllib.request.urlretrieve(url, local_path)\n",
    "\n",
    "# Ler com Spark\n",
    "df = spark.read.csv(local_path, sep=\",\", header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f02931f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma tabela temporaria na sessão atual\n",
    "df.createOrReplaceTempView(\"cereal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bed5855e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|                name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|   rating|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|           100% Bran|  N|   C|      70|      4|  1|   130| 10.0|  5.0|     6|   280|      25|    3|   1.0|0.33|68.402973|\n",
      "|   100% Natural Bran|  Q|   C|     120|      3|  5|    15|  2.0|  8.0|     8|   135|       0|    3|   1.0| 1.0|33.983679|\n",
      "|            All-Bran|  K|   C|      70|      4|  1|   260|  9.0|  7.0|     5|   320|      25|    3|   1.0|0.33|59.425505|\n",
      "|All-Bran with Ext...|  K|   C|      50|      4|  0|   140| 14.0|  8.0|     0|   330|      25|    3|   1.0| 0.5|93.704912|\n",
      "|      Almond Delight|  R|   C|     110|      2|  2|   200|  1.0| 14.0|     8|    -1|      25|    3|   1.0|0.75|34.384843|\n",
      "|Apple Cinnamon Ch...|  G|   C|     110|      2|  2|   180|  1.5| 10.5|    10|    70|      25|    1|   1.0|0.75|29.509541|\n",
      "|         Apple Jacks|  K|   C|     110|      2|  0|   125|  1.0| 11.0|    14|    30|      25|    2|   1.0| 1.0|33.174094|\n",
      "|             Basic 4|  G|   C|     130|      3|  2|   210|  2.0| 18.0|     8|   100|      25|    3|  1.33|0.75|37.038562|\n",
      "|           Bran Chex|  R|   C|      90|      2|  1|   200|  4.0| 15.0|     6|   125|      25|    1|   1.0|0.67|49.120253|\n",
      "|         Bran Flakes|  P|   C|      90|      3|  0|   210|  5.0| 13.0|     5|   190|      25|    3|   1.0|0.67|53.313813|\n",
      "|        Cap'n'Crunch|  Q|   C|     120|      1|  2|   220|  0.0| 12.0|    12|    35|      25|    2|   1.0|0.75|18.042851|\n",
      "|            Cheerios|  G|   C|     110|      6|  2|   290|  2.0| 17.0|     1|   105|      25|    1|   1.0|1.25|50.764999|\n",
      "|Cinnamon Toast Cr...|  G|   C|     120|      1|  3|   210|  0.0| 13.0|     9|    45|      25|    2|   1.0|0.75|19.823573|\n",
      "|            Clusters|  G|   C|     110|      3|  2|   140|  2.0| 13.0|     7|   105|      25|    3|   1.0| 0.5|40.400208|\n",
      "|         Cocoa Puffs|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    55|      25|    2|   1.0| 1.0|22.736446|\n",
      "|           Corn Chex|  R|   C|     110|      2|  0|   280|  0.0| 22.0|     3|    25|      25|    1|   1.0| 1.0|41.445019|\n",
      "|         Corn Flakes|  K|   C|     100|      2|  0|   290|  1.0| 21.0|     2|    35|      25|    1|   1.0| 1.0|45.863324|\n",
      "|           Corn Pops|  K|   C|     110|      1|  0|    90|  1.0| 13.0|    12|    20|      25|    2|   1.0| 1.0|35.782791|\n",
      "|       Count Chocula|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    65|      25|    2|   1.0| 1.0|22.396513|\n",
      "|  Cracklin' Oat Bran|  K|   C|     110|      3|  3|   140|  4.0| 10.0|     7|   160|      25|    3|   1.0| 0.5|40.448772|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Realizar o select via sql\n",
    "cereal = spark.sql(''' SELECT * FROM cereal ''')\n",
    "cereal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b4af1dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|                name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|   rating|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|           100% Bran|  N|   C|      70|      4|  1|   130| 10.0|  5.0|     6|   280|      25|    3|   1.0|0.33|68.402973|\n",
      "|   100% Natural Bran|  Q|   C|     120|      3|  5|    15|  2.0|  8.0|     8|   135|       0|    3|   1.0| 1.0|33.983679|\n",
      "|            All-Bran|  K|   C|      70|      4|  1|   260|  9.0|  7.0|     5|   320|      25|    3|   1.0|0.33|59.425505|\n",
      "|All-Bran with Ext...|  K|   C|      50|      4|  0|   140| 14.0|  8.0|     0|   330|      25|    3|   1.0| 0.5|93.704912|\n",
      "|      Almond Delight|  R|   C|     110|      2|  2|   200|  1.0| 14.0|     8|    -1|      25|    3|   1.0|0.75|34.384843|\n",
      "|Apple Cinnamon Ch...|  G|   C|     110|      2|  2|   180|  1.5| 10.5|    10|    70|      25|    1|   1.0|0.75|29.509541|\n",
      "|         Apple Jacks|  K|   C|     110|      2|  0|   125|  1.0| 11.0|    14|    30|      25|    2|   1.0| 1.0|33.174094|\n",
      "|             Basic 4|  G|   C|     130|      3|  2|   210|  2.0| 18.0|     8|   100|      25|    3|  1.33|0.75|37.038562|\n",
      "|           Bran Chex|  R|   C|      90|      2|  1|   200|  4.0| 15.0|     6|   125|      25|    1|   1.0|0.67|49.120253|\n",
      "|         Bran Flakes|  P|   C|      90|      3|  0|   210|  5.0| 13.0|     5|   190|      25|    3|   1.0|0.67|53.313813|\n",
      "|        Cap'n'Crunch|  Q|   C|     120|      1|  2|   220|  0.0| 12.0|    12|    35|      25|    2|   1.0|0.75|18.042851|\n",
      "|            Cheerios|  G|   C|     110|      6|  2|   290|  2.0| 17.0|     1|   105|      25|    1|   1.0|1.25|50.764999|\n",
      "|Cinnamon Toast Cr...|  G|   C|     120|      1|  3|   210|  0.0| 13.0|     9|    45|      25|    2|   1.0|0.75|19.823573|\n",
      "|            Clusters|  G|   C|     110|      3|  2|   140|  2.0| 13.0|     7|   105|      25|    3|   1.0| 0.5|40.400208|\n",
      "|         Cocoa Puffs|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    55|      25|    2|   1.0| 1.0|22.736446|\n",
      "|           Corn Chex|  R|   C|     110|      2|  0|   280|  0.0| 22.0|     3|    25|      25|    1|   1.0| 1.0|41.445019|\n",
      "|         Corn Flakes|  K|   C|     100|      2|  0|   290|  1.0| 21.0|     2|    35|      25|    1|   1.0| 1.0|45.863324|\n",
      "|           Corn Pops|  K|   C|     110|      1|  0|    90|  1.0| 13.0|    12|    20|      25|    2|   1.0| 1.0|35.782791|\n",
      "|       Count Chocula|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    65|      25|    2|   1.0| 1.0|22.396513|\n",
      "|  Cracklin' Oat Bran|  K|   C|     110|      3|  3|   140|  4.0| 10.0|     7|   160|      25|    3|   1.0| 0.5|40.448772|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Diferença entre o df.show e cereal.show() é o que df.show é via Python e o cereal.show() é via SQL\n",
    "# As duas formas utiliza o mesmo motor que é o Spark\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9bdb8ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|                name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|   rating|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|           100% Bran|  N|   C|      70|      4|  1|   130| 10.0|  5.0|     6|   280|      25|    3|   1.0|0.33|68.402973|\n",
      "|   100% Natural Bran|  Q|   C|     120|      3|  5|    15|  2.0|  8.0|     8|   135|       0|    3|   1.0| 1.0|33.983679|\n",
      "|            All-Bran|  K|   C|      70|      4|  1|   260|  9.0|  7.0|     5|   320|      25|    3|   1.0|0.33|59.425505|\n",
      "|All-Bran with Ext...|  K|   C|      50|      4|  0|   140| 14.0|  8.0|     0|   330|      25|    3|   1.0| 0.5|93.704912|\n",
      "|      Almond Delight|  R|   C|     110|      2|  2|   200|  1.0| 14.0|     8|    -1|      25|    3|   1.0|0.75|34.384843|\n",
      "|Apple Cinnamon Ch...|  G|   C|     110|      2|  2|   180|  1.5| 10.5|    10|    70|      25|    1|   1.0|0.75|29.509541|\n",
      "|         Apple Jacks|  K|   C|     110|      2|  0|   125|  1.0| 11.0|    14|    30|      25|    2|   1.0| 1.0|33.174094|\n",
      "|             Basic 4|  G|   C|     130|      3|  2|   210|  2.0| 18.0|     8|   100|      25|    3|  1.33|0.75|37.038562|\n",
      "|           Bran Chex|  R|   C|      90|      2|  1|   200|  4.0| 15.0|     6|   125|      25|    1|   1.0|0.67|49.120253|\n",
      "|         Bran Flakes|  P|   C|      90|      3|  0|   210|  5.0| 13.0|     5|   190|      25|    3|   1.0|0.67|53.313813|\n",
      "|        Cap'n'Crunch|  Q|   C|     120|      1|  2|   220|  0.0| 12.0|    12|    35|      25|    2|   1.0|0.75|18.042851|\n",
      "|            Cheerios|  G|   C|     110|      6|  2|   290|  2.0| 17.0|     1|   105|      25|    1|   1.0|1.25|50.764999|\n",
      "|Cinnamon Toast Cr...|  G|   C|     120|      1|  3|   210|  0.0| 13.0|     9|    45|      25|    2|   1.0|0.75|19.823573|\n",
      "|            Clusters|  G|   C|     110|      3|  2|   140|  2.0| 13.0|     7|   105|      25|    3|   1.0| 0.5|40.400208|\n",
      "|         Cocoa Puffs|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    55|      25|    2|   1.0| 1.0|22.736446|\n",
      "|           Corn Chex|  R|   C|     110|      2|  0|   280|  0.0| 22.0|     3|    25|      25|    1|   1.0| 1.0|41.445019|\n",
      "|         Corn Flakes|  K|   C|     100|      2|  0|   290|  1.0| 21.0|     2|    35|      25|    1|   1.0| 1.0|45.863324|\n",
      "|           Corn Pops|  K|   C|     110|      1|  0|    90|  1.0| 13.0|    12|    20|      25|    2|   1.0| 1.0|35.782791|\n",
      "|       Count Chocula|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    65|      25|    2|   1.0| 1.0|22.396513|\n",
      "|  Cracklin' Oat Bran|  K|   C|     110|      3|  3|   140|  4.0| 10.0|     7|   160|      25|    3|   1.0| 0.5|40.448772|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Realizar o select via sql com filtro\n",
    "cereal = spark.sql(''' SELECT * FROM cereal WHERE TYPE = 'C' ''')\n",
    "cereal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "50bb59ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|                name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|   rating|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|           100% Bran|  N|   C|      70|      4|  1|   130| 10.0|  5.0|     6|   280|      25|    3|   1.0|0.33|68.402973|\n",
      "|   100% Natural Bran|  Q|   C|     120|      3|  5|    15|  2.0|  8.0|     8|   135|       0|    3|   1.0| 1.0|33.983679|\n",
      "|            All-Bran|  K|   C|      70|      4|  1|   260|  9.0|  7.0|     5|   320|      25|    3|   1.0|0.33|59.425505|\n",
      "|All-Bran with Ext...|  K|   C|      50|      4|  0|   140| 14.0|  8.0|     0|   330|      25|    3|   1.0| 0.5|93.704912|\n",
      "|      Almond Delight|  R|   C|     110|      2|  2|   200|  1.0| 14.0|     8|    -1|      25|    3|   1.0|0.75|34.384843|\n",
      "|Apple Cinnamon Ch...|  G|   C|     110|      2|  2|   180|  1.5| 10.5|    10|    70|      25|    1|   1.0|0.75|29.509541|\n",
      "|         Apple Jacks|  K|   C|     110|      2|  0|   125|  1.0| 11.0|    14|    30|      25|    2|   1.0| 1.0|33.174094|\n",
      "|             Basic 4|  G|   C|     130|      3|  2|   210|  2.0| 18.0|     8|   100|      25|    3|  1.33|0.75|37.038562|\n",
      "|           Bran Chex|  R|   C|      90|      2|  1|   200|  4.0| 15.0|     6|   125|      25|    1|   1.0|0.67|49.120253|\n",
      "|         Bran Flakes|  P|   C|      90|      3|  0|   210|  5.0| 13.0|     5|   190|      25|    3|   1.0|0.67|53.313813|\n",
      "|        Cap'n'Crunch|  Q|   C|     120|      1|  2|   220|  0.0| 12.0|    12|    35|      25|    2|   1.0|0.75|18.042851|\n",
      "|            Cheerios|  G|   C|     110|      6|  2|   290|  2.0| 17.0|     1|   105|      25|    1|   1.0|1.25|50.764999|\n",
      "|Cinnamon Toast Cr...|  G|   C|     120|      1|  3|   210|  0.0| 13.0|     9|    45|      25|    2|   1.0|0.75|19.823573|\n",
      "|            Clusters|  G|   C|     110|      3|  2|   140|  2.0| 13.0|     7|   105|      25|    3|   1.0| 0.5|40.400208|\n",
      "|         Cocoa Puffs|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    55|      25|    2|   1.0| 1.0|22.736446|\n",
      "|           Corn Chex|  R|   C|     110|      2|  0|   280|  0.0| 22.0|     3|    25|      25|    1|   1.0| 1.0|41.445019|\n",
      "|         Corn Flakes|  K|   C|     100|      2|  0|   290|  1.0| 21.0|     2|    35|      25|    1|   1.0| 1.0|45.863324|\n",
      "|           Corn Pops|  K|   C|     110|      1|  0|    90|  1.0| 13.0|    12|    20|      25|    2|   1.0| 1.0|35.782791|\n",
      "|       Count Chocula|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    65|      25|    2|   1.0| 1.0|22.396513|\n",
      "|  Cracklin' Oat Bran|  K|   C|     110|      3|  3|   140|  4.0| 10.0|     7|   160|      25|    3|   1.0| 0.5|40.448772|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Realizar o filtro via Python\n",
    "df.where(df['type'] == 'C').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b030f0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|                name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|   rating|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|Apple Cinnamon Ch...|  G|   C|     110|      2|  2|   180|  1.5| 10.5|    10|    70|      25|    1|   1.0|0.75|29.509541|\n",
      "|             Basic 4|  G|   C|     130|      3|  2|   210|  2.0| 18.0|     8|   100|      25|    3|  1.33|0.75|37.038562|\n",
      "|            Cheerios|  G|   C|     110|      6|  2|   290|  2.0| 17.0|     1|   105|      25|    1|   1.0|1.25|50.764999|\n",
      "|Cinnamon Toast Cr...|  G|   C|     120|      1|  3|   210|  0.0| 13.0|     9|    45|      25|    2|   1.0|0.75|19.823573|\n",
      "|            Clusters|  G|   C|     110|      3|  2|   140|  2.0| 13.0|     7|   105|      25|    3|   1.0| 0.5|40.400208|\n",
      "|         Cocoa Puffs|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    55|      25|    2|   1.0| 1.0|22.736446|\n",
      "|       Count Chocula|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    65|      25|    2|   1.0| 1.0|22.396513|\n",
      "|Crispy Wheat & Ra...|  G|   C|     100|      2|  1|   140|  2.0| 11.0|    10|   120|      25|    3|   1.0|0.75|36.176196|\n",
      "|      Golden Grahams|  G|   C|     110|      1|  1|   280|  0.0| 15.0|     9|    45|      25|    2|   1.0|0.75|23.804043|\n",
      "|  Honey Nut Cheerios|  G|   C|     110|      3|  1|   250|  1.5| 11.5|    10|    90|      25|    1|   1.0|0.75|31.072217|\n",
      "|                 Kix|  G|   C|     110|      2|  1|   260|  0.0| 21.0|     3|    40|      25|    2|   1.0| 1.5|39.241114|\n",
      "|        Lucky Charms|  G|   C|     110|      2|  1|   180|  0.0| 12.0|    12|    55|      25|    2|   1.0| 1.0|26.734515|\n",
      "|Multi-Grain Cheerios|  G|   C|     100|      2|  1|   220|  2.0| 15.0|     6|    90|      25|    1|   1.0| 1.0|40.105965|\n",
      "|Oatmeal Raisin Crisp|  G|   C|     130|      3|  2|   170|  1.5| 13.5|    10|   120|      25|    3|  1.25| 0.5|30.450843|\n",
      "|     Raisin Nut Bran|  G|   C|     100|      3|  2|   140|  2.5| 10.5|     8|   140|      25|    3|   1.0| 0.5|  39.7034|\n",
      "|   Total Corn Flakes|  G|   C|     110|      2|  1|   200|  0.0| 21.0|     3|    35|     100|    3|   1.0| 1.0|38.839746|\n",
      "|   Total Raisin Bran|  G|   C|     140|      3|  1|   190|  4.0| 15.0|    14|   230|     100|    3|   1.5| 1.0|28.592785|\n",
      "|   Total Whole Grain|  G|   C|     100|      3|  1|   200|  3.0| 16.0|     3|   110|     100|    3|   1.0| 1.0|46.658844|\n",
      "|             Triples|  G|   C|     110|      2|  1|   250|  0.0| 21.0|     3|    60|      25|    3|   1.0|0.75|39.106174|\n",
      "|                Trix|  G|   C|     110|      1|  1|   140|  0.0| 13.0|    12|    25|      25|    2|   1.0| 1.0|27.753301|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Qtd de dados: 22\n"
     ]
    }
   ],
   "source": [
    "# Realizar o select via sql com filtro\n",
    "cereal = spark.sql(''' SELECT * FROM cereal WHERE mfr = 'G' ''')\n",
    "cereal.show()\n",
    "print()\n",
    "print(f\"Qtd de dados: {cereal.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "41040a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|                name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|   rating|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|Apple Cinnamon Ch...|  G|   C|     110|      2|  2|   180|  1.5| 10.5|    10|    70|      25|    1|   1.0|0.75|29.509541|\n",
      "|             Basic 4|  G|   C|     130|      3|  2|   210|  2.0| 18.0|     8|   100|      25|    3|  1.33|0.75|37.038562|\n",
      "|            Cheerios|  G|   C|     110|      6|  2|   290|  2.0| 17.0|     1|   105|      25|    1|   1.0|1.25|50.764999|\n",
      "|Cinnamon Toast Cr...|  G|   C|     120|      1|  3|   210|  0.0| 13.0|     9|    45|      25|    2|   1.0|0.75|19.823573|\n",
      "|            Clusters|  G|   C|     110|      3|  2|   140|  2.0| 13.0|     7|   105|      25|    3|   1.0| 0.5|40.400208|\n",
      "|         Cocoa Puffs|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    55|      25|    2|   1.0| 1.0|22.736446|\n",
      "|       Count Chocula|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    65|      25|    2|   1.0| 1.0|22.396513|\n",
      "|Crispy Wheat & Ra...|  G|   C|     100|      2|  1|   140|  2.0| 11.0|    10|   120|      25|    3|   1.0|0.75|36.176196|\n",
      "|      Golden Grahams|  G|   C|     110|      1|  1|   280|  0.0| 15.0|     9|    45|      25|    2|   1.0|0.75|23.804043|\n",
      "|  Honey Nut Cheerios|  G|   C|     110|      3|  1|   250|  1.5| 11.5|    10|    90|      25|    1|   1.0|0.75|31.072217|\n",
      "|                 Kix|  G|   C|     110|      2|  1|   260|  0.0| 21.0|     3|    40|      25|    2|   1.0| 1.5|39.241114|\n",
      "|        Lucky Charms|  G|   C|     110|      2|  1|   180|  0.0| 12.0|    12|    55|      25|    2|   1.0| 1.0|26.734515|\n",
      "|Multi-Grain Cheerios|  G|   C|     100|      2|  1|   220|  2.0| 15.0|     6|    90|      25|    1|   1.0| 1.0|40.105965|\n",
      "|Oatmeal Raisin Crisp|  G|   C|     130|      3|  2|   170|  1.5| 13.5|    10|   120|      25|    3|  1.25| 0.5|30.450843|\n",
      "|     Raisin Nut Bran|  G|   C|     100|      3|  2|   140|  2.5| 10.5|     8|   140|      25|    3|   1.0| 0.5|  39.7034|\n",
      "|   Total Corn Flakes|  G|   C|     110|      2|  1|   200|  0.0| 21.0|     3|    35|     100|    3|   1.0| 1.0|38.839746|\n",
      "|   Total Raisin Bran|  G|   C|     140|      3|  1|   190|  4.0| 15.0|    14|   230|     100|    3|   1.5| 1.0|28.592785|\n",
      "|   Total Whole Grain|  G|   C|     100|      3|  1|   200|  3.0| 16.0|     3|   110|     100|    3|   1.0| 1.0|46.658844|\n",
      "|             Triples|  G|   C|     110|      2|  1|   250|  0.0| 21.0|     3|    60|      25|    3|   1.0|0.75|39.106174|\n",
      "|                Trix|  G|   C|     110|      1|  1|   140|  0.0| 13.0|    12|    25|      25|    2|   1.0| 1.0|27.753301|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Realizar o filtro via Python\n",
    "df.where(df['mfr'] == 'G').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a39bb404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|Qtd|\n",
      "+---+\n",
      "| 22|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Realizar o select + count via sql com filtro\n",
    "cereal = spark.sql(''' SELECT COUNT(*) AS Qtd FROM cereal WHERE mfr = 'G' ''')\n",
    "cereal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "22749398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Realizar a contagem via python\n",
    "df.where(df['mfr'] == 'G').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "86a747e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- mfr: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- calories: integer (nullable = true)\n",
      " |-- protein: integer (nullable = true)\n",
      " |-- fat: integer (nullable = true)\n",
      " |-- sodium: integer (nullable = true)\n",
      " |-- fiber: double (nullable = true)\n",
      " |-- carbo: double (nullable = true)\n",
      " |-- sugars: integer (nullable = true)\n",
      " |-- potass: integer (nullable = true)\n",
      " |-- vitamins: integer (nullable = true)\n",
      " |-- shelf: integer (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- cups: double (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ver o Schema dos dados\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "56847547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma tabela temporaria na sessão atual\n",
    "df.createOrReplaceTempView(\"cereal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4ec8ff18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+---+\n",
      "|                name|type|mfr|\n",
      "+--------------------+----+---+\n",
      "|           100% Bran|   C|  N|\n",
      "|   100% Natural Bran|   C|  Q|\n",
      "|            All-Bran|   C|  K|\n",
      "|All-Bran with Ext...|   C|  K|\n",
      "|      Almond Delight|   C|  R|\n",
      "|Apple Cinnamon Ch...|   C|  G|\n",
      "|         Apple Jacks|   C|  K|\n",
      "|             Basic 4|   C|  G|\n",
      "|           Bran Chex|   C|  R|\n",
      "|         Bran Flakes|   C|  P|\n",
      "|        Cap'n'Crunch|   C|  Q|\n",
      "|            Cheerios|   C|  G|\n",
      "|Cinnamon Toast Cr...|   C|  G|\n",
      "|            Clusters|   C|  G|\n",
      "|         Cocoa Puffs|   C|  G|\n",
      "|           Corn Chex|   C|  R|\n",
      "|         Corn Flakes|   C|  K|\n",
      "|           Corn Pops|   C|  K|\n",
      "|       Count Chocula|   C|  G|\n",
      "|  Cracklin' Oat Bran|   C|  K|\n",
      "+--------------------+----+---+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Fazer o select dos dados\n",
    "cereal = spark.sql(''' SELECT name, type, mfr FROM cereal''')\n",
    "cereal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0f7ac9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|type|mfr|\n",
      "+----+---+\n",
      "|   C|  G|\n",
      "|   C|  K|\n",
      "|   C|  N|\n",
      "|   C|  P|\n",
      "|   C|  Q|\n",
      "|   C|  R|\n",
      "|   H|  A|\n",
      "|   H|  N|\n",
      "|   H|  Q|\n",
      "+----+---+\n",
      "\n",
      "Qtd de dados: 9\n"
     ]
    }
   ],
   "source": [
    "# Fazer o select com distinct dos dados\n",
    "cereal = spark.sql(''' SELECT DISTINCT type, mfr FROM cereal ORDER BY type, mfr''')\n",
    "cereal.show()\n",
    "print(f\"Qtd de dados: {cereal.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7b6686b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtd de dados: 23\n"
     ]
    }
   ],
   "source": [
    "# Fazer o select com where dos dados\n",
    "cereal = spark.sql(''' SELECT * FROM cereal WHERE mfr = 'K' ''')\n",
    "print(f\"Qtd de dados: {cereal.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "39ed29bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtd de dados: 17\n"
     ]
    }
   ],
   "source": [
    "# Fazer o select com where dos dados\n",
    "cereal = spark.sql(''' SELECT * FROM cereal WHERE calories = 100 ''')\n",
    "print(f\"Qtd de dados: {cereal.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8bcc8331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|               name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|   rating|\n",
      "+-------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|        Corn Flakes|  K|   C|     100|      2|  0|   290|  1.0| 21.0|     2|    35|      25|    1|   1.0| 1.0|45.863324|\n",
      "|Frosted Mini-Wheats|  K|   C|     100|      3|  0|     0|  3.0| 14.0|     7|   100|      25|    2|   1.0| 0.8|58.345141|\n",
      "|         Product 19|  K|   C|     100|      3|  0|   320|  1.0| 20.0|     3|    45|     100|    3|   1.0| 1.0| 41.50354|\n",
      "+-------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "\n",
      "Qtd de dados: 3\n"
     ]
    }
   ],
   "source": [
    "# Fazer o select com where dos dados\n",
    "cereal = spark.sql(''' SELECT * FROM cereal WHERE mfr = 'K' and calories = 100 ''')\n",
    "cereal.show()\n",
    "print(f\"Qtd de dados: {cereal.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c5afe106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+--------------+\n",
      "|mfr|type|total|total_calories|\n",
      "+---+----+-----+--------------+\n",
      "|  A|   H|    1|           100|\n",
      "|  G|   C|   22|          2450|\n",
      "|  K|   C|   23|          2500|\n",
      "|  N|   C|    5|           420|\n",
      "|  N|   H|    1|           100|\n",
      "|  P|   C|    9|           980|\n",
      "|  Q|   C|    7|           660|\n",
      "|  Q|   H|    1|           100|\n",
      "|  R|   C|    8|           920|\n",
      "+---+----+-----+--------------+\n",
      "\n",
      "Qtd de dados: 9\n"
     ]
    }
   ],
   "source": [
    "# Fazer o group by \n",
    "cereal = spark.sql(''' SELECT mfr, type, COUNT(*) AS total, SUM(calories) as total_calories FROM cereal GROUP BY mfr, type ORDER BY mfr, type''')\n",
    "cereal.show()\n",
    "print(f\"Qtd de dados: {cereal.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3572cfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+--------------+\n",
      "|mfr|type|total|total_calories|\n",
      "+---+----+-----+--------------+\n",
      "|  A|   H|    1|           100|\n",
      "|  G|   C|   22|          2450|\n",
      "|  K|   C|   23|          2500|\n",
      "|  N|   C|    5|           420|\n",
      "|  N|   H|    1|           100|\n",
      "|  P|   C|    9|           980|\n",
      "|  Q|   C|    7|           660|\n",
      "|  Q|   H|    1|           100|\n",
      "|  R|   C|    8|           920|\n",
      "+---+----+-----+--------------+\n",
      "\n",
      "Qtd de dados: 9\n"
     ]
    }
   ],
   "source": [
    "# Fazer o group by de forma identada\n",
    "cereal = spark.sql(\n",
    "    ''' \n",
    "\n",
    "    SELECT \n",
    "        mfr, type, COUNT(*) AS total, SUM(calories) as total_calories \n",
    "        \n",
    "    FROM \n",
    "        cereal \n",
    "    \n",
    "    GROUP BY \n",
    "        mfr, type \n",
    "        \n",
    "    ORDER BY \n",
    "        mfr, type\n",
    "\n",
    "    ''')\n",
    "cereal.show()\n",
    "print(f\"Qtd de dados: {cereal.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5d511375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|type|\n",
      "+----+\n",
      "|   C|\n",
      "|   H|\n",
      "+----+\n",
      "\n",
      "Qtd de dados: 2\n"
     ]
    }
   ],
   "source": [
    "# Fazer o select distinct\n",
    "cereal = spark.sql(\n",
    "    ''' \n",
    "\n",
    "    SELECT DISTINCT\n",
    "        type\n",
    "        \n",
    "    FROM \n",
    "        cereal \n",
    "\n",
    "    ''')\n",
    "cereal.show()\n",
    "print(f\"Qtd de dados: {cereal.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "04925367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------+-----+--------------+\n",
      "|mfr|type|type_ney|total|total_calories|\n",
      "+---+----+--------+-----+--------------+\n",
      "|  A|   H|       B|    1|           100|\n",
      "|  P|   C|       A|    9|           980|\n",
      "|  K|   C|       A|   23|          2500|\n",
      "|  G|   C|       A|   22|          2450|\n",
      "|  Q|   C|       A|    7|           660|\n",
      "|  R|   C|       A|    8|           920|\n",
      "|  Q|   H|       B|    1|           100|\n",
      "|  N|   H|       B|    1|           100|\n",
      "|  N|   C|       A|    5|           420|\n",
      "+---+----+--------+-----+--------------+\n",
      "\n",
      "Qtd de dados: 9\n"
     ]
    }
   ],
   "source": [
    "# Fazer o case when\n",
    "cereal = spark.sql(\n",
    "    ''' \n",
    "\n",
    "    SELECT \n",
    "        mfr, \n",
    "        \n",
    "        type,\n",
    "        case when type = 'C' then 'A' else 'B' end as type_ney,\n",
    "\n",
    "        COUNT(*) AS total,\n",
    "        SUM(calories) as total_calories \n",
    "        \n",
    "    FROM \n",
    "        cereal \n",
    "    \n",
    "    GROUP BY \n",
    "        mfr, type \n",
    "\n",
    "    ''')\n",
    "cereal.show()\n",
    "print(f\"Qtd de dados: {cereal.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ecd96943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------+-----+--------------+\n",
      "|mfr|type|type_ney|total|total_calories|\n",
      "+---+----+--------+-----+--------------+\n",
      "|  A|   H|       B|    1|           100|\n",
      "|  P|   C|       A|    9|           980|\n",
      "|  K|   C|       A|   23|          2500|\n",
      "|  G|   C|       A|   22|          2450|\n",
      "|  Q|   C|       A|    7|           660|\n",
      "|  R|   C|       A|    8|           920|\n",
      "|  Q|   H|       B|    1|           100|\n",
      "|  N|   H|       B|    1|           100|\n",
      "|  N|   C|       A|    5|           420|\n",
      "+---+----+--------+-----+--------------+\n",
      "\n",
      "Qtd de dados: 9\n"
     ]
    }
   ],
   "source": [
    "# Fazer o case when\n",
    "cereal = spark.sql(\n",
    "    ''' \n",
    "\n",
    "    SELECT \n",
    "        mfr, \n",
    "        \n",
    "        type,\n",
    "\n",
    "        case \n",
    "            when type = 'C' then 'A' \n",
    "            when type = 'H' then 'B' \n",
    "            else 'C' \n",
    "            \n",
    "        end as type_ney,\n",
    "\n",
    "        COUNT(*) AS total,\n",
    "        SUM(calories) as total_calories \n",
    "        \n",
    "    FROM \n",
    "        cereal \n",
    "    \n",
    "    GROUP BY \n",
    "        mfr, type \n",
    "\n",
    "    ''')\n",
    "cereal.show()\n",
    "print(f\"Qtd de dados: {cereal.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0c6f5fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|                name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|   rating|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|           100% Bran|  N|   C|      70|      4|  1|   130| 10.0|  5.0|     6|   280|      25|    3|   1.0|0.33|68.402973|\n",
      "|   100% Natural Bran|  Q|   C|     120|      3|  5|    15|  2.0|  8.0|     8|   135|       0|    3|   1.0| 1.0|33.983679|\n",
      "|            All-Bran|  K|   C|      70|      4|  1|   260|  9.0|  7.0|     5|   320|      25|    3|   1.0|0.33|59.425505|\n",
      "|All-Bran with Ext...|  K|   C|      50|      4|  0|   140| 14.0|  8.0|     0|   330|      25|    3|   1.0| 0.5|93.704912|\n",
      "|      Almond Delight|  R|   C|     110|      2|  2|   200|  1.0| 14.0|     8|    -1|      25|    3|   1.0|0.75|34.384843|\n",
      "|Apple Cinnamon Ch...|  G|   C|     110|      2|  2|   180|  1.5| 10.5|    10|    70|      25|    1|   1.0|0.75|29.509541|\n",
      "|         Apple Jacks|  K|   C|     110|      2|  0|   125|  1.0| 11.0|    14|    30|      25|    2|   1.0| 1.0|33.174094|\n",
      "|             Basic 4|  G|   C|     130|      3|  2|   210|  2.0| 18.0|     8|   100|      25|    3|  1.33|0.75|37.038562|\n",
      "|           Bran Chex|  R|   C|      90|      2|  1|   200|  4.0| 15.0|     6|   125|      25|    1|   1.0|0.67|49.120253|\n",
      "|         Bran Flakes|  P|   C|      90|      3|  0|   210|  5.0| 13.0|     5|   190|      25|    3|   1.0|0.67|53.313813|\n",
      "|        Cap'n'Crunch|  Q|   C|     120|      1|  2|   220|  0.0| 12.0|    12|    35|      25|    2|   1.0|0.75|18.042851|\n",
      "|            Cheerios|  G|   C|     110|      6|  2|   290|  2.0| 17.0|     1|   105|      25|    1|   1.0|1.25|50.764999|\n",
      "|Cinnamon Toast Cr...|  G|   C|     120|      1|  3|   210|  0.0| 13.0|     9|    45|      25|    2|   1.0|0.75|19.823573|\n",
      "|            Clusters|  G|   C|     110|      3|  2|   140|  2.0| 13.0|     7|   105|      25|    3|   1.0| 0.5|40.400208|\n",
      "|         Cocoa Puffs|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    55|      25|    2|   1.0| 1.0|22.736446|\n",
      "|           Corn Chex|  R|   C|     110|      2|  0|   280|  0.0| 22.0|     3|    25|      25|    1|   1.0| 1.0|41.445019|\n",
      "|         Corn Flakes|  K|   C|     100|      2|  0|   290|  1.0| 21.0|     2|    35|      25|    1|   1.0| 1.0|45.863324|\n",
      "|           Corn Pops|  K|   C|     110|      1|  0|    90|  1.0| 13.0|    12|    20|      25|    2|   1.0| 1.0|35.782791|\n",
      "|       Count Chocula|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    65|      25|    2|   1.0| 1.0|22.396513|\n",
      "|  Cracklin' Oat Bran|  K|   C|     110|      3|  3|   140|  4.0| 10.0|     7|   160|      25|    3|   1.0| 0.5|40.448772|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Fazer select de todas as tabelas \n",
    "cereal = spark.sql (\"\"\" select * from cereal \"\"\")\n",
    "cereal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "620c3548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------------+------------+------------+------------------+--------------------+-----------+\n",
      "|mfr|type|sum_calories|min_calories|max_calories|      avg_calories|count_distinct_names|count_names|\n",
      "+---+----+------------+------------+------------+------------------+--------------------+-----------+\n",
      "|  A|   H|         100|         100|         100|             100.0|                   1|          1|\n",
      "|  G|   C|        2450|         100|         140|111.36363636363636|                  22|         22|\n",
      "|  K|   C|        2500|          50|         160|108.69565217391305|                  23|         23|\n",
      "|  N|   C|         420|          70|          90|              84.0|                   5|          5|\n",
      "|  N|   H|         100|         100|         100|             100.0|                   1|          1|\n",
      "|  P|   C|         980|          90|         120|108.88888888888889|                   9|          9|\n",
      "|  Q|   C|         660|          50|         120| 94.28571428571429|                   7|          7|\n",
      "|  Q|   H|         100|         100|         100|             100.0|                   1|          1|\n",
      "|  R|   C|         920|          90|         150|             115.0|                   8|          8|\n",
      "+---+----+------------+------------+------------+------------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agrupar dados\n",
    "cereal = spark.sql (\"\"\" \n",
    "                    \n",
    "    SELECT\n",
    "        mfr,\n",
    "        type,\n",
    "        sum(calories) as sum_calories,\n",
    "        min(calories) as min_calories,\n",
    "        max(calories) as max_calories,\n",
    "        avg(calories) as avg_calories,\n",
    "        count(distinct name) as count_distinct_names,\n",
    "        count(name) as count_names\n",
    "    \n",
    "    FROM\n",
    "        cereal\n",
    "    \n",
    "    GROUP BY\n",
    "        mfr,\n",
    "        type\n",
    "    \n",
    "    ORDER BY\n",
    "            mfr,\n",
    "            type\n",
    "\n",
    "        \"\"\")\n",
    "cereal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8651af3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------------+------------+------------+------------------+---------+---------+---------+------------------+------------+------------+------------+------------------+--------------------+-----------+\n",
      "|mfr|type|sum_calories|min_calories|max_calories|      avg_calories|sum_carbo|min_carbo|max_carbo|         avg_carbo|sum_vitamins|min_vitamins|max_vitamins|      avg_vitamins|count_distinct_names|count_names|\n",
      "+---+----+------------+------------+------------+------------------+---------+---------+---------+------------------+------------+------------+------------+------------------+--------------------+-----------+\n",
      "|  A|   H|         100|         100|         100|             100.0|     16.0|     16.0|     16.0|              16.0|          25|          25|          25|              25.0|                   1|          1|\n",
      "|  G|   C|        2450|         100|         140|111.36363636363636|    324.0|     10.5|     21.0|14.727272727272727|         775|          25|         100| 35.22727272727273|                  22|         22|\n",
      "|  K|   C|        2500|          50|         160|108.69565217391305|    348.0|      7.0|     22.0|15.130434782608695|         800|          25|         100| 34.78260869565217|                  23|         23|\n",
      "|  N|   C|         420|          70|          90|              84.0|     75.0|      5.0|     20.0|              15.0|          50|           0|          25|              10.0|                   5|          5|\n",
      "|  N|   H|         100|         100|         100|             100.0|     21.0|     21.0|     21.0|              21.0|           0|           0|           0|               0.0|                   1|          1|\n",
      "|  P|   C|         980|          90|         120|108.88888888888889|    119.0|     11.0|     17.0|13.222222222222221|         225|          25|          25|              25.0|                   9|          9|\n",
      "|  Q|   C|         660|          50|         120| 94.28571428571429|     81.0|      8.0|     14.0|11.571428571428571|         100|           0|          25|14.285714285714286|                   7|          7|\n",
      "|  Q|   H|         100|         100|         100|             100.0|     -1.0|     -1.0|     -1.0|              -1.0|           0|           0|           0|               0.0|                   1|          1|\n",
      "|  R|   C|         920|          90|         150|             115.0|    141.0|     14.0|     23.0|            17.625|         200|          25|          25|              25.0|                   8|          8|\n",
      "+---+----+------------+------------+------------+------------------+---------+---------+---------+------------------+------------+------------+------------+------------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agrupar dados\n",
    "cereal = spark.sql (\"\"\" \n",
    "                    \n",
    "    SELECT\n",
    "        mfr,\n",
    "        type,\n",
    "                    \n",
    "        sum(calories) as sum_calories,\n",
    "        min(calories) as min_calories,\n",
    "        max(calories) as max_calories,\n",
    "        avg(calories) as avg_calories,\n",
    "\n",
    "        sum(carbo) as sum_carbo,\n",
    "        min(carbo) as min_carbo,\n",
    "        max(carbo) as max_carbo,\n",
    "        avg(carbo) as avg_carbo,\n",
    "\n",
    "        sum(vitamins) as sum_vitamins,\n",
    "        min(vitamins) as min_vitamins,\n",
    "        max(vitamins) as max_vitamins,\n",
    "        avg(vitamins) as avg_vitamins,        \n",
    "\n",
    "        count(distinct name) as count_distinct_names,\n",
    "        count(name) as count_names\n",
    "    \n",
    "    FROM\n",
    "        cereal\n",
    "    \n",
    "    GROUP BY\n",
    "        mfr,\n",
    "        type\n",
    "    \n",
    "    ORDER BY\n",
    "            mfr,\n",
    "            type\n",
    "\n",
    "        \"\"\")\n",
    "cereal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "499b5bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------------+------------+------------+------------+---------+---------+---------+---------+------------+------------+------------+------------+--------------------+-----------+\n",
      "|mfr|type|sum_calories|min_calories|max_calories|avg_calories|sum_carbo|min_carbo|max_carbo|avg_carbo|sum_vitamins|min_vitamins|max_vitamins|avg_vitamins|count_distinct_names|count_names|\n",
      "+---+----+------------+------------+------------+------------+---------+---------+---------+---------+------------+------------+------------+------------+--------------------+-----------+\n",
      "|  A|   H|         100|         100|         100|      100.00|     16.0|     16.0|     16.0|    16.00|          25|          25|          25|       25.00|                   1|          1|\n",
      "|  G|   C|        2450|         100|         140|      111.36|    324.0|     10.5|     21.0|    14.73|         775|          25|         100|       35.23|                  22|         22|\n",
      "|  K|   C|        2500|          50|         160|      108.70|    348.0|      7.0|     22.0|    15.13|         800|          25|         100|       34.78|                  23|         23|\n",
      "|  N|   C|         420|          70|          90|       84.00|     75.0|      5.0|     20.0|    15.00|          50|           0|          25|       10.00|                   5|          5|\n",
      "|  N|   H|         100|         100|         100|      100.00|     21.0|     21.0|     21.0|    21.00|           0|           0|           0|        0.00|                   1|          1|\n",
      "|  P|   C|         980|          90|         120|      108.89|    119.0|     11.0|     17.0|    13.22|         225|          25|          25|       25.00|                   9|          9|\n",
      "|  Q|   C|         660|          50|         120|       94.29|     81.0|      8.0|     14.0|    11.57|         100|           0|          25|       14.29|                   7|          7|\n",
      "|  Q|   H|         100|         100|         100|      100.00|     -1.0|     -1.0|     -1.0|    -1.00|           0|           0|           0|        0.00|                   1|          1|\n",
      "|  R|   C|         920|          90|         150|      115.00|    141.0|     14.0|     23.0|    17.63|         200|          25|          25|       25.00|                   8|          8|\n",
      "+---+----+------------+------------+------------+------------+---------+---------+---------+---------+------------+------------+------------+------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agrupar dados\n",
    "cereal = spark.sql (\"\"\" \n",
    "                    \n",
    "    SELECT\n",
    "        mfr,\n",
    "        type,\n",
    "                    \n",
    "        sum(calories) as sum_calories,\n",
    "        min(calories) as min_calories,\n",
    "        max(calories) as max_calories,\n",
    "        cast(avg(calories) as decimal(10,2)) as avg_calories,\n",
    "\n",
    "        sum(carbo) as sum_carbo,\n",
    "        min(carbo) as min_carbo,\n",
    "        max(carbo) as max_carbo,\n",
    "        cast(avg(carbo) as decimal(10,2)) as avg_carbo,\n",
    "\n",
    "        sum(vitamins) as sum_vitamins,\n",
    "        min(vitamins) as min_vitamins,\n",
    "        max(vitamins) as max_vitamins,\n",
    "        cast(avg(vitamins) as decimal(10,2)) as avg_vitamins,        \n",
    "\n",
    "        count(distinct name) as count_distinct_names,\n",
    "        count(name) as count_names\n",
    "    \n",
    "    FROM\n",
    "        cereal\n",
    "    \n",
    "    GROUP BY\n",
    "        mfr,\n",
    "        type\n",
    "    \n",
    "    ORDER BY\n",
    "            mfr,\n",
    "            type\n",
    "\n",
    "        \"\"\")\n",
    "cereal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "92728865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+------------+------------+------------+------------+---------+---------+---------+---------+------------+------------+------------+------------+--------------------+-----------+\n",
      "|mfr|type|type_fruit|sum_calories|min_calories|max_calories|avg_calories|sum_carbo|min_carbo|max_carbo|avg_carbo|sum_vitamins|min_vitamins|max_vitamins|avg_vitamins|count_distinct_names|count_names|\n",
      "+---+----+----------+------------+------------+------------+------------+---------+---------+---------+---------+------------+------------+------------+------------+--------------------+-----------+\n",
      "|  A|   H|   Abacaxi|         100|         100|         100|      100.00|     16.0|     16.0|     16.0|    16.00|          25|          25|          25|       25.00|                   1|          1|\n",
      "|  G|   C|    Goiaba|        2450|         100|         140|      111.36|    324.0|     10.5|     21.0|    14.73|         775|          25|         100|       35.23|                  22|         22|\n",
      "|  K|   C|    Banana|        2500|          50|         160|      108.70|    348.0|      7.0|     22.0|    15.13|         800|          25|         100|       34.78|                  23|         23|\n",
      "|  N|   C|      Maça|         420|          70|          90|       84.00|     75.0|      5.0|     20.0|    15.00|          50|           0|          25|       10.00|                   5|          5|\n",
      "|  N|   H|      Maça|         100|         100|         100|      100.00|     21.0|     21.0|     21.0|    21.00|           0|           0|           0|        0.00|                   1|          1|\n",
      "|  P|   C|    Tomate|         980|          90|         120|      108.89|    119.0|     11.0|     17.0|    13.22|         225|          25|          25|       25.00|                   9|          9|\n",
      "|  Q|   C|      Pera|         660|          50|         120|       94.29|     81.0|      8.0|     14.0|    11.57|         100|           0|          25|       14.29|                   7|          7|\n",
      "|  Q|   H|      Pera|         100|         100|         100|      100.00|     -1.0|     -1.0|     -1.0|    -1.00|           0|           0|           0|        0.00|                   1|          1|\n",
      "|  R|   C|       Uva|         920|          90|         150|      115.00|    141.0|     14.0|     23.0|    17.63|         200|          25|          25|       25.00|                   8|          8|\n",
      "+---+----+----------+------------+------------+------------+------------+---------+---------+---------+---------+------------+------------+------------+------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agrupar dados com case \n",
    "cereal = spark.sql (\"\"\" \n",
    "                    \n",
    "    SELECT\n",
    "        mfr,\n",
    "        type,\n",
    "                    \n",
    "        case\n",
    "            when mfr = 'A' then 'Abacaxi'       \n",
    "            when mfr = 'G' then 'Goiaba'   \n",
    "            when mfr = 'K' then 'Banana'   \n",
    "            when mfr = 'N' then 'Maça'   \n",
    "            when mfr = 'P' then 'Tomate'   \n",
    "            when mfr = 'Q' then 'Pera'   \n",
    "            when mfr = 'R' then 'Uva'   \n",
    "            else 'NA' \n",
    "        end \n",
    "                    \n",
    "        as type_fruit,\n",
    "      \n",
    "        sum(calories) as sum_calories,\n",
    "        min(calories) as min_calories,\n",
    "        max(calories) as max_calories,\n",
    "        cast(avg(calories) as decimal(10,2)) as avg_calories,\n",
    "\n",
    "        sum(carbo) as sum_carbo,\n",
    "        min(carbo) as min_carbo,\n",
    "        max(carbo) as max_carbo,\n",
    "        cast(avg(carbo) as decimal(10,2)) as avg_carbo,\n",
    "\n",
    "        sum(vitamins) as sum_vitamins,\n",
    "        min(vitamins) as min_vitamins,\n",
    "        max(vitamins) as max_vitamins,\n",
    "        cast(avg(vitamins) as decimal(10,2)) as avg_vitamins,        \n",
    "\n",
    "        count(distinct name) as count_distinct_names,\n",
    "        count(name) as count_names\n",
    "    \n",
    "    FROM\n",
    "        cereal\n",
    "    \n",
    "    GROUP BY\n",
    "        mfr,\n",
    "        type\n",
    "    \n",
    "    ORDER BY\n",
    "            mfr,\n",
    "            type\n",
    "\n",
    "        \"\"\")\n",
    "cereal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa76ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar base Sales\n",
    "url = \"https://github.com/FIAP/Pos_Tech_DTAT/raw/refs/heads/Framework-de-Big-Data/Aula%203/sales_data_sample.csv\"\n",
    "local_path = \"sales_data_sample.csv\"\n",
    "urllib.request.urlretrieve(url, local_path)\n",
    "\n",
    "sales = spark.read.csv(local_path, sep=\",\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b827e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+---------+---------------+-------+---------------+-------+------+--------+-------+-----------+----+-----------+--------------------+----------------+--------------------+------------+-------------+--------+----------+---------+---------+---------------+----------------+--------+\n",
      "|ORDERNUMBER|QUANTITYORDERED|PRICEEACH|ORDERLINENUMBER|  SALES|      ORDERDATE| STATUS|QTR_ID|MONTH_ID|YEAR_ID|PRODUCTLINE|MSRP|PRODUCTCODE|        CUSTOMERNAME|           PHONE|        ADDRESSLINE1|ADDRESSLINE2|         CITY|   STATE|POSTALCODE|  COUNTRY|TERRITORY|CONTACTLASTNAME|CONTACTFIRSTNAME|DEALSIZE|\n",
      "+-----------+---------------+---------+---------------+-------+---------------+-------+------+--------+-------+-----------+----+-----------+--------------------+----------------+--------------------+------------+-------------+--------+----------+---------+---------+---------------+----------------+--------+\n",
      "|      10107|             30|     95.7|              2| 2871.0| 2/24/2003 0:00|Shipped|     1|       2|   2003|Motorcycles|  95|   S10_1678|   Land of Toys Inc.|      2125557818|897 Long Airport ...|        NULL|          NYC|      NY|     10022|      USA|       NA|             Yu|            Kwai|   Small|\n",
      "|      10121|             34|    81.35|              5| 2765.9|  5/7/2003 0:00|Shipped|     2|       5|   2003|Motorcycles|  95|   S10_1678|  Reims Collectables|      26.47.1555|  59 rue de l'Abbaye|        NULL|        Reims|    NULL|     51100|   France|     EMEA|        Henriot|            Paul|   Small|\n",
      "|      10134|             41|    94.74|              2|3884.34|  7/1/2003 0:00|Shipped|     3|       7|   2003|Motorcycles|  95|   S10_1678|     Lyon Souveniers|+33 1 46 62 7555|27 rue du Colonel...|        NULL|        Paris|    NULL|     75508|   France|     EMEA|       Da Cunha|          Daniel|  Medium|\n",
      "|      10145|             45|    83.26|              6| 3746.7| 8/25/2003 0:00|Shipped|     3|       8|   2003|Motorcycles|  95|   S10_1678|   Toys4GrownUps.com|      6265557265|  78934 Hillside Dr.|        NULL|     Pasadena|      CA|     90003|      USA|       NA|          Young|           Julie|  Medium|\n",
      "|      10159|             49|    100.0|             14|5205.27|10/10/2003 0:00|Shipped|     4|      10|   2003|Motorcycles|  95|   S10_1678|Corporate Gift Id...|      6505551386|     7734 Strong St.|        NULL|San Francisco|      CA|      NULL|      USA|       NA|          Brown|           Julie|  Medium|\n",
      "|      10168|             36|    96.66|              1|3479.76|10/28/2003 0:00|Shipped|     4|      10|   2003|Motorcycles|  95|   S10_1678|Technics Stores Inc.|      6505556809|   9408 Furth Circle|        NULL|   Burlingame|      CA|     94217|      USA|       NA|         Hirano|            Juri|  Medium|\n",
      "|      10180|             29|    86.13|              9|2497.77|11/11/2003 0:00|Shipped|     4|      11|   2003|Motorcycles|  95|   S10_1678|Daedalus Designs ...|      20.16.1555|184, chausse de T...|        NULL|        Lille|    NULL|     59000|   France|     EMEA|          Rance|         Martine|   Small|\n",
      "|      10188|             48|    100.0|              1|5512.32|11/18/2003 0:00|Shipped|     4|      11|   2003|Motorcycles|  95|   S10_1678|        Herkku Gifts|   +47 2267 3215|Drammen 121, PR 7...|        NULL|       Bergen|    NULL|    N 5804|   Norway|     EMEA|         Oeztan|          Veysel|  Medium|\n",
      "|      10201|             22|    98.57|              2|2168.54| 12/1/2003 0:00|Shipped|     4|      12|   2003|Motorcycles|  95|   S10_1678|     Mini Wheels Co.|      6505555787|5557 North Pendal...|        NULL|San Francisco|      CA|      NULL|      USA|       NA|         Murphy|           Julie|   Small|\n",
      "|      10211|             41|    100.0|             14|4708.44| 1/15/2004 0:00|Shipped|     1|       1|   2004|Motorcycles|  95|   S10_1678|    Auto Canal Petit|  (1) 47.55.6555|   25, rue Lauriston|        NULL|        Paris|    NULL|     75016|   France|     EMEA|        Perrier|       Dominique|  Medium|\n",
      "|      10223|             37|    100.0|              1|3965.66| 2/20/2004 0:00|Shipped|     1|       2|   2004|Motorcycles|  95|   S10_1678|Australian Collec...|    03 9520 4555|   636 St Kilda Road|     Level 3|    Melbourne|Victoria|      3004|Australia|     APAC|       Ferguson|           Peter|  Medium|\n",
      "|      10237|             23|    100.0|              7|2333.12|  4/5/2004 0:00|Shipped|     2|       4|   2004|Motorcycles|  95|   S10_1678|     Vitachrome Inc.|      2125551500|   2678 Kingston Rd.|   Suite 101|          NYC|      NY|     10022|      USA|       NA|          Frick|         Michael|   Small|\n",
      "|      10251|             28|    100.0|              2|3188.64| 5/18/2004 0:00|Shipped|     2|       5|   2004|Motorcycles|  95|   S10_1678|Tekni Collectable...|      2015559350|       7476 Moss Rd.|        NULL|       Newark|      NJ|     94019|      USA|       NA|          Brown|         William|  Medium|\n",
      "|      10263|             34|    100.0|              2|3676.76| 6/28/2004 0:00|Shipped|     2|       6|   2004|Motorcycles|  95|   S10_1678|     Gift Depot Inc.|      2035552570| 25593 South Bay Ln.|        NULL|  Bridgewater|      CT|     97562|      USA|       NA|           King|           Julie|  Medium|\n",
      "|      10275|             45|    92.83|              1|4177.35| 7/23/2004 0:00|Shipped|     3|       7|   2004|Motorcycles|  95|   S10_1678|   La Rochelle Gifts|      40.67.8555|67, rue des Cinqu...|        NULL|       Nantes|    NULL|     44000|   France|     EMEA|        Labrune|          Janine|  Medium|\n",
      "|      10285|             36|    100.0|              6|4099.68| 8/27/2004 0:00|Shipped|     3|       8|   2004|Motorcycles|  95|   S10_1678|Marta's Replicas Co.|      6175558555| 39323 Spinnaker Dr.|        NULL|    Cambridge|      MA|     51247|      USA|       NA|      Hernandez|           Marta|  Medium|\n",
      "|      10299|             23|    100.0|              9|2597.39| 9/30/2004 0:00|Shipped|     3|       9|   2004|Motorcycles|  95|   S10_1678|Toys of Finland, Co.|     90-224 8555|       Keskuskatu 45|        NULL|     Helsinki|    NULL|     21240|  Finland|     EMEA|      Karttunen|           Matti|   Small|\n",
      "|      10309|             41|    100.0|              5|4394.38|10/15/2004 0:00|Shipped|     4|      10|   2004|Motorcycles|  95|   S10_1678|  Baane Mini Imports|      07-98 9555|Erling Skakkes ga...|        NULL|      Stavern|    NULL|      4110|   Norway|     EMEA|     Bergulfsen|           Jonas|  Medium|\n",
      "|      10318|             46|    94.74|              1|4358.04| 11/2/2004 0:00|Shipped|     4|      11|   2004|Motorcycles|  95|   S10_1678|Diecast Classics ...|      2155551555|    7586 Pompton St.|        NULL|    Allentown|      PA|     70267|      USA|       NA|             Yu|           Kyung|  Medium|\n",
      "|      10329|             42|    100.0|              1|4396.14|11/15/2004 0:00|Shipped|     4|      11|   2004|Motorcycles|  95|   S10_1678|   Land of Toys Inc.|      2125557818|897 Long Airport ...|        NULL|          NYC|      NY|     10022|      USA|       NA|             Yu|            Kwai|  Medium|\n",
      "+-----------+---------------+---------+---------------+-------+---------------+-------+------+--------+-------+-----------+----+-----------+--------------------+----------------+--------------------+------------+-------------+--------+----------+---------+---------+---------------+----------------+--------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Ver os dados \n",
    "sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ORDERNUMBER: integer (nullable = true)\n",
      " |-- QUANTITYORDERED: integer (nullable = true)\n",
      " |-- PRICEEACH: double (nullable = true)\n",
      " |-- ORDERLINENUMBER: integer (nullable = true)\n",
      " |-- SALES: double (nullable = true)\n",
      " |-- ORDERDATE: string (nullable = true)\n",
      " |-- STATUS: string (nullable = true)\n",
      " |-- QTR_ID: integer (nullable = true)\n",
      " |-- MONTH_ID: integer (nullable = true)\n",
      " |-- YEAR_ID: integer (nullable = true)\n",
      " |-- PRODUCTLINE: string (nullable = true)\n",
      " |-- MSRP: integer (nullable = true)\n",
      " |-- PRODUCTCODE: string (nullable = true)\n",
      " |-- CUSTOMERNAME: string (nullable = true)\n",
      " |-- PHONE: string (nullable = true)\n",
      " |-- ADDRESSLINE1: string (nullable = true)\n",
      " |-- ADDRESSLINE2: string (nullable = true)\n",
      " |-- CITY: string (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- POSTALCODE: string (nullable = true)\n",
      " |-- COUNTRY: string (nullable = true)\n",
      " |-- TERRITORY: string (nullable = true)\n",
      " |-- CONTACTLASTNAME: string (nullable = true)\n",
      " |-- CONTACTFIRSTNAME: string (nullable = true)\n",
      " |-- DEALSIZE: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1764caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar tabela \n",
    "sales.createOrReplaceTempView('sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "9441ceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar join\n",
    "\n",
    "calendar = spark.sql( \"\"\" \n",
    "\n",
    "    SELECT DISTINCT\n",
    "        orderdate,\n",
    "        qtr_id,\n",
    "        month_id,\n",
    "        year_id\n",
    "                     \n",
    "    FROM\n",
    "        sales\n",
    "                     \n",
    "    ORDER BY\n",
    "        orderdate\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "sales_data = spark.sql( \"\"\" \n",
    "\n",
    "    SELECT DISTINCT\n",
    "        ordernumber,\n",
    "        customername,\n",
    "        sales,\n",
    "        quantityordered,\n",
    "        productcode,\n",
    "        orderlinenumber,\n",
    "        priceeach               \n",
    "                                    \n",
    "    FROM\n",
    "        sales\n",
    "                     \n",
    "    ORDER BY\n",
    "        ordernumber\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "customers = spark.sql( \"\"\" \n",
    "\n",
    "    SELECT DISTINCT \n",
    "        customername,\n",
    "        phone,\n",
    "        addressline1,\n",
    "        addressline2,\n",
    "        city,\n",
    "        state,\n",
    "        postalcode,\n",
    "        country,\n",
    "        territory\n",
    "\n",
    "    FROM \n",
    "        sales\n",
    "                  \n",
    "    ORDER BY \n",
    "        customername\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# Criar Temp View para cada variavel criada\n",
    "sales_data.createOrReplaceTempView('sales_data')\n",
    "calendar.createOrReplaceTempView('calendar')\n",
    "customers.createOrReplaceTempView('customers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "348b11f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calendar count: 252\n",
      "sales count: 2823\n",
      "customers count: 92\n"
     ]
    }
   ],
   "source": [
    "# Count dos dados\n",
    "print(f\"calendar count: {calendar.count()}\")\n",
    "print(f\"sales count: {sales.count()}\")\n",
    "print(f\"customers count: {customers.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "4468b966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|ordernumber|         city|\n",
      "+-----------+-------------+\n",
      "|      10300|    Frankfurt|\n",
      "|      10385|   San Rafael|\n",
      "|      10241|   Strasbourg|\n",
      "|      10182|   San Rafael|\n",
      "|      10140|   Burlingame|\n",
      "|      10153|       Madrid|\n",
      "|      10293|       Torino|\n",
      "|      10161|      Aaarhus|\n",
      "|      10406|    Kobenhavn|\n",
      "|      10414|       Boston|\n",
      "|      10311|       Madrid|\n",
      "|      10357|   San Rafael|\n",
      "|      10195| White Plains|\n",
      "|      10422|    Allentown|\n",
      "|      10189|     Pasadena|\n",
      "|      10111|San Francisco|\n",
      "|      10204|          NYC|\n",
      "|      10304|   Versailles|\n",
      "|      10151|         Oulu|\n",
      "|      10290|   Brickhaven|\n",
      "+-----------+-------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Realizar Inner Join\n",
    "master = spark.sql (\"\"\" \n",
    "\n",
    "\n",
    "SELECT DISTINCT \n",
    "    s.ordernumber, c.city\n",
    "                    \n",
    "FROM \n",
    "    sales_data s\n",
    "\n",
    "INNER JOIN customers c \n",
    "    ON s.customername=c.customername\n",
    "\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "master.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8530260",
   "metadata": {},
   "source": [
    "#### Aula 4 - Operações entre Dataframes e Armazenamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b843ad",
   "metadata": {},
   "source": [
    "#### Aula 5 - Introdução aos Sistemas de Recomendação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5093e302",
   "metadata": {},
   "source": [
    "#### Aula 6 - Recomendações com o Algoritmo ALS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ambiente_fase3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
