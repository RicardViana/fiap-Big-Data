{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89d6ca84",
   "metadata": {},
   "source": [
    "#### Framework de Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd59115f",
   "metadata": {},
   "source": [
    "#### Conteúdo - Bases e Notebook da aula\n",
    "\n",
    "https://github.com/FIAP/Pos_Tech_DTAT/tree/Framework-de-Big-Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a770d68e",
   "metadata": {},
   "source": [
    "#### Importação de pacotes, bibliotecas e funções (def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "733f8511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar biblioteca completa\n",
    "import pandas as pd \n",
    "import findspark\n",
    "import urllib.request\n",
    "\n",
    "# Importar função especifica de um módulo\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Import spark libraries\n",
    "from pyspark.sql import Row, DataFrame\n",
    "from pyspark.sql.types import StringType, StructType, StructField, IntegerType\n",
    "from pyspark.sql.functions import col, expr, lit, substring, concat, concat_ws, when, coalesce\n",
    "from pyspark.sql import functions as F\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d2a35d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como não queremos configurar as variaveis de ambiente, precisamos usar o findspark.init\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d70f28a",
   "metadata": {},
   "source": [
    "#### Aula 1 - Conhecendo o Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "352c0cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.count: 561\n",
      "df.col ct: 6\n",
      "df.columns: ['Bank Name', 'City', 'ST', 'CERT', 'Acquiring Institution', 'Closing Date']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "1. spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "Esta é a linha que inicializa o Spark. Ela é o ponto de entrada para qualquer funcionalidade do Spark.\n",
    "\n",
    "SparkSession: É o principal ponto de entrada para a programação com DataFrames e Datasets no Spark.\n",
    "\n",
    ".builder: É um padrão de projeto (builder pattern) usado para construir o objeto SparkSession com diferentes configurações.\n",
    "\n",
    ".master(\"local[*]\"): Esta é uma das configurações mais importantes. Ela define como e onde o Spark irá executar suas tarefas.\n",
    "\n",
    "\"local[*]\" instrui o Spark a ser executado em modo local, utilizando todos os núcleos de processamento (cores) disponíveis na sua máquina. \n",
    "Se você quisesse usar apenas 2 núcleos, por exemplo, usaria \"local[2]\". Este modo é ideal para desenvolvimento, testes e aprendizado em uma única máquina.\n",
    "\n",
    ".getOrCreate(): Este método cria uma nova SparkSession se uma não existir. Se já houver uma sessão ativa com as mesmas configurações, ele simplesmente a retorna. \n",
    "Isso evita a criação de múltiplas sessões desnecessariamente.\n",
    "\n",
    "Em resumo: esta linha cria e configura uma sessão do Spark para rodar localmente na sua máquina, usando todos os recursos de processamento disponíveis, e armazena essa sessão na variável spark.\n",
    "\n",
    "2. df = spark.read.csv(link, sep= \",\", inferSchema = True, header = True)\n",
    "Esta é a linha que efetivamente lê os dados e os transforma em um DataFrame, que é a principal estrutura de dados do Spark.\n",
    "\n",
    "spark.read: É o objeto usado para ler dados de fontes externas (arquivos, bancos de dados, etc.).\n",
    "\n",
    ".csv(link, ...): Especifica que o formato do arquivo a ser lido é CSV. O primeiro argumento, link, é o caminho para o arquivo.\n",
    "\n",
    "sep= \",\": Este parâmetro (separador) informa ao Spark que as colunas no arquivo CSV são delimitadas por uma vírgula. É o padrão para arquivos CSV, mas é uma boa prática especificá-lo.\n",
    "\n",
    "inferSchema = True: Esta é uma opção muito útil. Ela instrui o Spark a analisar uma amostra dos dados para inferir (deduzir) automaticamente o tipo de dado de cada coluna. \n",
    "Por exemplo, ele tentará identificar se uma coluna contém números inteiros (IntegerType), números decimais (DoubleType) ou texto (StringType). Sem isso, todas as colunas seriam tratadas como texto (String).\n",
    "\n",
    "header = True: Esta opção informa ao Spark que a primeira linha do arquivo CSV contém os nomes das colunas (cabeçalho) e que essa linha não deve ser tratada como dados. \n",
    "O Spark usará esses nomes para as colunas do DataFrame.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "\n",
    "# Baixar o arquivo pois não conseguimos usar o Spark para ler diretamnte o arquivo no Github\n",
    "url = \"https://raw.githubusercontent.com/FIAP/Pos_Tech_DTAT/refs/heads/Framework-de-Big-Data/Aula%201/banklist.csv\"\n",
    "local_path = \"banklist.csv\"\n",
    "urllib.request.urlretrieve(url, local_path)\n",
    "\n",
    "# Ler com Spark\n",
    "df = spark.read.csv(local_path, sep=\",\", header=True, inferSchema=True)\n",
    "\n",
    "print(f\"df.count: {df.count()}\")\n",
    "print(f\"df.col ct: {len(df.columns)}\")\n",
    "print(f\"df.columns: {df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74af7dfc",
   "metadata": {},
   "source": [
    "#### Aula 2 - Operações Básicas no Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17a17a",
   "metadata": {},
   "source": [
    "#### Aula 3 - Consultas e Seleções"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8530260",
   "metadata": {},
   "source": [
    "#### Aula 4 - Operações entre Dataframes e Armazenamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b843ad",
   "metadata": {},
   "source": [
    "#### Aula 5 - Introdução aos Sistemas de Recomendação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5093e302",
   "metadata": {},
   "source": [
    "#### Aula 6 - Recomendações com o Algoritmo ALS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ambiente_fase3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
