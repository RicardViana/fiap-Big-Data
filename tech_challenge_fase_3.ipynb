{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4e35b16",
   "metadata": {},
   "source": [
    "## **Tech Challenge**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d816d8",
   "metadata": {},
   "source": [
    "### **Projeto**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff9834d",
   "metadata": {},
   "source": [
    "Entender como foi o comportamento da populaÃ§Ã£o na Ã©poca da pandemia da COVID-19 e quais indicadores seriam importantes para o planejamento, caso haja um novo surto da doenÃ§a, utilizando o estudo do PNAD-COVID 19 do IBGE para termos respostas ao problema proposto, pois sÃ£o dados confiÃ¡veis, porÃ©m, nÃ£o serÃ¡ necessÃ¡rio utilizar todas as perguntas realizadas na pesquisa para enxergar todas as oportunidades ali postas, mas hÃ¡ dados triviais que precisam estar no projeto, pois auxiliam muito na anÃ¡lise dos dados:\n",
    "\n",
    "â€¢ CaracterÃ­sticas clÃ­nicas dos sintomas;  \n",
    "â€¢ CaracterÃ­sticas da populaÃ§Ã£o;  \n",
    "â€¢ CaracterÃ­sticas econÃ´micas da sociedade.  \n",
    "\n",
    "Dessa forma, acessar os dados do PNAD-COVID-19 do IBGE (https://covid19.ibge.gov.br/pnad-covid/) e organizar esta base para anÃ¡lise, utilizando Banco de Dados em Nuvem e trazendo as seguintes caracterÃ­sticas:\n",
    "\n",
    "a. UtilizaÃ§Ã£o de no mÃ¡ximo 20 questionamentos realizados na pesquisa;  \n",
    "b. Utilizar 3 meses para construÃ§Ã£o da soluÃ§Ã£o;  \n",
    "c. CaracterizaÃ§Ã£o dos sintomas clÃ­nicos da populaÃ§Ã£o;  \n",
    "d. Comportamento da populaÃ§Ã£o na Ã©poca da COVID-19;  \n",
    "e. CaracterÃ­sticas econÃ´micas da Sociedade;  \n",
    "\n",
    "Com objetivo de trazer uma breve anÃ¡lise dessas informaÃ§Ãµes, como foi a organizaÃ§Ã£o do banco, as perguntas selecionadas para a resposta do problema e quais seriam as principais aÃ§Ãµes que o hospital deverÃ¡ tomar em caso de um novo surto de COVID-19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d119f738",
   "metadata": {},
   "source": [
    "### Arquitetura de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41df532f",
   "metadata": {},
   "source": [
    "![Arquitetura](https://github.com/RicardViana/fiap-Big-Data/blob/main/arquitetura.jpeg?raw=true)\n",
    "\n",
    "1) Dados utilizado referente a Pesquisa Nacional por Amostra de DomicÃ­lios (PNAD COVID19) atravÃ©s do link [PNAD](https://www.ibge.gov.br/estatisticas/investigacoes-experimentais/estatisticas-experimentais/27946-divulgacao-semanal-pnadcovid1?t=microdados&utm_source=covid19&utm_medium=hotsite&utm_campaign=covid_19) *\n",
    "2) Devido a volumetria dos dados, Ã© feito o armazenamento no AWS S3 e usando camandas medalhÃ£o\n",
    "3) Realizar o carregamento dos da camanda GOLD para o banco de dados PostgreSQL criado via AWS RDS\n",
    "4) Construir a analise utilizando o Power BI conectado ao Data Mart\n",
    "\n",
    "\\* Os dados foram armazenados no GitHub para a construÃ§Ã£o desse trabalho"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44548dfe",
   "metadata": {},
   "source": [
    "### **ObservaÃ§Ãµes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc1e36e",
   "metadata": {},
   "source": [
    "#### **VS Code**\n",
    "\n",
    "Para correto funcionamento, sempre reiniciar o Kernel antes de rodar o cÃ³digo\n",
    "\n",
    "#### **Bibliotecas**\n",
    "\n",
    "Para correto funcionamento, caso nÃ£o tenha instalado as seguintes bibliotes: \n",
    "\n",
    "- `requests`\n",
    "- `pandas`  \n",
    "- `boto3`  \n",
    "- `python-dotenv`  \n",
    "- `SQLAlchemy`  \n",
    "- `psycopg2-binary`  \n",
    "\n",
    "Realizar a instalaÃ§Ã£o via PIP INSTALL (https://pypi.org/) ou CONDA-FORGE (https://anaconda.org/conda-forge)\n",
    "\n",
    "#### **ConexÃµes**\n",
    "\n",
    "Para realizar as conexÃµes Ã© utilizado um arquivo .env salvo no mesmo local do projeto e com a seguinte estrutura\n",
    "\n",
    "```ini\n",
    "# Credenciais do PostgreSQL PNAD\n",
    "POSTGRES_USER_PNAD=\n",
    "POSTGRES_PASSWORD_PNAD=\n",
    "POSTGRES_HOST_PNAD=\n",
    "POSTGRES_PORT_PNAD=5432\n",
    "POSTGRES_DB_PNAD=\n",
    "\n",
    "# Credenciais AWS\n",
    "aws_access_key_id=\n",
    "aws_secret_access_key=\n",
    "aws_session_token=\n",
    "\n",
    "region=us-east-1\n",
    "output=json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e629b614",
   "metadata": {},
   "source": [
    "#### **ConfiguraÃ§Ã£o do PostgreSQL na AWS RDS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cddb182",
   "metadata": {},
   "source": [
    "##### 1. **Criar instÃ¢ncia RDS com PostgreSQL (SandBox)**\n",
    "\n",
    "1. Acesse o console AWS â†’ [https://us-east-1.console.aws.amazon.com/rds/home?region=us-east-1#](https://console.aws.amazon.com/rds/)\n",
    "2. Clique em **Criar banco de dados**\n",
    "3. Selecione:\n",
    "   - **Tipo de banco:** PostgreSQL\n",
    "   - **VersÃ£o:** PostgreSQL 15 (ou mais recente)\n",
    "   - **Modelo de uso:** SandBox\n",
    "   - **Identificador da instÃ¢ncia:** `postgres-tc`\n",
    "   - **UsuÃ¡rio:** `postgres`\n",
    "   - **Senha:** crie uma senha segura\n",
    "4. Tipo de instÃ¢ncia: `db.t3.micro`\n",
    "5. Armazenamento: 20 GB (SSD General Purpose)\n",
    "6. **Acesso pÃºblico:** Habilitado (Sim)\n",
    "7. **Nome do banco de dados inicial:** `pnad_covid`\n",
    "8. Clique em **Criar banco de dados**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e2ef42",
   "metadata": {},
   "source": [
    "##### **2. Liberar o IP na VPC / Grupo de SeguranÃ§a (Security Group)**\n",
    "\n",
    "1. VÃ¡ para **EC2 > Grupos de SeguranÃ§a**\n",
    "2. Encontre o grupo associado Ã  instÃ¢ncia RDS\n",
    "3. Clique em **Editar regras de entrada**\n",
    "4. Adicione uma nova regra:\n",
    "   - Tipo: `PostgreSQL`\n",
    "   - Porta: `5432`\n",
    "   - Origem: `Seu IP` (ou `0.0.0.0/0` temporariamente para teste â€“ cuidado com isso em produÃ§Ã£o)\n",
    "5. Salve as alteraÃ§Ãµes.\n",
    "\n",
    "âœ… Agora o acesso externo ao banco estarÃ¡ liberado para seu IP\n",
    "\n",
    "**Obs.: Essa configuraÃ§Ã£o serÃ¡ feita apenas uma vez no VPC Default**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b274fe91",
   "metadata": {},
   "source": [
    "### **CÃ³digos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af7b5bc",
   "metadata": {},
   "source": [
    "#### **Importar bibliotecas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "434646bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar biblioteca completa\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import pandas as pd\n",
    "import os\n",
    "import boto3\n",
    "import sys\n",
    "import time \n",
    "import csv\n",
    "\n",
    "# Importar algo especifico de uma biblioteca\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text\n",
    "from botocore.exceptions import BotoCoreError, ClientError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf22db21",
   "metadata": {},
   "source": [
    "#### **FunÃ§Ãµes (DEF)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c5991ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar a conexÃ£o ao banco de dados\n",
    "def test_connection(engine):\n",
    "\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            \n",
    "            # Testar a versÃ£o do PostgreSQL\n",
    "            result = connection.execute(text(\"SELECT version();\"))\n",
    "            versao = result.fetchone()\n",
    "            print(\"âœ… Conectado com sucesso:\", versao[0])\n",
    "\n",
    "            # Listar as tabelas no schema pÃºblico\n",
    "            result = connection.execute(text(\"\"\"\n",
    "                SELECT table_name\n",
    "                FROM information_schema.tables\n",
    "                WHERE table_schema = 'public';\n",
    "            \"\"\"))\n",
    "            tabelas = result.fetchall()\n",
    "            print(\"ðŸ“„ Tabelas no banco:\")\n",
    "            for tabela in tabelas:\n",
    "                print(\"-\", tabela[0])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"âŒ Erro ao executar comandos:\", e)\n",
    "        sys.exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9a82527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular o tempo de execuÃ§Ã£o\n",
    "def calcular_tempo_execucao(tempo_inicio, tempo_final):\n",
    "    \"\"\"\n",
    "    Calcula e exibe o tempo total de execuÃ§Ã£o de um processo.\n",
    "\n",
    "    ParÃ¢metros:\n",
    "    tempo_inicio (float): O tempo de inÃ­cio capturado com time.time()\n",
    "    tempo_final (float): O tempo final capturado com time.time()\n",
    "\n",
    "    A funÃ§Ã£o nÃ£o retorna nada, apenas imprime o resultado formatado.\n",
    "    \"\"\"\n",
    "    # Calcula a diferenÃ§a total em segundos\n",
    "    tempo_total_segundos = tempo_final - tempo_inicio\n",
    "\n",
    "    # Converte o total de segundos para minutos e segundos\n",
    "    minutos, segundos = divmod(tempo_total_segundos, 60)\n",
    "\n",
    "    # Imprime o resultado\n",
    "    print(f\"\\nTempo total de execuÃ§Ã£o do cÃ³digo:\")\n",
    "    print(f\"{int(minutos)} minutos e {int(segundos)} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e210f6",
   "metadata": {},
   "source": [
    "#### **Variaveis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cb79bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nome do bucket S3 e subpastas\n",
    "s3_bucket = 'tech-challenge-fase3'\n",
    "s3_subpasta_bronze = 'bronze'\n",
    "s3_subpasta_silver = 'silver'\n",
    "s3_subpasta_gold = 'gold'\n",
    "pnad_consolidado = 'pnad_consolidado.parquet'\n",
    "\n",
    "bronze_prefix = f\"s3://{s3_bucket}/{s3_subpasta_bronze}/\" \n",
    "silver_prefix = f's3://{s3_bucket}/{s3_subpasta_silver}/'\n",
    "gold_prefix = f's3://{s3_bucket}/{s3_subpasta_gold}/'\n",
    "\n",
    "# Nome arquivo .parquet\n",
    "nome_arquivo_gold = 'pnad_final_tratado.parquet'\n",
    "\n",
    "# Caminho de saida e entrada camada silver\n",
    "caminho_saida_silver = silver_prefix + 'pnad_consolidado_enriquecido.parquet'\n",
    "caminho_entrada_silver = silver_prefix + 'pnad_consolidado_enriquecido.parquet'\n",
    "caminho_saida_gold = gold_prefix + 'pnad_final_tratado.parquet'\n",
    "caminho_completo_gold = gold_prefix + nome_arquivo_gold\n",
    "\n",
    "# Caminho do Github com dados do PNAD\n",
    "api_url = 'https://api.github.com/repos/RicardViana/fiap-Big-Data/contents/PNAD-COVID/Microdados'\n",
    "\n",
    "# Caminho do Github com dados do cÃ³digo IBGE UF\n",
    "link_codigo_uf = 'https://raw.githubusercontent.com/RicardViana/tabela-uf-ibge/refs/heads/main/codigo_uf.csv'\n",
    "\n",
    "# Nome da tabela no PostgreSQL\n",
    "nome_tabela = 'questionario_pnad_covid'\n",
    "tabela_origem = nome_tabela\n",
    "\n",
    "# Quantidade do chunksize\n",
    "chunksize = 100000\n",
    "\n",
    "# Carregar novamente a tabela \n",
    "carregar_tabela = 's'\n",
    "\n",
    "# Role para usar no AWS Glue\n",
    "role_arn = 'arn:aws:iam::430854566059:role/LabRole'\n",
    "\n",
    "# Nome do banco e crawler do AWS Glue\n",
    "db_name = 'pnad_db'\n",
    "crawler_name = 'pnad_gold_crawler'\n",
    "TablePrefix = 'pnad_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ed8d34",
   "metadata": {},
   "source": [
    "#### **ConfiguraÃ§Ã£o AWS e Banco de Dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7199af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar as credencias do .env\n",
    "load_dotenv()\n",
    "\n",
    "# ConfiguraÃ§Ã£o storage_options\n",
    "storage_options = {\n",
    "    \"key\": os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "    \"secret\": os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "    \"token\": os.getenv('AWS_SESSION_TOKEN')\n",
    "}\n",
    "\n",
    "# ConfiguraÃ§Ã£o S3\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "    aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "    aws_session_token=os.getenv('AWS_SESSION_TOKEN'),\n",
    "    region_name=os.getenv('AWS_REGION')\n",
    ")\n",
    "\n",
    "# Credenciais do PostgreSQL\n",
    "usuario_pg = os.getenv(\"POSTGRES_USER_PNAD\")\n",
    "senha_pg = os.getenv(\"POSTGRES_PASSWORD_PNAD\")\n",
    "host_pg = os.getenv(\"POSTGRES_HOST_PNAD\")\n",
    "porta_pg = os.getenv(\"POSTGRES_PORT_PNAD\")\n",
    "banco_pg = os.getenv(\"POSTGRES_DB_PNAD\")\n",
    "\n",
    "# ConfiguraÃ§Ã£o Glue\n",
    "glue_client = boto3.client('glue', region_name=os.getenv('region'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c42cb0",
   "metadata": {},
   "source": [
    "#### **Validar ConexÃµes e criar engine banco de dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59e21536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Conectado Ã  conta\n",
      "\n",
      "UserId: AROAWIUHO6CVTPL5SPXB7:user4308167=ricardviana1@gmail.com\n",
      "Account: 430854566059\n",
      "Arn: arn:aws:sts::430854566059:assumed-role/voclabs/user4308167=ricardviana1@gmail.com\n"
     ]
    }
   ],
   "source": [
    "# Validar conexÃ£o com a AWS atravÃ©s do .env\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    sts_client = boto3.client(\n",
    "        'sts',\n",
    "        aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "        aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "        aws_session_token=os.getenv('AWS_SESSION_TOKEN'),\n",
    "        region_name=os.getenv('AWS_REGION')\n",
    "    )\n",
    "    \n",
    "    identity = sts_client.get_caller_identity()\n",
    "    print(\"âœ… Conectado Ã  conta\\n\")\n",
    "    print(\"UserId:\", identity[\"UserId\"])\n",
    "    print(\"Account:\", identity[\"Account\"])\n",
    "    print(\"Arn:\", identity[\"Arn\"])\n",
    "\n",
    "except (BotoCoreError, ClientError) as e:\n",
    "    print(\"âŒ Erro ao conectar Ã  AWS. Verifique suas credenciais e tente novamente.\")\n",
    "    print(\"Detalhes do erro:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce23d8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Conectado com sucesso: PostgreSQL 17.4 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 12.4.0, 64-bit\n",
      "ðŸ“„ Tabelas no banco:\n",
      "- questionario_pnad_covid\n"
     ]
    }
   ],
   "source": [
    "# Criar engine com banco \n",
    "engine = create_engine(f\"postgresql+psycopg2://{usuario_pg}:{senha_pg}@{host_pg}:{porta_pg}/{banco_pg}\")\n",
    "\n",
    "# Testar a conexÃ£o\n",
    "test_connection(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e412ee",
   "metadata": {},
   "source": [
    "#### **ETL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2b216e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registrar o tempo inicio do cÃ³digo\n",
    "variavel_tempo_inicio = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "510a915e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando PrÃ©-Scan para definir estrutura unificada\n",
      "Analisando cabeÃ§alho de: PNAD_COVID_052020.zip\n",
      "Analisando cabeÃ§alho de: PNAD_COVID_062020.zip\n",
      "Analisando cabeÃ§alho de: PNAD_COVID_072020.zip\n",
      "Analisando cabeÃ§alho de: PNAD_COVID_082020.zip\n",
      "Analisando cabeÃ§alho de: PNAD_COVID_092020.zip\n",
      "Analisando cabeÃ§alho de: PNAD_COVID_102020.zip\n",
      "Analisando cabeÃ§alho de: PNAD_COVID_112020.zip\n",
      "\n",
      "âœ… Estrutura unificada definida com 148 colunas\n",
      "\n",
      "Criando a tabela 'questionario_pnad_covid' com a estrutura completa\n",
      "âœ… Tabela criada com sucesso\n",
      "\n",
      "Iniciando carga de dados para a tabela 'questionario_pnad_covid'\n",
      "\n",
      "Processando arquivo: PNAD_COVID_052020.zip\n",
      "Carregando chunk 1 via COPY\n",
      "Carregando chunk 2 via COPY\n",
      "Carregando chunk 3 via COPY\n",
      "Carregando chunk 4 via COPY\n",
      "\n",
      "Processando arquivo: PNAD_COVID_062020.zip\n",
      "Carregando chunk 1 via COPY\n",
      "Carregando chunk 2 via COPY\n",
      "Carregando chunk 3 via COPY\n",
      "Carregando chunk 4 via COPY\n",
      "\n",
      "Processando arquivo: PNAD_COVID_072020.zip\n",
      "Carregando chunk 1 via COPY\n",
      "Carregando chunk 2 via COPY\n",
      "Carregando chunk 3 via COPY\n",
      "Carregando chunk 4 via COPY\n",
      "\n",
      "Processando arquivo: PNAD_COVID_082020.zip\n",
      "Carregando chunk 1 via COPY\n",
      "Carregando chunk 2 via COPY\n",
      "Carregando chunk 3 via COPY\n",
      "Carregando chunk 4 via COPY\n",
      "\n",
      "Processando arquivo: PNAD_COVID_092020.zip\n",
      "Carregando chunk 1 via COPY\n",
      "Carregando chunk 2 via COPY\n",
      "Carregando chunk 3 via COPY\n",
      "Carregando chunk 4 via COPY\n",
      "\n",
      "Processando arquivo: PNAD_COVID_102020.zip\n",
      "Carregando chunk 1 via COPY\n",
      "Carregando chunk 2 via COPY\n",
      "Carregando chunk 3 via COPY\n",
      "Carregando chunk 4 via COPY\n",
      "\n",
      "Processando arquivo: PNAD_COVID_112020.zip\n",
      "Carregando chunk 1 via COPY\n",
      "Carregando chunk 2 via COPY\n",
      "Carregando chunk 3 via COPY\n",
      "Carregando chunk 4 via COPY\n",
      "\n",
      "âœ… Carga de dados concluÃ­da com sucesso!\n",
      "\n",
      "Adicionando colunas de metadados Ã  tabela 'questionario_pnad_covid'\n",
      "âœ… Colunas 'data_hora_carga' e 'usuario_carga' adicionadas com sucesso\n"
     ]
    }
   ],
   "source": [
    "# Criar tabela + Carregar dados no PostgreSQL\n",
    "\n",
    "# Criar da tabela atravÃ©s de um prÃ©-scan\n",
    "if carregar_tabela.lower() == 'n':\n",
    "    print(f'Etapa de carregar os dados do Github para o PostgreSQL nÃ£o realizada pois a variavel carregar_tabela Ã© `n`')\n",
    "\n",
    "else:\n",
    "\n",
    "    # Criar a tabela\n",
    "    print(\"Iniciando PrÃ©-Scan para definir estrutura unificada\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(api_url) # Fazer requisiÃ§Ã£o HTTP\n",
    "        response.raise_for_status() # Verificar se a requisiÃ§Ã£o foi bem feita\n",
    "        files = response.json() # Transformar em uma lista de dicionario\n",
    "        colunas_unicas = set() # Criar um conjuto vazio\n",
    "\n",
    "        # Iterar sobre cada item no diretÃ³rio do GitHub\n",
    "        for file_info in files:\n",
    "\n",
    "            if file_info['name'].endswith('.zip'):\n",
    "\n",
    "                print(f\"Analisando cabeÃ§alho de: {file_info['name']}\")\n",
    "\n",
    "                zip_url = file_info['download_url']\n",
    "                r_zip = requests.get(zip_url) # Fazer o download do arquivo\n",
    "                zip_content = io.BytesIO(r_zip.content) # Arquivo temporario salvo em memoria\n",
    "\n",
    "                with zipfile.ZipFile(zip_content) as z: # Abrir o arquivo .zip\n",
    "                    csv_filename = next(f for f in z.namelist() if f.endswith('.csv'))\n",
    "\n",
    "                    with z.open(csv_filename) as csv_file: # Abrir o arquivo .csv\n",
    "                        df_header = pd.read_csv(csv_file, nrows=1) # LÃª apenas as primeiras linhas para pegar o cabeÃ§alho rapidamente\n",
    "                        colunas_unicas.update(df_header.columns.tolist())\n",
    "\n",
    "        lista_final_colunas = sorted(list(colunas_unicas))\n",
    "        print(f\"\\nâœ… Estrutura unificada definida com {len(lista_final_colunas)} colunas\")\n",
    "\n",
    "        # Cria um DataFrame vazio com a estrutura completa e unificada\n",
    "        df_schema_unificado = pd.DataFrame(columns=lista_final_colunas)\n",
    "\n",
    "        # Cria a tabela no PostgreSQL com esta estrutura completa\n",
    "        print(f\"\\nCriando a tabela '{nome_tabela}' com a estrutura completa\")\n",
    "        df_schema_unificado.to_sql(nome_tabela, engine, if_exists='replace', index=False)\n",
    "        print(\"âœ… Tabela criada com sucesso\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erro durante o prÃ©-scan ou criaÃ§Ã£o da tabela: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Carregar os dados via Copy\n",
    "    print(f\"\\nIniciando carga de dados para a tabela '{nome_tabela}'\")\n",
    "    \n",
    "    try:\n",
    "        with engine.connect() as connection: # Abrir conexÃ£o \n",
    "            raw_conn = connection.connection\n",
    "\n",
    "            with raw_conn.cursor() as cursor: # Criar o cursor para executar os cÃ³digos\n",
    "\n",
    "                # Iterar sobre cada item no diretÃ³rio do GitHub\n",
    "                for file_info in files:\n",
    "\n",
    "                    if file_info['name'].endswith('.zip'):\n",
    "                        \n",
    "                        zip_name = file_info['name']\n",
    "                        print(f\"\\nProcessando arquivo: {zip_name}\")\n",
    "\n",
    "                        zip_url = file_info['download_url']\n",
    "                        r_zip = requests.get(zip_url) # Fazer o download do arquivo\n",
    "                        zip_content = io.BytesIO(r_zip.content) # Arquivo temporario salvo em memoria\n",
    "\n",
    "                        with zipfile.ZipFile(zip_content) as z: # Abrir o arquivo .zip\n",
    "                            csv_filename = next(f for f in z.namelist() if f.endswith('.csv'))\n",
    "\n",
    "                            with z.open(csv_filename) as csv_file: # Abrir o arquivo .csv\n",
    "                                \n",
    "                                # Carregar o CSV por pedaÃ§os (chunks)\n",
    "                                chunk_iterator = pd.read_csv(csv_file, chunksize=chunksize, low_memory=False)\n",
    "                                \n",
    "                                # Iterar sobre cada pedaÃ§o e carregar para o banco de dados\n",
    "                                for i, chunk in enumerate(chunk_iterator):\n",
    "                                    \n",
    "                                    buffer = io.StringIO()\n",
    "\n",
    "                                    # Garantir que o chunk tenha todas as colunas da tabela, preenchendo as faltantes com None\n",
    "                                    chunk_reindexed = chunk.reindex(columns=lista_final_colunas)\n",
    "                                    chunk_reindexed.to_csv(buffer, index=False, header=False, quoting=csv.QUOTE_MINIMAL)\n",
    "                                    buffer.seek(0)\n",
    "                                    \n",
    "                                    print(f\"Carregando chunk {i+1} via COPY\")\n",
    "                                    sql_copy_command = f\"COPY {nome_tabela} FROM STDIN WITH (FORMAT CSV, HEADER FALSE)\"\n",
    "                                    cursor.copy_expert(sql_copy_command, buffer)\n",
    "                \n",
    "                raw_conn.commit()\n",
    "                print(f\"\\nâœ… Carga de dados concluÃ­da com sucesso!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erro durante a carga dos dados: {e}\")\n",
    "        if 'raw_conn' in locals() and raw_conn and not raw_conn.closed:\n",
    "            raw_conn.rollback()\n",
    "        raise\n",
    "\n",
    "    # Inserir a coluna de data e hora + usuario\n",
    "    print(f\"\\nAdicionando colunas de metadados Ã  tabela '{nome_tabela}'\")\n",
    "\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            \n",
    "            # Comando para adicionar a coluna de data/hora da carga\n",
    "            # IF NOT EXISTS: Garante que o script nÃ£o darÃ¡ erro se a coluna jÃ¡ existir\n",
    "            # DEFAULT NOW(): Diz ao PostgreSQL para preencher este campo automaticamente com a data e hora atuais\n",
    "            sql_add_timestamp = text(f\"\"\"\n",
    "                ALTER TABLE {nome_tabela}\n",
    "                ADD COLUMN IF NOT EXISTS data_hora_carga TIMESTAMPTZ DEFAULT NOW();\n",
    "            \"\"\")\n",
    "            \n",
    "            # Comando para adicionar a coluna do usuÃ¡rio que fez a carga\n",
    "            # DEFAULT CURRENT_USER: Preenche automaticamente com o nome do usuÃ¡rio do banco conectado.\n",
    "            sql_add_user = text(f\"\"\"\n",
    "                ALTER TABLE {nome_tabela}\n",
    "                ADD COLUMN IF NOT EXISTS usuario_carga VARCHAR(255) DEFAULT CURRENT_USER;\n",
    "            \"\"\")\n",
    "\n",
    "            # Executa os comandos\n",
    "            connection.execute(sql_add_timestamp)\n",
    "            connection.execute(sql_add_user)\n",
    "            \n",
    "            # Efetiva as alteraÃ§Ãµes na estrutura da tabela\n",
    "            connection.commit() \n",
    "\n",
    "            print(\"âœ… Colunas 'data_hora_carga' e 'usuario_carga' adicionadas com sucesso\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erro ao adicionar colunas de metadados: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f80345c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando Processo de ValidaÃ§Ã£o e CriaÃ§Ã£o no S3\n",
      "\n",
      "Validando o bucket 'tech-challenge-fase3'\n",
      "âž¡ï¸  Bucket 'tech-challenge-fase3' jÃ¡ existe\n",
      "\n",
      "Validando as subpastas no bucket 'tech-challenge-fase3'\n",
      "âž¡ï¸ Pasta 'bronze/' jÃ¡ existe\n",
      "âž¡ï¸ Pasta 'silver/' jÃ¡ existe\n",
      "âž¡ï¸ Pasta 'gold/' jÃ¡ existe\n",
      "\n",
      "Processo Finalizado\n"
     ]
    }
   ],
   "source": [
    "# Criar Bucket e subpastas (camadas medalhÃµes)\n",
    "\n",
    "# Lista com o nome das subpastas\n",
    "pastas_a_criar = [s3_subpasta_bronze, s3_subpasta_silver, s3_subpasta_gold]\n",
    "\n",
    "# Armazenar a regiÃ£o da conexÃ£o com a AWS\n",
    "aws_region = s3_client.meta.region_name\n",
    "\n",
    "print(f\"\\nIniciando Processo de ValidaÃ§Ã£o e CriaÃ§Ã£o no S3\")\n",
    "\n",
    "print(f\"\\nValidando o bucket '{s3_bucket}'\")\n",
    "bucket_pronto = False\n",
    "\n",
    "# Validar se o Bucket esta criado\n",
    "try:\n",
    "    s3_client.head_bucket(Bucket=s3_bucket)\n",
    "    print(f\"âž¡ï¸  Bucket '{s3_bucket}' jÃ¡ existe\")\n",
    "    bucket_pronto = True\n",
    "\n",
    "except ClientError as e:\n",
    "    \n",
    "    if e.response['Error']['Code'] == '404':\n",
    "        print(f\"Bucket '{s3_bucket}' nÃ£o encontrado. Tentando criar\")\n",
    "\n",
    "        try:\n",
    "            if aws_region == \"us-east-1\":\n",
    "                s3_client.create_bucket(Bucket=s3_bucket)\n",
    "\n",
    "            else:\n",
    "                location = {'LocationConstraint': aws_region}\n",
    "                s3_client.create_bucket(\n",
    "                    Bucket=s3_bucket,\n",
    "                    CreateBucketConfiguration=location\n",
    "                )\n",
    "            print(f\"âœ… Bucket '{s3_bucket}' criado com sucesso!\")\n",
    "            bucket_pronto = True\n",
    "\n",
    "        except Exception as create_e:\n",
    "            print(f\"âŒ Falha ao tentar criar o bucket: {create_e}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"âŒ Erro de permissÃ£o ou outro problema ao verificar o bucket: {e}\")\n",
    "\n",
    "# Validar se as subpastas estÃ£o criadas\n",
    "if bucket_pronto:\n",
    "    print(f\"\\nValidando as subpastas no bucket '{s3_bucket}'\")\n",
    "\n",
    "    for nome_pasta in pastas_a_criar:\n",
    "        chave_pasta = nome_pasta if nome_pasta.endswith('/') else nome_pasta + '/'\n",
    "        \n",
    "        try:\n",
    "            s3_client.head_object(Bucket=s3_bucket, Key=chave_pasta)\n",
    "            print(f\"âž¡ï¸ Pasta '{chave_pasta}' jÃ¡ existe\")\n",
    "\n",
    "        except ClientError as e:\n",
    "\n",
    "            if e.response['Error']['Code'] == '404':\n",
    "                try:\n",
    "                    s3_client.put_object(\n",
    "                        Bucket=s3_bucket,\n",
    "                        Key=chave_pasta,\n",
    "                        Body=''\n",
    "                    )\n",
    "                    print(f\"âœ… Pasta '{chave_pasta}' criada com sucesso\")\n",
    "\n",
    "                except Exception as create_e:\n",
    "                    print(f\"âŒ Falha ao TENTAR CRIAR a pasta '{chave_pasta}': {create_e}\")\n",
    "\n",
    "            else:\n",
    "                print(f\"âŒ Erro ao verificar a pasta '{chave_pasta}': {e}\")\n",
    "else:\n",
    "    print(\"\\nCriaÃ§Ã£o das pastas abortada, pois houve um problema com o bucket\")\n",
    "\n",
    "print(\"\\nProcesso Finalizado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24cf26c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando extraÃ§Ã£o da tabela 'questionario_pnad_covid'\n",
      "\n",
      "Calculando o total de registros a serem processados\n",
      "Total de registros encontrado: 2650459\n",
      "\n",
      "Aguarde, executando o comando SELECT * FROM questionario_pnad_covid e isso pode levar um determinado tempo\n",
      "\n",
      "Processando e salvando dados na Camada Bronze em 's3://tech-challenge-fase3/bronze/'\n",
      "\n",
      "Processando chunk 1 (4%): 100000 linhas\n",
      "âœ… Chunk 1 salvo em 's3://tech-challenge-fase3/bronze/part-0000.parquet'\n",
      "\n",
      "Processando chunk 2 (8%): 100000 linhas\n",
      "âœ… Chunk 2 salvo em 's3://tech-challenge-fase3/bronze/part-0001.parquet'\n",
      "\n",
      "Processando chunk 3 (11%): 100000 linhas\n",
      "âœ… Chunk 3 salvo em 's3://tech-challenge-fase3/bronze/part-0002.parquet'\n",
      "\n",
      "Processando chunk 4 (15%): 100000 linhas\n",
      "âœ… Chunk 4 salvo em 's3://tech-challenge-fase3/bronze/part-0003.parquet'\n",
      "\n",
      "Processando chunk 5 (19%): 100000 linhas\n",
      "âœ… Chunk 5 salvo em 's3://tech-challenge-fase3/bronze/part-0004.parquet'\n",
      "\n",
      "Processando chunk 6 (23%): 100000 linhas\n",
      "âœ… Chunk 6 salvo em 's3://tech-challenge-fase3/bronze/part-0005.parquet'\n",
      "\n",
      "Processando chunk 7 (26%): 100000 linhas\n",
      "âœ… Chunk 7 salvo em 's3://tech-challenge-fase3/bronze/part-0006.parquet'\n",
      "\n",
      "Processando chunk 8 (30%): 100000 linhas\n",
      "âœ… Chunk 8 salvo em 's3://tech-challenge-fase3/bronze/part-0007.parquet'\n",
      "\n",
      "Processando chunk 9 (34%): 100000 linhas\n",
      "âœ… Chunk 9 salvo em 's3://tech-challenge-fase3/bronze/part-0008.parquet'\n",
      "\n",
      "Processando chunk 10 (38%): 100000 linhas\n",
      "âœ… Chunk 10 salvo em 's3://tech-challenge-fase3/bronze/part-0009.parquet'\n",
      "\n",
      "Processando chunk 11 (42%): 100000 linhas\n",
      "âœ… Chunk 11 salvo em 's3://tech-challenge-fase3/bronze/part-0010.parquet'\n",
      "\n",
      "Processando chunk 12 (45%): 100000 linhas\n",
      "âœ… Chunk 12 salvo em 's3://tech-challenge-fase3/bronze/part-0011.parquet'\n",
      "\n",
      "Processando chunk 13 (49%): 100000 linhas\n",
      "âœ… Chunk 13 salvo em 's3://tech-challenge-fase3/bronze/part-0012.parquet'\n",
      "\n",
      "Processando chunk 14 (53%): 100000 linhas\n",
      "âœ… Chunk 14 salvo em 's3://tech-challenge-fase3/bronze/part-0013.parquet'\n",
      "\n",
      "Processando chunk 15 (57%): 100000 linhas\n",
      "âœ… Chunk 15 salvo em 's3://tech-challenge-fase3/bronze/part-0014.parquet'\n",
      "\n",
      "Processando chunk 16 (60%): 100000 linhas\n",
      "âœ… Chunk 16 salvo em 's3://tech-challenge-fase3/bronze/part-0015.parquet'\n",
      "\n",
      "Processando chunk 17 (64%): 100000 linhas\n",
      "âœ… Chunk 17 salvo em 's3://tech-challenge-fase3/bronze/part-0016.parquet'\n",
      "\n",
      "Processando chunk 18 (68%): 100000 linhas\n",
      "âœ… Chunk 18 salvo em 's3://tech-challenge-fase3/bronze/part-0017.parquet'\n",
      "\n",
      "Processando chunk 19 (72%): 100000 linhas\n",
      "âœ… Chunk 19 salvo em 's3://tech-challenge-fase3/bronze/part-0018.parquet'\n",
      "\n",
      "Processando chunk 20 (75%): 100000 linhas\n",
      "âœ… Chunk 20 salvo em 's3://tech-challenge-fase3/bronze/part-0019.parquet'\n",
      "\n",
      "Processando chunk 21 (79%): 100000 linhas\n",
      "âœ… Chunk 21 salvo em 's3://tech-challenge-fase3/bronze/part-0020.parquet'\n",
      "\n",
      "Processando chunk 22 (83%): 100000 linhas\n",
      "âœ… Chunk 22 salvo em 's3://tech-challenge-fase3/bronze/part-0021.parquet'\n",
      "\n",
      "Processando chunk 23 (87%): 100000 linhas\n",
      "âœ… Chunk 23 salvo em 's3://tech-challenge-fase3/bronze/part-0022.parquet'\n",
      "\n",
      "Processando chunk 24 (91%): 100000 linhas\n",
      "âœ… Chunk 24 salvo em 's3://tech-challenge-fase3/bronze/part-0023.parquet'\n",
      "\n",
      "Processando chunk 25 (94%): 100000 linhas\n",
      "âœ… Chunk 25 salvo em 's3://tech-challenge-fase3/bronze/part-0024.parquet'\n",
      "\n",
      "Processando chunk 26 (98%): 100000 linhas\n",
      "âœ… Chunk 26 salvo em 's3://tech-challenge-fase3/bronze/part-0025.parquet'\n",
      "\n",
      "Processando chunk 27 (100%): 50459 linhas\n",
      "âœ… Chunk 27 salvo em 's3://tech-challenge-fase3/bronze/part-0026.parquet'\n",
      "\n",
      "âœ… ExtraÃ§Ã£o concluÃ­da! Total de 2650459 registros salvos em mÃºltiplos arquivos Parquet\n"
     ]
    }
   ],
   "source": [
    "# Carregar os dados do PostgreSQL para o S3 \n",
    "\n",
    "if carregar_tabela.lower() == 'n':\n",
    "    print(f'Etapa de carregar os dados do PostgreSQL para o S3 nÃ£o realizada pois a variavel carregar_tabela Ã© `n`')\n",
    "\n",
    "else:\n",
    "    print(f\"Iniciando extraÃ§Ã£o da tabela '{tabela_origem}'\")\n",
    "\n",
    "    try:\n",
    "        print(\"\\nCalculando o total de registros a serem processados\")\n",
    "        with engine.connect() as connection:\n",
    "            total_de_linhas = connection.execute(text(f\"SELECT COUNT(*) FROM {tabela_origem}\")).scalar()\n",
    "        print(f\"Total de registros encontrado: {total_de_linhas}\")\n",
    "\n",
    "        sql_query = f\"SELECT * FROM {tabela_origem}\"\n",
    "        print(f\"\\nAguarde, executando o comando {sql_query} e isso pode levar um determinado tempo\")\n",
    "        chunk_iterator = pd.read_sql_query(sql_query, engine, chunksize=chunksize)\n",
    "        \n",
    "        total_registros_processados = 0\n",
    "\n",
    "        print(f\"\\nProcessando e salvando dados na Camada Bronze em '{bronze_prefix}'\")\n",
    "        for i, chunk_df in enumerate(chunk_iterator):\n",
    "            \n",
    "            num_linhas_chunk = len(chunk_df)\n",
    "            total_registros_processados += num_linhas_chunk\n",
    "            \n",
    "            percentual_concluido = (total_registros_processados / total_de_linhas) * 100\n",
    "            print(f\"\\nProcessando chunk {i+1} ({percentual_concluido:.0f}%): {num_linhas_chunk} linhas\")\n",
    "            \n",
    "            caminho_arquivo_parquet = f\"{bronze_prefix}part-{i:04d}.parquet\"\n",
    "            \n",
    "            chunk_df.to_parquet(\n",
    "                caminho_arquivo_parquet,\n",
    "                engine='pyarrow',\n",
    "                index=False,\n",
    "                storage_options=storage_options\n",
    "            )\n",
    "            print(f\"âœ… Chunk {i+1} salvo em '{caminho_arquivo_parquet}'\")\n",
    "\n",
    "        print(f\"\\nâœ… ExtraÃ§Ã£o concluÃ­da! Total de {total_registros_processados} registros salvos em mÃºltiplos arquivos Parquet\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erro durante a extraÃ§Ã£o em chunks: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0246ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lendo tabela com os cÃ³digos do IBGE de: https://raw.githubusercontent.com/RicardViana/tabela-uf-ibge/refs/heads/main/codigo_uf.csv\n",
      "Data Frame criado com sucesso\n"
     ]
    }
   ],
   "source": [
    "# Ler arquivos CSV Codigo IBGE e gerar Data Frame\n",
    "print(f\"\\nLendo tabela com os cÃ³digos do IBGE de: {link_codigo_uf}\")\n",
    "df_uf = pd.read_csv(link_codigo_uf, sep=\",\")\n",
    "print(\"Data Frame criado com sucesso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb0d492c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo arquivos da camada Bronze no S3\n",
      "Lendo s3://tech-challenge-fase3/bronze/part-0000.parquet\n",
      "Lendo s3://tech-challenge-fase3/bronze/part-0001.parquet\n",
      "Lendo s3://tech-challenge-fase3/bronze/part-0002.parquet\n",
      "Lendo s3://tech-challenge-fase3/bronze/part-0003.parquet\n",
      "Lendo s3://tech-challenge-fase3/bronze/part-0004.parquet\n",
      "Lendo s3://tech-challenge-fase3/bronze/part-0005.parquet\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms3_bucket\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLendo \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m     df_temp \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     lista_de_dataframes\u001b[38;5;241m.\u001b[39mappend(df_temp)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Consolidar os data frame\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ricar\\anaconda3\\envs\\ambiente_fase3\\Lib\\site-packages\\pandas\\io\\parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[0;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ricar\\anaconda3\\envs\\ambiente_fase3\\Lib\\site-packages\\pandas\\io\\parquet.py:274\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[0;32m    268\u001b[0m     path,\n\u001b[0;32m    269\u001b[0m     filesystem,\n\u001b[0;32m    270\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    271\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    272\u001b[0m )\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m     result \u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mto_pandas(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_pandas_kwargs)\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ricar\\anaconda3\\envs\\ambiente_fase3\\Lib\\site-packages\\pyarrow\\parquet\\core.py:3002\u001b[0m, in \u001b[0;36mread_table\u001b[1;34m(source, columns, use_threads, metadata, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit)\u001b[0m\n\u001b[0;32m   2991\u001b[0m         \u001b[38;5;66;03m# TODO test that source is not a directory or a list\u001b[39;00m\n\u001b[0;32m   2992\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m ParquetFile(\n\u001b[0;32m   2993\u001b[0m             source, metadata\u001b[38;5;241m=\u001b[39mmetadata, read_dictionary\u001b[38;5;241m=\u001b[39mread_dictionary,\n\u001b[0;32m   2994\u001b[0m             memory_map\u001b[38;5;241m=\u001b[39mmemory_map, buffer_size\u001b[38;5;241m=\u001b[39mbuffer_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2999\u001b[0m             thrift_container_size_limit\u001b[38;5;241m=\u001b[39mthrift_container_size_limit,\n\u001b[0;32m   3000\u001b[0m         )\n\u001b[1;32m-> 3002\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3003\u001b[0m \u001b[43m                        \u001b[49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3005\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   3006\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_legacy_dataset=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to get the legacy behaviour is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3007\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated as of pyarrow 8.0.0, and the legacy implementation will \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3008\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe removed in a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3009\u001b[0m     \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m   3011\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_prefixes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ricar\\anaconda3\\envs\\ambiente_fase3\\Lib\\site-packages\\pyarrow\\parquet\\core.py:2630\u001b[0m, in \u001b[0;36m_ParquetDatasetV2.read\u001b[1;34m(self, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[0;32m   2622\u001b[0m         index_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   2623\u001b[0m             col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m _get_pandas_index_columns(metadata)\n\u001b[0;32m   2624\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m   2625\u001b[0m         ]\n\u001b[0;32m   2626\u001b[0m         columns \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2627\u001b[0m             \u001b[38;5;28mlist\u001b[39m(columns) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(index_columns) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(columns))\n\u001b[0;32m   2628\u001b[0m         )\n\u001b[1;32m-> 2630\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filter_expression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2632\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\n\u001b[0;32m   2633\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2635\u001b[0m \u001b[38;5;66;03m# if use_pandas_metadata, restore the pandas metadata (which gets\u001b[39;00m\n\u001b[0;32m   2636\u001b[0m \u001b[38;5;66;03m# lost if doing a specific `columns` selection in to_table)\u001b[39;00m\n\u001b[0;32m   2637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pandas_metadata:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Carregar dados para camada silver\n",
    "\n",
    "# Ler arquivos CSV PNAD COVID do S3, consolidar e gerar Data Frame\n",
    "response = s3_client.list_objects_v2(Bucket=s3_bucket, Prefix=f'{s3_subpasta_bronze}/') # Listar todos os objetos (arquivos) dentro do S3\n",
    "bronze_files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.parquet')] # Pegar apenas arquivos .csv\n",
    "lista_de_dataframes = []\n",
    "\n",
    "print(\"Lendo arquivos da camada Bronze no S3\")\n",
    "\n",
    "# Iterar sobre os arquivos\n",
    "for file_key in bronze_files:\n",
    "\n",
    "    file_path = f\"s3://{s3_bucket}/{file_key}\"\n",
    "    print(f\"Lendo {file_path}\")\n",
    "    df_temp = pd.read_parquet(file_path, storage_options=storage_options)\n",
    "    lista_de_dataframes.append(df_temp)\n",
    "\n",
    "# Consolidar os data frame\n",
    "df_consolidado = pd.concat(lista_de_dataframes, ignore_index=True)\n",
    "print(f\"\\nConsolidaÃ§Ã£o concluÃ­da e {len(df_consolidado)} linhas lidas da camada Bronze\")\n",
    "\n",
    "# Converter a coluna UF e CÃ³digo para objetct\n",
    "df_consolidado[\"UF\"] = df_consolidado[\"UF\"].astype(str)\n",
    "df_uf[\"CÃ³digo\"] = df_uf[\"CÃ³digo\"].astype(str)\n",
    "\n",
    "# Relacionar os Data Frame\n",
    "print(\"\\nIniciando enriquecimento dos dados com merge com o Data Frame df_uf\")\n",
    "df_silver = pd.merge(\n",
    "    df_consolidado,\n",
    "    df_uf,\n",
    "    how='left',\n",
    "    left_on='UF', \n",
    "    right_on='CÃ³digo'\n",
    ")\n",
    "\n",
    "# Remover colunas nÃ£o necessarias e ajustar o nome das colunas\n",
    "df_silver = df_silver.drop(columns=[\"CÃ³digo\"])\n",
    "df_silver = df_silver.rename(columns={\"UF_x\": \"UF\", \"UF_y\": \"UF_Nome\", \"RegiÃ£o\": \"Regiao\"})\n",
    "print(\"Enriquecimento concluÃ­do\")\n",
    "\n",
    "colunas_para_tratar = [\"A006A\", \"A006B\"]\n",
    "\n",
    "# Iterar sobre cada coluna da lista\n",
    "for coluna in colunas_para_tratar:\n",
    "    \n",
    "    # Passo 1: Converter a coluna para um tipo numÃ©rico (float).\n",
    "    # 'errors='coerce'' Ã© muito Ãºtil: se ele encontrar um texto que nÃ£o consegue converter\n",
    "    # (ex: \"nÃ£o aplicÃ¡vel\"), ele transforma esse valor em NaN (nulo) em vez de dar erro.\n",
    "    df_silver[coluna] = pd.to_numeric(df_silver[coluna], errors='coerce')\n",
    "    \n",
    "    # Passo 2: Agora que tudo Ã© numÃ©rico (ou NaN), podemos preencher os nulos com 0.\n",
    "    df_silver[coluna] = df_silver[coluna].fillna(0)\n",
    "    \n",
    "    # Passo 3: Finalmente, com a coluna limpa e sem decimais indesejados, convertemos para inteiro.\n",
    "    df_silver[coluna] = df_silver[coluna].astype(int)\n",
    "\n",
    "# Ver o resultado final\n",
    "print(\"\\nData Frame com os primeiros registros\")\n",
    "display(df_silver.head(5))\n",
    "\n",
    "print(\"Data Frame com os ultimos registros\")\n",
    "display(df_silver.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd98b217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar Data Frame df_silver na camada silver\n",
    "print(f\"\\nSalvando dados da camada Silver em: {caminho_saida_silver}\")\n",
    "\n",
    "df_silver.to_parquet(\n",
    "    caminho_saida_silver,\n",
    "    index=False,\n",
    "    storage_options=storage_options\n",
    ")\n",
    "\n",
    "print(\"âœ… Sucesso! Camada Silver criada e salva no S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f2a4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratar os dados para a camada Gold\n",
    "print(f\"Lendo dados da camada Silver de: {caminho_entrada_silver}\")\n",
    "\n",
    "try:\n",
    "    df_silver = pd.read_parquet(caminho_entrada_silver, storage_options=storage_options)\n",
    "    print(f\"Leitura concluÃ­da onde o dataFrame contem {len(df_silver)} linhas e {len(df_silver.columns)} colunas\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erro ao ler o arquivo da camada Silver. Verifique o caminho e as permissÃµes. Detalhes: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "# Copiar e padronizar colunas para minÃºsculas\n",
    "df_estruturado = df_silver.copy()\n",
    "df_estruturado.columns = df_estruturado.columns.str.lower()\n",
    "\n",
    "# Colunas fixas (com descriÃ§Ã£o)\n",
    "colunas_fixas = [\n",
    "    'ano',       # Ano\n",
    "    'v1013',     # MÃªs do Ano\n",
    "    'v1012',     # Semana do MÃªs\n",
    "    'uf',        # Sigla da Unidade da FederaÃ§Ã£o\n",
    "    'capital',   # Capital do Estado\n",
    "    'rm_ride',   # RegiÃ£o Metropolitana e RegiÃ£o Administrativa Integrada de Desenvolvimento\n",
    "    'uf_nome',   # Nome do UF\n",
    "    'sigla',     # Sigla do UF\n",
    "    'regiao',    # RegiÃ£o\n",
    "]\n",
    "\n",
    "# Colunas desejadas (com descriÃ§Ã£o)\n",
    "colunas_desejadas = [\n",
    "    'a002',      # Idade\n",
    "    'a003',      # Sexo\n",
    "    'a004',      # RaÃ§a ou Cor\n",
    "    'a006b',     # VocÃª estÃ¡ tendo aulas presenciais?\n",
    "    'b008',      # O(A) Sr(a) fez algum teste para saber se estava infectado(a) pelo coronavÃ­rus?\n",
    "    'b009a',     # Fez o exame coletado com cotonete na boca e/ou nariz (SWAB)?\n",
    "    'b009c',     # Fez o exame de coleta de sangue atravÃ©s de furo no dedo?\n",
    "    'b009e',     # Fez o exame de coleta de sangue atravÃ©s da veia do braÃ§o?\n",
    "    'a005',      # Escolaridade\n",
    "    'a006',      # Frequenta escola\n",
    "    'a006a',     # A escola/faculdade que frequenta Ã© pÃºblica ou privada?\n",
    "    'b0011',     # Na semana passada teve febre?\n",
    "    'b0012',     # Na semana passada teve tosse?\n",
    "    'b0013',     # Na semana passada teve dor de garganta?\n",
    "    'b0014',     # Na semana passada teve dificuldade para respirar?\n",
    "    'b0015',     # Na semana passada teve dor de cabeÃ§a?\n",
    "    'b0016',     # Na semana passada teve dor no peito?\n",
    "    'b0017',     # Na semana passada teve nÃ¡usea?\n",
    "    'b0018',     # Na semana passada teve nariz entupido ou escorrendo?\n",
    "    'b0019',     # Na semana passada teve fadiga?\n",
    "    'b00110',    # Na semana passada teve dor nos olhos?\n",
    "    'b00111',    # Na semana passada teve perda de cheiro ou sabor?\n",
    "    'b00112',    # Na semana passada teve dor muscular?\n",
    "    'b00113',    # Na semana passada teve diarreia?\n",
    "]\n",
    "\n",
    "# Unir listas mantendo a ordem e sem duplicar\n",
    "todas_colunas = list(dict.fromkeys([*colunas_fixas, *colunas_desejadas]))\n",
    "\n",
    "# Manter apenas as colunas que existem no DataFrame apÃ³s padronizaÃ§Ã£o\n",
    "colunas_existentes = [c for c in todas_colunas if c in df_estruturado.columns]\n",
    "\n",
    "# Filtro com os 3 Ãºltimos valores de v1013\n",
    "df_estruturado[\"v1013\"] = df_estruturado[\"v1013\"].astype(int)\n",
    "\n",
    "if 'v1013' in df_estruturado.columns:\n",
    "    # Coletar valores Ãºnicos (excluindo NaN), ordenar e pegar os 3 Ãºltimos\n",
    "    ultimos_3 = sorted(pd.unique(df_estruturado['v1013'].dropna()))[-3:]\n",
    "\n",
    "    # Filtrar registros pertencentes aos 3 Ãºltimos meses e selecionar colunas\n",
    "    estrutura_final = (\n",
    "        df_estruturado[df_estruturado['v1013'].isin(ultimos_3)][colunas_existentes]\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "else:\n",
    "    # Se nÃ£o houver v1013, apenas seleciona as colunas existentes\n",
    "    estrutura_final = df_estruturado[colunas_existentes].reset_index(drop=True)\n",
    "\n",
    "# Avisos sobre colunas ausentes\n",
    "faltantes = [c for c in todas_colunas if c not in df_estruturado.columns]\n",
    "if faltantes:\n",
    "    print(f\"AtenÃ§Ã£o: estas colunas nÃ£o foram encontradas e ficaram de fora: {faltantes}\")\n",
    "\n",
    "# Converter colunas para o formato correto\n",
    "colunas_para_converter = [\n",
    "    'a002', 'a003', 'a004', 'a005', 'a006', 'a006a', 'a006b'\n",
    "    'b0011', 'b0012', 'b0013', 'b0014', 'b0015', 'b0016', 'b0017',\n",
    "    'b0018', 'b0019', 'b00110', 'b00111', 'b00112', 'b00113',\n",
    "    'b008', 'b009a', 'b009c', 'b009e'\n",
    "]\n",
    "\n",
    "# Itera sobre a lista e converte cada coluna\n",
    "for col in colunas_para_converter:\n",
    "    # 'errors='coerce'' Ã© um parÃ¢metro de seguranÃ§a: se algum valor nÃ£o puder\n",
    "    # ser convertido para nÃºmero, ele se tornarÃ¡ NaN (nulo), evitando que o cÃ³digo quebre\n",
    "    if col in estrutura_final.columns:\n",
    "        estrutura_final[col] = pd.to_numeric(estrutura_final[col], errors='coerce')\n",
    "\n",
    "# Tratar o campo de sintomas\n",
    "estrutura_final[\"febre\"] = (estrutura_final['b0011'] == 1).astype(int)\n",
    "estrutura_final[\"tosse\"] = (estrutura_final[\"b0012\"] == 1).astype(int)\n",
    "estrutura_final[\"dor_garganta\"] = (estrutura_final[\"b0013\"] == 1).astype(int)\n",
    "estrutura_final[\"dificuldade_respirar\"] = (estrutura_final[\"b0014\"] == 1).astype(int)\n",
    "estrutura_final[\"dor_cabeca\"] = (estrutura_final[\"b0015\"] == 1).astype(int)\n",
    "estrutura_final[\"dor_peito\"] = (estrutura_final[\"b0016\"] == 1).astype(int)\n",
    "estrutura_final[\"nausea\"] = (estrutura_final[\"b0017\"] == 1).astype(int)\n",
    "estrutura_final[\"nariz_entupido\"] = (estrutura_final[\"b0018\"] == 1).astype(int)\n",
    "estrutura_final[\"fadiga\"] = (estrutura_final[\"b0019\"] == 1).astype(int)\n",
    "estrutura_final[\"dor_olhos\"] = (estrutura_final[\"b00110\"] == 1).astype(int)\n",
    "estrutura_final[\"perda_olfato\"] = (estrutura_final[\"b00111\"] == 1).astype(int)\n",
    "estrutura_final[\"dor_muscular\"] = (estrutura_final[\"b00112\"] == 1).astype(int)\n",
    "estrutura_final[\"diarreia\"] = (estrutura_final[\"b00113\"] == 1).astype(int)\n",
    "estrutura_final[\"soma_sintomas\"] = estrutura_final[\"febre\"] + estrutura_final[\"tosse\"] + estrutura_final[\"dor_garganta\"] + estrutura_final[\"dificuldade_respirar\"] + estrutura_final[\"dor_cabeca\"] + estrutura_final[\"dor_peito\"] + estrutura_final[\"nausea\"] + estrutura_final[\"nariz_entupido\"] + estrutura_final[\"fadiga\"] + estrutura_final[\"dor_olhos\"] + estrutura_final[\"perda_olfato\"] + estrutura_final[\"dor_muscular\"] + estrutura_final[\"diarreia\"]\n",
    "\n",
    "# Tratar o campo de sexo e raÃ§a\n",
    "estrutura_final['sexo'] = estrutura_final['a003'].apply(lambda x: 'homem' if x == 1 else ('mulher' if x == 2 else None))\n",
    "\n",
    "raca_mapping = {\n",
    "    1: 'Branca',\n",
    "    2: 'Preta',\n",
    "    3: 'Amarela',\n",
    "    4: 'Parda',\n",
    "    5: 'Indigena',\n",
    "    9: 'Ignorado'\n",
    "}\n",
    "\n",
    "estrutura_final['raca'] = estrutura_final['a004'].map(raca_mapping)\n",
    "\n",
    "# Tratar o campo de teste\n",
    "estrutura_final[\"fez_teste\"] = (estrutura_final[\"b008\"] == 1).astype(int)\n",
    "estrutura_final[\"teste_swab\"] = (estrutura_final[\"b009a\"] == 1).astype(int)\n",
    "estrutura_final[\"teste_furo_dedo\"] = (estrutura_final[\"b009c\"] == 1).astype(int)\n",
    "estrutura_final[\"teste_exame_sangue\"] = (estrutura_final[\"b009e\"] == 1).astype(int)\n",
    "\n",
    "# Tratar o campo de escola\n",
    "escolaridade_map = {\n",
    "    1: 'Sem instruÃ§Ã£o',\n",
    "    2: 'Fundamental incompleto',\n",
    "    3: 'Fundamental completa',\n",
    "    4: 'MÃ©dio incompleto',\n",
    "    5: 'MÃ©dio completo',\n",
    "    6: 'Superior incompleto',\n",
    "    7: 'Superior completo',\n",
    "    8: 'PÃ³s-graduaÃ§Ã£o, mestrado ou doutorado',\n",
    "    9: 'Ignorado'\n",
    "}\n",
    "\n",
    "tipo_escola_map = {\n",
    "    1: 'Publica',\n",
    "    2: 'Privada'\n",
    "}\n",
    "\n",
    "aulas_presenciais_map = {\n",
    "    1: 'Sim, normalmente',\n",
    "    2: 'Sim, mas apenas parcialmente',\n",
    "    3: 'NÃ£o, e meu normalmente Ã© presencial/semipresencial',\n",
    "    4: 'NÃ£o, meu curso Ã© online'\n",
    "}\n",
    "\n",
    "estrutura_final['escolaridade'] = estrutura_final['a005'].map(escolaridade_map)\n",
    "estrutura_final['tipo_escola'] = estrutura_final['a006a'].map(tipo_escola_map)\n",
    "estrutura_final['aulas_presenciais'] = estrutura_final['a006b'].map(aulas_presenciais_map)\n",
    "estrutura_final[\"frequenta_escola\"] = (estrutura_final[\"a006\"] == 1).astype(int)\n",
    "\n",
    "# Ver o resultado final\n",
    "print(\"\\nData Frame com os primeiros registros\")\n",
    "display(estrutura_final.head(5))\n",
    "\n",
    "print(\"Data Frame com os ultimos registros\")\n",
    "display(estrutura_final.tail(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa3c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validar a camada silver vs a nova camada gold\n",
    "linhas, colunas = df_silver.shape\n",
    "print(f\"O DataFrame df_silver tem {linhas} linhas\")\n",
    "print(f\"O DataFrame df_silver tem {colunas} colunas\")\n",
    "\n",
    "linhas, colunas = estrutura_final.shape\n",
    "print(f\"\\nO DataFrame estrutura_final tem {linhas} linhas\")\n",
    "print(f\"O DataFrame estrutura_final tem {colunas} colunas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5d1f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar os dados para a Gold\n",
    "print(f\"\\nSalvando dados da camada Gold em: {caminho_saida_gold}\")\n",
    "\n",
    "# Salvar o data frame como .parquet \n",
    "estrutura_final.to_parquet(\n",
    "    caminho_saida_gold,\n",
    "    index=False,\n",
    "    storage_options=storage_options # Parametro necessario para conseguir conectar o Pandas ao AWS\n",
    ")\n",
    "\n",
    "print(f\"âœ… Sucesso! Camada Gold criada com {len(estrutura_final)} linhas e salva no S3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bc759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizar o Glue para catalogar a camada gold usar o Athena\n",
    "\n",
    "# Criar o banco de dados se nÃ£o existir\n",
    "try:\n",
    "    glue_client.get_database(Name=db_name)\n",
    "    print(f\"âž¡ï¸ Banco de dados '{db_name}' jÃ¡ existe\")\n",
    "    \n",
    "except glue_client.exceptions.EntityNotFoundException:\n",
    "    print(f\"Criando banco de dados '{db_name}'\")\n",
    "    glue_client.create_database(DatabaseInput={'Name': db_name})\n",
    "    print(f\"âœ… Banco de dados '{db_name}' criado\")\n",
    "\n",
    "# Criar o Crawler se nÃ£o existir\n",
    "try:\n",
    "    glue_client.get_crawler(Name=crawler_name)\n",
    "    print(f\"\\nâž¡ï¸ Crawler '{crawler_name}' jÃ¡ existe\")\n",
    "\n",
    "except glue_client.exceptions.EntityNotFoundException:\n",
    "    print(f\"Criando crawler '{crawler_name}'\")\n",
    "    glue_client.create_crawler(\n",
    "        Name=crawler_name,\n",
    "        Role=role_arn,\n",
    "        DatabaseName=db_name,\n",
    "        Targets={\n",
    "            'S3Targets': [\n",
    "                {\n",
    "                    'Path': gold_prefix,\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        TablePrefix = TablePrefix\n",
    "    )\n",
    "    print(f\"âœ… Crawler '{crawler_name}' criado\")\n",
    "\n",
    "# Executar o Crawler\n",
    "print(f\"\\nIniciando a execuÃ§Ã£o do crawler '{crawler_name}'\")\n",
    "glue_client.start_crawler(Name=crawler_name)\n",
    "\n",
    "# Monitorar a execuÃ§Ã£o do crawler\n",
    "while True:\n",
    "    try:\n",
    "        crawler_status = glue_client.get_crawler(Name=crawler_name)\n",
    "        state = crawler_status['Crawler']['State']\n",
    "        last_crawl_info = crawler_status['Crawler'].get('LastCrawl', {})\n",
    "        status_detail = last_crawl_info.get('Status', 'N/A')\n",
    "        \n",
    "        print(f\"Status do Crawler: {state} | Detalhe da Ãšltima ExecuÃ§Ã£o: {status_detail}\")\n",
    "\n",
    "        if state == 'READY':\n",
    "            if status_detail == 'SUCCEEDED':\n",
    "                print(\"\\nâœ… Crawler finalizou a execuÃ§Ã£o com sucesso!\")\n",
    "            elif status_detail in ['FAILED', 'CANCELLED']:\n",
    "                print(f\"\\nâŒ A Ãºltima execuÃ§Ã£o do crawler falhou ou foi cancelada. Detalhe: {last_crawl_info.get('ErrorMessage', 'Sem detalhes')}\")\n",
    "            else:\n",
    "                 print(\"âœ… Crawler estÃ¡ pronto para a prÃ³xima execuÃ§Ã£o.\")\n",
    "            break # Sair do loop em qualquer cenÃ¡rio onde o crawler estÃ¡ 'READY'\n",
    "\n",
    "        elif state == 'FAILED':\n",
    "            print(f\"âŒ Erro crÃ­tico no crawler. Detalhe: {crawler_status['Crawler'].get('LastCrawl', {}).get('ErrorMessage', 'Sem detalhes')}\")\n",
    "            break\n",
    "\n",
    "        # Se estiver em RUNNING ou STOPPING, continua esperando\n",
    "        time.sleep(30)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro ao monitorar o crawler: {e}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b3b6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registrar o tempo final do cÃ³digo\n",
    "variavel_tempo_final = time.time()\n",
    "\n",
    "# Gerar o resultado\n",
    "calcular_tempo_execucao(variavel_tempo_inicio, variavel_tempo_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ambiente_fase3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
