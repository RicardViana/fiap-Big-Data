{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89d6ca84",
   "metadata": {},
   "source": [
    "#### **Fundamentos de Bancos de Dados Relacionais e NoSQL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd59115f",
   "metadata": {},
   "source": [
    "#### **Conte√∫do - Bases e Notebook da aula**\n",
    "\n",
    "https://github.com/FIAP/Pos_Tech_DTAT/tree/main/Fase%203"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a770d68e",
   "metadata": {},
   "source": [
    "#### **Importa√ß√£o de pacotes, bibliotecas e fun√ß√µes (def)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5ca788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar biblioteca completa\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "import plotly.express as px\n",
    "import requests\n",
    "import botocore\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Importar fun√ß√£o especifica de um m√≥dulo\n",
    "from botocore.exceptions import BotoCoreError, ClientError\n",
    "from sqlalchemy import create_engine, text, inspect\n",
    "from dotenv import load_dotenv\n",
    "from io import StringIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efdef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar a conex√£o ao banco de dados\n",
    "def test_connection(engine):\n",
    "\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            \n",
    "            # Testar a vers√£o do PostgreSQL\n",
    "            result = connection.execute(text(\"SELECT version();\"))\n",
    "            versao = result.fetchone()\n",
    "            print(\"‚úÖ Conectado com sucesso:\", versao[0])\n",
    "\n",
    "            # Listar as tabelas no schema p√∫blico\n",
    "            result = connection.execute(text(\"\"\"\n",
    "                SELECT table_name\n",
    "                FROM information_schema.tables\n",
    "                WHERE table_schema = 'public';\n",
    "            \"\"\"))\n",
    "            tabelas = result.fetchall()\n",
    "            print(\"üìÑ Tabelas no banco:\")\n",
    "            for tabela in tabelas:\n",
    "                print(\"  -\", tabela[0])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Erro ao executar comandos:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dfe187",
   "metadata": {},
   "source": [
    "#### **Instala√ß√£o do AWS CLI (Opcional, mas Recomendado)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c64a44",
   "metadata": {},
   "source": [
    "Para rodar comandos da AWS no terminal para testar credenciais, criar buckets ou consultar recursos instale o **AWS CLI v2**.\n",
    "\n",
    "##### **macOS / Linux (zsh ou bash)**\n",
    "\n",
    "**Instala√ß√£o com Homebrew (recomendado para macOS)**\n",
    "```bash\n",
    "brew install awscli\n",
    "```\n",
    "\n",
    "##### **Instala√ß√£o manual**\n",
    "```bash\n",
    "curl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\"\n",
    "sudo installer -pkg AWSCLIV2.pkg -target /\n",
    "```\n",
    "\n",
    "##### **Verificar instala√ß√£o**\n",
    "```bash\n",
    "aws --version\n",
    "```\n",
    "\n",
    "##### **Windows**\n",
    "\n",
    "1. Baixe o instalador: [AWS CLI v2](https://awscli.amazonaws.com/AWSCLIV2.msi)\n",
    "2. Execute o instalador e finalize.\n",
    "3. Teste no PowerShell:\n",
    "```bash\n",
    "aws --version\n",
    "```\n",
    "\n",
    "##### **Testar credenciais**\n",
    "Ap√≥s instalar o **AWS CLI**, teste o perfil `academy` criado:\n",
    "\n",
    "```bash\n",
    "aws sts get-caller-identity --profile academy\n",
    "```\n",
    "\n",
    "Se retornar o **Account ID** e o **ARN**, as credenciais est√£o funcionando ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2044ca",
   "metadata": {},
   "source": [
    "#### **Credenciais do AWS Academy** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e5cf04",
   "metadata": {},
   "source": [
    "Para seguir com as etapas desse notebook ser√° necessario pegar as credencias do AWS Academy e criar o arquivo com essas credencias:\n",
    "\n",
    "**1. Pegar credenciais do AWS Academy**  \n",
    "1. Acesse [AWS Academy](https://awsacademy.instructure.com/)\n",
    "2. Acesse a op√ß√£o **Cursos** no lado esquerdo e acesse o seu respectivo curso\n",
    "3. Acessa a op√ß√£p **M√≥dulos**\n",
    "4. Acesse a op√ß√£o **Iniciar os laborat√≥rios de aprendizagem da AWS Academy**\n",
    "5. Clique em **Start Lab**\n",
    "6. Clique em **AWS Details**\n",
    "7. Clique em **AWS CLI: Show** e copie:\n",
    "   - **AWS Access Key ID**\n",
    "   - **AWS Secret Access Key**\n",
    "   - **Session Token**\n",
    "\n",
    "‚ö†Ô∏è As credenciais s√£o tempor√°rias (v√°lidas por 3 horas).\n",
    "\n",
    "**2. Criar arquivo de credenciais**  \n",
    "\n",
    "Crie o arquivo **`credentials`** no caminho:\n",
    "\n",
    "- **macOS/Linux:** `~/.aws/credentials`\n",
    "- **Windows:** `%USERPROFILE%\\.aws\\credentials`\n",
    "\n",
    "‚ö†Ô∏è Salvar o arquivo sem formato --> Na op√ß√£p Tipo deixar Todos os arquivos\n",
    "\n",
    "**3. Conte√∫do do arquivo**\n",
    "```ini\n",
    "[academy]\n",
    "aws_access_key_id = SUA_ACCESS_KEY\n",
    "aws_secret_access_key = SUA_SECRET_KEY\n",
    "aws_session_token = SEU_SESSION_TOKEN\n",
    "region = sa-east-1\n",
    "output = json\n",
    "```\n",
    "\n",
    "**4. Testar configura√ß√£o**\n",
    "```bash\n",
    "aws sts get-caller-identity --profile academy\n",
    "```\n",
    "\n",
    "Se tudo estiver certo, rode o notebook.  \n",
    "Agora ele **usa automaticamente o perfil `academy`** para se conectar √† AWS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680f092b",
   "metadata": {},
   "source": [
    "#### **Testar conex√£o AWS via Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02baef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validar conex√£o\n",
    "try:\n",
    "    session = boto3.Session(profile_name=\"academy\")\n",
    "    sts = session.client(\"sts\")\n",
    "    identity = sts.get_caller_identity()\n",
    "    print(\"‚úÖ Conectado √† conta\\n\")\n",
    "    print(\"UserId:\", identity[\"UserId\"])\n",
    "    print(\"Account:\", identity[\"Account\"])\n",
    "    print(\"Arn:\", identity[\"Arn\"])\n",
    "\n",
    "except (BotoCoreError, ClientError) as e:\n",
    "    print(\"‚ùå Erro ao conectar √† AWS. Verifique suas credenciais e tente novamente.\")\n",
    "    print(\"Detalhes do erro:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6164f6",
   "metadata": {},
   "source": [
    "#### **Configura√ß√£o do PostgreSQL na AWS RDS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427e99a5",
   "metadata": {},
   "source": [
    "##### 1. **Criar inst√¢ncia RDS com PostgreSQL (SandBox)**\n",
    "\n",
    "1. Acesse o console AWS ‚Üí [https://us-east-1.console.aws.amazon.com/rds/home?region=us-east-1#](https://console.aws.amazon.com/rds/)\n",
    "2. Clique em **Criar banco de dados**\n",
    "3. Selecione:\n",
    "   - **Tipo de banco:** PostgreSQL\n",
    "   - **Vers√£o:** PostgreSQL 15 (ou mais recente)\n",
    "   - **Modelo de uso:** SandBox\n",
    "   - **Identificador da inst√¢ncia:** `postgres-db`\n",
    "   - **Usu√°rio:** `postgres`\n",
    "   - **Senha:** crie uma senha segura\n",
    "4. Tipo de inst√¢ncia: `db.t3.micro`\n",
    "5. Armazenamento: 20 GB (SSD General Purpose)\n",
    "6. **Acesso p√∫blico:** Habilitado (Sim)\n",
    "7. **Nome do banco de dados inicial:** `db_relacional`\n",
    "8. Clique em **Criar banco de dados**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3f5cd6",
   "metadata": {},
   "source": [
    "##### **2. Liberar o IP na VPC / Grupo de Seguran√ßa (Security Group)**\n",
    "\n",
    "1. V√° para **EC2 > Grupos de Seguran√ßa**\n",
    "2. Encontre o grupo associado √† inst√¢ncia RDS\n",
    "3. Clique em **Editar regras de entrada**\n",
    "4. Adicione uma nova regra:\n",
    "   - Tipo: `PostgreSQL`\n",
    "   - Porta: `5432`\n",
    "   - Origem: `Seu IP` (ou `0.0.0.0/0` temporariamente para teste ‚Äì cuidado com isso em produ√ß√£o)\n",
    "5. Salve as altera√ß√µes.\n",
    "\n",
    "‚úÖ Agora o acesso externo ao banco estar√° liberado para seu IP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5546cb69",
   "metadata": {},
   "source": [
    "##### **3. Copie o Endpoint da RDS**\n",
    "\n",
    "1. Volte ao RDS > Banco de dados > `bd-relacional`\n",
    "2. Copie o valor do campo **Endpoint** (algo como `bd-relacional.xxxxxx.us-east-1.rds.amazonaws.com`)\n",
    "3. Use esse endpoint no notebook para se conectar com o PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d029bb20",
   "metadata": {},
   "source": [
    "#### **Aula 1 - Introdu√ß√£o ao Banco De Dados Relacionais**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d94c24",
   "metadata": {},
   "source": [
    "##### **Conectar ao PostgreSQL via RDS + Executar Comandos SQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf802d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar a engine para conex√£o ao banco de dados usando .env\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "usuario = os.getenv(\"POSTGRES_USER\")\n",
    "senha = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "host = os.getenv(\"POSTGRES_HOST\")\n",
    "porta = os.getenv(\"POSTGRES_PORT\")\n",
    "banco = os.getenv(\"POSTGRES_DB\")\n",
    "\n",
    "engine = create_engine(f\"postgresql+psycopg2://{usuario}:{senha}@{host}:{porta}/{banco}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a93819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar a conex√£o\n",
    "test_connection(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b6a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropar as tabelas --> CASCADE garante que todas as depend√™ncias (FKs) sejam eliminadas junto com a tabela\n",
    "drop_script = \"\"\"\n",
    "DROP TABLE IF EXISTS itens_pedido CASCADE;\n",
    "DROP TABLE IF EXISTS pedidos CASCADE;\n",
    "DROP TABLE IF EXISTS produtos CASCADE;\n",
    "DROP TABLE IF EXISTS clientes CASCADE;\n",
    "DROP TABLE IF EXISTS tipos_produto CASCADE;\n",
    "\"\"\"\n",
    "\n",
    "conn = engine.raw_connection()\n",
    "try:\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(drop_script)\n",
    "    conn.commit()\n",
    "    print(\"üóëÔ∏è Todas as tabelas foram deletadas com sucesso.\")\n",
    "finally:\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "# Listar de comandos individuais\n",
    "ddl_commands = [\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS tipos_produto (\n",
    "      id_tipo SERIAL PRIMARY KEY,\n",
    "      nome_tipo VARCHAR(50) NOT NULL\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS produtos (\n",
    "      id_produto SERIAL PRIMARY KEY,\n",
    "      nome_produto VARCHAR(100) NOT NULL,\n",
    "      preco DECIMAL(10,2) NOT NULL,\n",
    "      id_tipo INT REFERENCES tipos_produto(id_tipo)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS clientes (\n",
    "      id_cliente SERIAL PRIMARY KEY,\n",
    "      nome VARCHAR(100) NOT NULL,\n",
    "      email VARCHAR(100),\n",
    "      telefone VARCHAR(20), \n",
    "      cidade VARCHAR(100) NOT NULL, \n",
    "      estado VARCHAR(2) NOT NULL\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS pedidos (\n",
    "      id_pedido SERIAL PRIMARY KEY,\n",
    "      data_pedido DATE NOT NULL,\n",
    "      status VARCHAR(20) NOT NULL,\n",
    "      id_cliente INT NOT NULL REFERENCES clientes(id_cliente)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS itens_pedido (\n",
    "    id_item SERIAL PRIMARY KEY,\n",
    "    id_pedido INT NOT NULL,\n",
    "    id_produto INT NOT NULL,\n",
    "    quantidade INT NOT NULL,\n",
    "    preco_unitario DECIMAL(10,2) NOT NULL,\n",
    "    CONSTRAINT fk_pedido FOREIGN KEY (id_pedido) REFERENCES pedidos(id_pedido) ON DELETE CASCADE,\n",
    "    CONSTRAINT fk_produto FOREIGN KEY (id_produto) REFERENCES produtos(id_produto) ON DELETE CASCADE\n",
    "    );\n",
    "\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Lista de nomes das tabelas na mesma ordem dos comandos\n",
    "table_names = [\n",
    "    \"tipos_produto\",\n",
    "    \"produtos\",\n",
    "    \"clientes\",\n",
    "    \"pedidos\",\n",
    "    \"itens_pedido\"\n",
    "]\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    for cmd, table in zip(ddl_commands, table_names):\n",
    "        conn.execute(text(cmd))\n",
    "        print(f\"‚úÖ Tabela '{table}' criada com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32758d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar as chaves primarias\n",
    "df_pks = pd.read_sql_query(\"\"\"\n",
    "SELECT \n",
    "    kcu.table_schema,\n",
    "    kcu.table_name,\n",
    "    kcu.column_name,\n",
    "    tc.constraint_name\n",
    "FROM information_schema.table_constraints tc\n",
    "JOIN information_schema.key_column_usage kcu\n",
    "  ON tc.constraint_name = kcu.constraint_name\n",
    "WHERE tc.constraint_type = 'PRIMARY KEY'\n",
    "  AND kcu.table_schema = 'public';\n",
    "\"\"\", con=engine)\n",
    "\n",
    "df_pks.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b5cf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar chaves estrangeiras e relacionamento\n",
    "df_fks = pd.read_sql_query(\"\"\"\n",
    "SELECT \n",
    "    tc.table_name AS tabela_origem,\n",
    "    kcu.column_name AS coluna_origem,\n",
    "    ccu.table_name AS tabela_referenciada,\n",
    "    ccu.column_name AS coluna_referenciada\n",
    "FROM information_schema.table_constraints AS tc\n",
    "JOIN information_schema.key_column_usage AS kcu\n",
    "  ON tc.constraint_name = kcu.constraint_name\n",
    "JOIN information_schema.constraint_column_usage AS ccu\n",
    "  ON ccu.constraint_name = tc.constraint_name\n",
    "WHERE tc.constraint_type = 'FOREIGN KEY'\n",
    "  AND tc.table_schema = 'public';\n",
    "\"\"\", con=engine)\n",
    "\n",
    "df_fks.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce09d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar join entre as tabelas\n",
    "df_relacionamentos = pd.read_sql_query(\"\"\"\n",
    "SELECT \n",
    "  tc.constraint_name,\n",
    "  tc.table_name AS origem,\n",
    "  kcu.column_name AS coluna_origem,\n",
    "  ccu.table_name AS destino,\n",
    "  ccu.column_name AS coluna_destino\n",
    "FROM information_schema.table_constraints AS tc\n",
    "JOIN information_schema.key_column_usage AS kcu\n",
    "  ON tc.constraint_name = kcu.constraint_name\n",
    "JOIN information_schema.constraint_column_usage AS ccu\n",
    "  ON ccu.constraint_name = tc.constraint_name\n",
    "WHERE tc.constraint_type = 'FOREIGN KEY'\n",
    "ORDER BY origem;\n",
    "\"\"\", con=engine)\n",
    "\n",
    "df_relacionamentos.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108dc922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar arquivos .sql\n",
    "truncate_script = \"\"\"\n",
    "TRUNCATE TABLE itens_pedido CASCADE;\n",
    "TRUNCATE TABLE pedidos CASCADE;\n",
    "TRUNCATE TABLE produtos CASCADE;\n",
    "TRUNCATE TABLE clientes CASCADE;\n",
    "TRUNCATE TABLE tipos_produto CASCADE;\n",
    "\"\"\"\n",
    "\n",
    "github_urls = [\n",
    "    \"https://raw.githubusercontent.com/FIAP/Pos_Tech_DTAT/refs/heads/main/Fase%203/Aula4/sql/aula4/tipos_produto.sql\",\n",
    "    \"https://raw.githubusercontent.com/FIAP/Pos_Tech_DTAT/refs/heads/main/Fase%203/Aula4/sql/aula4/produtos.sql\",\n",
    "    \"https://raw.githubusercontent.com/FIAP/Pos_Tech_DTAT/refs/heads/main/Fase%203/Aula4/sql/aula4/clientes.sql\",\n",
    "    \"https://raw.githubusercontent.com/FIAP/Pos_Tech_DTAT/refs/heads/main/Fase%203/Aula4/sql/aula4/pedidos.sql\",\n",
    "    \"https://raw.githubusercontent.com/FIAP/Pos_Tech_DTAT/refs/heads/main/Fase%203/Aula4/sql/aula4/itens_pedido.sql\"\n",
    "]\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(truncate_script))\n",
    "    print(\"üóëÔ∏è Dados apagados de todas as tabelas.\")\n",
    "\n",
    "    for url in github_urls:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        sql_content = response.text\n",
    "        conn.execute(text(sql_content))\n",
    "        print(f\"‚úÖ Executado: {url.split('/')[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82ae39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join para analise de vendas\n",
    "df = pd.read_sql_query(\n",
    "\n",
    "\"\"\"\n",
    "                       \n",
    "SELECT \n",
    "  c.nome AS cliente,\n",
    "  p.data_pedido as data_pedido,\n",
    "  pr.nome_produto AS produto,\n",
    "  pr.preco as preco,\n",
    "  t.nome_tipo AS tipo_produto,\n",
    "  ip.quantidade as quantidade,\n",
    "  (pr.preco * ip.quantidade) AS total_venda\n",
    "\n",
    "FROM itens_pedido ip\n",
    "\n",
    "JOIN pedidos p ON ip.id_pedido = p.id_pedido\n",
    "JOIN clientes c ON p.id_cliente = c.id_cliente\n",
    "JOIN produtos pr ON ip.id_produto = pr.id_produto\n",
    "JOIN tipos_produto t ON pr.id_tipo = t.id_tipo;\n",
    "\n",
    "\"\"\"\n",
    ",con=engine)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad67b861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar grafico\n",
    "fig = px.bar(df, x=\"cliente\", y=\"total_venda\", color=\"produto\", \n",
    "             title=\"Total de Vendas por Cliente e Produto\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4771de9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "deletar_tabelas = 'n'  # Altere para s se quiser dropar\n",
    "\n",
    "if deletar_tabelas.lower() == 's':\n",
    "    drop_script = \"\"\"\n",
    "    DROP TABLE IF EXISTS itens_pedido CASCADE;\n",
    "    DROP TABLE IF EXISTS pedidos CASCADE;\n",
    "    DROP TABLE IF EXISTS produtos CASCADE;\n",
    "    DROP TABLE IF EXISTS clientes CASCADE;\n",
    "    DROP TABLE IF EXISTS tipos_produto CASCADE;\n",
    "    \"\"\"\n",
    "    conn = engine.raw_connection()\n",
    "\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(drop_script)\n",
    "        conn.commit()\n",
    "        print(\"üóëÔ∏è Todas as tabelas foram deletadas com sucesso\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Drop de tabelas n√£o executado. Vari√°vel de controle est√° {deletar_tabelas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31b4535",
   "metadata": {},
   "source": [
    "#### **Aula 3 - Stacks Modernas, Data Warehouse, Data Lake e Lakehouse, Data Mesh e Data Fabric**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee55d376",
   "metadata": {},
   "source": [
    "##### **Modelo Dimensional em DBML ‚Äì Diagrama DW (dimens√µes e fato)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9d2b9c",
   "metadata": {},
   "source": [
    "Criar o Diagrama Entidade-Relacionamento (DER) usando o site https://dbdiagram.io/home e o seguinte c√≥digo DBML (Database Markup Language ou Linguagem de Marca√ß√£o para Banco de Dados):   \n",
    "\n",
    "```dbml\n",
    "Table dim_cliente {\n",
    "  id_cliente int [pk]\n",
    "  nome varchar\n",
    "  idade int\n",
    "  cidade varchar\n",
    "}\n",
    "\n",
    "Table dim_produto {\n",
    "  id_produto int [pk]\n",
    "  nome_produto varchar\n",
    "  categoria varchar\n",
    "  preco decimal\n",
    "}\n",
    "\n",
    "Table fato_pedidos {\n",
    "  id_pedido int [pk]\n",
    "  id_cliente int [ref: > dim_cliente.id_cliente]\n",
    "  id_produto int [ref: > dim_produto.id_produto]\n",
    "  id_data int [ref: > dim_data.id_data]\n",
    "  id_regiao int [ref: > dim_regiao.id_regiao]\n",
    "  data_pedido date\n",
    "  quantidade int\n",
    "  valor_total decimal\n",
    "}\n",
    "\n",
    "Table dim_data {\n",
    "  id_data int [pk]\n",
    "  data date\n",
    "  ano int\n",
    "  mes int\n",
    "  dia int\n",
    "  dia_semana varchar\n",
    "  nome_mes varchar\n",
    "}\n",
    "\n",
    "Table dim_regiao {\n",
    "  id_regiao int [pk]\n",
    "  nome_regiao varchar\n",
    "  estado varchar\n",
    "  cidade varchar\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d52a3fd",
   "metadata": {},
   "source": [
    "![Diagrama DW](imagame.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e8ce5f",
   "metadata": {},
   "source": [
    "##### **C√≥digos - Carregar dados no PostgreSQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8e4fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar tabelas\n",
    "ddl_dim_fato = [\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS dim_cliente (\n",
    "      id_cliente INT PRIMARY KEY,\n",
    "      nome VARCHAR(100),\n",
    "      idade INT,\n",
    "      cidade VARCHAR(100)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS dim_produto (\n",
    "      id_produto INT PRIMARY KEY,\n",
    "      nome_produto VARCHAR(100),\n",
    "      categoria VARCHAR(50),\n",
    "      preco DECIMAL(10,2)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS fato_pedidos (\n",
    "      id_pedido INT PRIMARY KEY,\n",
    "      id_cliente INT REFERENCES dim_cliente(id_cliente),\n",
    "      id_produto INT REFERENCES dim_produto(id_produto),\n",
    "      data_pedido DATE,\n",
    "      quantidade INT,\n",
    "      valor_total DECIMAL(10,2)\n",
    "    );\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "table_names = [\n",
    "    \"dim_cliente\",\n",
    "    \"dim_produto\",\n",
    "    \"fato_pedidos\"\n",
    "]\n",
    "\n",
    "# Validar se as tabelas j√° existe\n",
    "with engine.begin() as conn:\n",
    "    for cmd, table in zip(ddl_dim_fato, table_names):\n",
    "        result = conn.execute(\n",
    "            text(f\"SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = '{table}');\")\n",
    "        )\n",
    "        exists = result.scalar()\n",
    "        if not exists:\n",
    "            conn.execute(text(cmd))\n",
    "            print(f\"‚úÖ Tabela '{table}' criada com sucesso!\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Tabela '{table}' j√° existe. Nenhuma altera√ß√£o feita.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2322d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apagar dados das tabelas e executar arquivos .sql\n",
    "github_urls = [\n",
    "    \"https://raw.githubusercontent.com/FIAP/Pos_Tech_DTAT/refs/heads/main/Fase%203/Aula4/sql/aula3-dw/insert_dim_cliente.sql\",\n",
    "    \"https://raw.githubusercontent.com/FIAP/Pos_Tech_DTAT/refs/heads/main/Fase%203/Aula4/sql/aula3-dw/insert_dim_produto.sql\"\n",
    "\n",
    "]\n",
    "\n",
    "delete_script = \"\"\"\n",
    "TRUNCATE TABLE dim_cliente, dim_produto CASCADE\n",
    "\"\"\"\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(delete_script))\n",
    "    print(\"üóëÔ∏è Dados apagados de todas as tabelas.\")\n",
    "\n",
    "    for url in github_urls:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        sql_content = response.text\n",
    "        conn.execute(text(sql_content))\n",
    "        print(f\"‚úÖ Executado: {url.split('/')[-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f3bddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apagar dados das tabelas e executar arquivos .sql\n",
    "github_urls = [\n",
    "    \"https://raw.githubusercontent.com/FIAP/Pos_Tech_DTAT/refs/heads/main/Fase%203/Aula4/sql/aula3-dw/insert_fato_pedidos.sql\"\n",
    "]\n",
    "\n",
    "delete_script = \"\"\"\n",
    "TRUNCATE TABLE fato_pedidos CASCADE\n",
    "\"\"\"\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(delete_script))\n",
    "    print(\"üóëÔ∏è Dados apagados de todas as tabelas.\")\n",
    "\n",
    "    for url in github_urls:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        sql_content = response.text\n",
    "        conn.execute(text(sql_content))\n",
    "        print(f\"‚úÖ Executado: {url.split('/')[-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21926089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join para ver a ultima compra do cliente\n",
    "df = pd.read_sql_query(\n",
    "\n",
    "\"\"\"\n",
    "                       \n",
    "select \n",
    "\tb.nome, max(a.data_pedido) ultima_compra\n",
    "\n",
    "from \n",
    "\tfato_pedidos as a\n",
    "\n",
    "join dim_cliente as b \n",
    "on a.id_cliente = b.id_cliente\n",
    "\n",
    "group by\n",
    "\tb.nome\n",
    "\t\n",
    "\"\"\"\n",
    ",con=engine)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7861cd6",
   "metadata": {},
   "source": [
    "##### **C√≥digos - Exportar Postgre para SQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca9456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar conex√£o PostgreSQL\n",
    "pg_config = {\n",
    "    \"host\": host,\n",
    "    \"database\": banco,\n",
    "    \"user\": usuario,\n",
    "    \"password\": senha,\n",
    "    \"port\": porta\n",
    "}\n",
    "\n",
    "# Configurar dados buckt do AWS S3\n",
    "bucket_name = \"aula-data-lake-430854566059\"\n",
    "s3_prefix = \"raw/\"\n",
    "\n",
    "# Listar tabelas a exportar \n",
    "tabelas = ['clientes', 'produtos', 'pedidos', 'itens_pedido', 'tipos_produto']\n",
    "\n",
    "# Validar conex√£o com PostgreSQL usando o engine\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"SELECT version();\"))\n",
    "        versao = result.fetchone()\n",
    "        print(f\"‚úÖ Conex√£o com o PostgreSQL estabelecida: {versao[0]}\\n\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Erro ao conectar ao PostgreSQL:\", e)\n",
    "    print()\n",
    "\n",
    "# Criar conex√£o com S3\n",
    "s3 = session.client('s3')\n",
    "region = s3.meta.region_name or \"us-east-1\" \n",
    "\n",
    "# Verificar buckt\n",
    "try:\n",
    "    s3.head_bucket(Bucket=bucket_name)\n",
    "    print(f\"‚úÖ Bucket '{bucket_name}' j√° existe\\n\")\n",
    "\n",
    "except ClientError as e:\n",
    "    error_code = int(e.response['Error']['Code'])\n",
    "    if error_code == 404:\n",
    "        print(f\"Bucket '{bucket_name}' n√£o existe, criando...\\n\")\n",
    "        if region == \"us-east-1\":\n",
    "            s3.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            s3.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "        print(f\"Bucket '{bucket_name}' criado com sucesso\\n\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Exportar dados do Postgres\n",
    "for tabela in tabelas:\n",
    "    print(f\"Exportando tabela: {tabela}\")\n",
    "    df = pd.read_sql(f\"SELECT * FROM {tabela};\", engine) \n",
    "\n",
    "    # Salvar como CSV em mem√≥ria\n",
    "    csv_buffer = StringIO()\n",
    "    df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "    # Enviar para S3\n",
    "    s3_key = f\"{s3_prefix}{tabela}.csv\"\n",
    "    s3.put_object(Bucket=bucket_name, Key=s3_key, Body=csv_buffer.getvalue())\n",
    "    print(f\"‚úÖ {tabela} salva no S3 em: s3://{bucket_name}/{s3_key}\\n\")\n",
    "\n",
    "print(\"Exporta√ß√£o conclu√≠da com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941f33a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processamento das camadas Silver e Gold no S3 particionado por ano mes e dia finalizado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Criar um dicion√°rio com as op√ß√µes de armazenamento, especificando o perfil\n",
    "storage_options = {\"profile\": \"academy\"}\n",
    "storage_options=storage_options\n",
    "\n",
    "# Para acessar S3, Pandas usa o s3fs automaticamente\n",
    "RAW_PREFIX = 's3://aula-data-lake-430854566059/raw/'\n",
    "SILVER_PREFIX = 's3://aula-data-lake-430854566059/silver/'\n",
    "GOLD_PREFIX = 's3://aula-data-lake-430854566059/gold/'\n",
    "\n",
    "# 1. Silver Layer: Leitura e Limpeza dos Dados\n",
    "clientes = pd.read_csv(RAW_PREFIX + 'clientes.csv', storage_options=storage_options)\n",
    "produtos = pd.read_csv(RAW_PREFIX + 'produtos.csv', storage_options=storage_options)\n",
    "tipos_produto = pd.read_csv(RAW_PREFIX + 'tipos_produto.csv', storage_options=storage_options)\n",
    "pedidos = pd.read_csv(RAW_PREFIX + 'pedidos.csv', storage_options=storage_options)\n",
    "itens_pedido = pd.read_csv(RAW_PREFIX + 'itens_pedido.csv', storage_options=storage_options)\n",
    "\n",
    "# Padronizar colunas para lower case\n",
    "for df in [clientes, produtos, tipos_produto, pedidos, itens_pedido]:\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "\n",
    "# Limpeza b√°sica\n",
    "clientes = clientes.drop_duplicates().dropna(subset=['id_cliente'])\n",
    "produtos = produtos.drop_duplicates().dropna(subset=['id_produto'])\n",
    "tipos_produto = tipos_produto.drop_duplicates().dropna(subset=['id_tipo'])\n",
    "pedidos = pedidos.drop_duplicates().dropna(subset=['id_pedido', 'id_cliente'])\n",
    "itens_pedido = itens_pedido.drop_duplicates().dropna(subset=['id_item', 'id_pedido', 'id_produto'])\n",
    "\n",
    "# Padroniza√ß√£o de datas\n",
    "if 'data_pedido' in pedidos.columns:\n",
    "    pedidos['data_pedido'] = pd.to_datetime(pedidos['data_pedido'], errors='coerce')\n",
    "\n",
    "# Salvar camada Silver em Parquet no S3\n",
    "clientes.to_parquet(SILVER_PREFIX + 'clientes/', index=False,storage_options=storage_options)\n",
    "produtos.to_parquet(SILVER_PREFIX + 'produtos/', index=False,storage_options=storage_options)\n",
    "tipos_produto.to_parquet(SILVER_PREFIX + 'tipos_produto/', index=False,storage_options=storage_options)\n",
    "pedidos.to_parquet(SILVER_PREFIX + 'pedidos/', index=False,storage_options=storage_options)\n",
    "itens_pedido.to_parquet(SILVER_PREFIX + 'itens_pedido/', index=False,storage_options=storage_options)\n",
    "\n",
    "# 2. Gold Layer: Enriquecimento e Agrega√ß√µes\n",
    "\n",
    "# Fato de vendas com todas as dimens√µes\n",
    "gold_vendas = itens_pedido.merge(pedidos, on='id_pedido') \\\n",
    "    .merge(produtos, on='id_produto') \\\n",
    "    .merge(tipos_produto, on='id_tipo') \\\n",
    "    .merge(clientes, on='id_cliente')\n",
    "\n",
    "# Valor total do item\n",
    "if 'quantidade' in gold_vendas.columns and 'preco_unitario' in gold_vendas.columns:\n",
    "    gold_vendas['valor_total_item'] = gold_vendas['quantidade'] * gold_vendas['preco_unitario']\n",
    "\n",
    "# Cria√ß√£o do campo anomesdia\n",
    "if 'data_pedido' in gold_vendas.columns:\n",
    "    gold_vendas['anomesdia'] = gold_vendas['data_pedido'].dt.strftime('%Y%m%d')\n",
    "\n",
    "# Exemplo: Resumo por cliente\n",
    "gold_vendas_por_cliente = gold_vendas.groupby(['id_cliente', 'nome']) \\\n",
    "    .agg({'valor_total_item': 'sum', 'id_pedido': 'nunique'}) \\\n",
    "    .rename(columns={'valor_total_item': 'valor_total_comprado', 'id_pedido': 'num_pedidos'}) \\\n",
    "    .reset_index()\n",
    "\n",
    "# Exemplo: Resumo por tipo de produto\n",
    "gold_vendas_por_tipo = gold_vendas.groupby(['id_tipo', 'nome_tipo']) \\\n",
    "    .agg({'valor_total_item': 'sum', 'quantidade': 'sum'}) \\\n",
    "    .rename(columns={'valor_total_item': 'total_vendido', 'quantidade': 'quantidade_total'}) \\\n",
    "    .reset_index()\n",
    "\n",
    "# Salvar camada Gold no S3, particionando por anomesdia\n",
    "gold_vendas.to_parquet(GOLD_PREFIX + 'fato_vendas/', partition_cols=['anomesdia'], index=False, storage_options=storage_options)\n",
    "gold_vendas_por_cliente.to_parquet(GOLD_PREFIX + 'vendas_por_cliente/', index=False, storage_options=storage_options)\n",
    "gold_vendas_por_tipo.to_parquet(GOLD_PREFIX + 'vendas_por_tipo/', index=False, storage_options=storage_options)\n",
    "\n",
    "print(\"Processamento das camadas Silver e Gold no S3 particionado por ano mes e dia finalizado com sucesso!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
